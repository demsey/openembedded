Index: Makefile.in
===================================================================
--- Makefile.in	(.../vendor/tags/4.2.4)	(revision 920)
+++ Makefile.in	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -359,8 +359,9 @@
 
 #### host and target specific makefile fragments come in here.
 @target_makefile_frag@
-@alphaieee_frag@
 @ospace_frag@
+@ieee_frag@
+@relax_frag@
 @host_makefile_frag@
 ###
 
Index: libstdc++-v3/src/ext-inst.cc
===================================================================
--- libstdc++-v3/src/ext-inst.cc	(.../vendor/tags/4.2.4)	(revision 920)
+++ libstdc++-v3/src/ext-inst.cc	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -31,6 +31,7 @@
 // ISO C++ 14882:
 //
 
+#include <ext/concurrence.h>
 #include <ext/rope>
 #include <ext/stdio_filebuf.h>
 
Index: libstdc++-v3/src/bitmap_allocator.cc
===================================================================
--- libstdc++-v3/src/bitmap_allocator.cc	(.../vendor/tags/4.2.4)	(revision 920)
+++ libstdc++-v3/src/bitmap_allocator.cc	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2,6 +2,8 @@
 
 // Copyright (C) 2004, 2005, 2006 Free Software Foundation, Inc.
 //
+// Copyright (C) 2009 STMicroelectronics
+//
 // This file is part of the GNU ISO C++ Library.  This library is free
 // software; you can redistribute it and/or modify it under the
 // terms of the GNU General Public License as published by the
@@ -53,6 +55,7 @@
   {
 #if defined __GTHREADS
     __mutex_type& __bfl_mutex = _M_get_mutex();
+    __bfl_mutex.lock(); 
 #endif
     const vector_type& __free_list = _M_get_free_list();
     using __gnu_cxx::__detail::__lower_bound;
Index: libstdc++-v3/ChangeLog.STM
===================================================================
--- libstdc++-v3/ChangeLog.STM	(.../vendor/tags/4.2.4)	(revision 0)
+++ libstdc++-v3/ChangeLog.STM	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,31 @@
+2009-02-24  Antony King  <antony.king@st.com>
+	    Christian Bruel  <christian.bruel@st.com>
+
+	INSbl30245:
+	* src/bitmap_allocator.cc (_M_get): lock mutex.
+
+2009-02-24  Antony King  <antony.king@st.com>
+	    Christian Bruel  <christian.bruel@st.com>
+
+	INSbl28513:
+	* include/ext/concurrence.h (__scoped_gmutex_lock): Defined.
+	(__mutex:_M_init): Declare and initialize.
+	(__recursive_mutex:_M_init): Idem.
+	(__mutex:lock): Initialize mutex if needed.
+	(__recursive_mutex:lock): Idem.
+	* libsupc++/eh_globals.cc (__cxa_get_globals): Initialize eh_globals.
+	(__eh_globals_init:_M_create): New function
+	(__eh_globals_init): Initialize _M_once.
+	(__cxa_get_globals): Call init_create once.
+	
+2007-03-23  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	INSbl27235:
+	* include/bits/concurence.h (__glibcxx_mutex_define_initialized): Add
+	attribute init_priority to ensure the mutex iniitialised before use.
+
+2006-05-12  Antony King <anthony.king@st.com>
+            J"orn Rennecke <joern.rennecke@st.com>
+
+	* include/Makefile.am (stamp-host): Remove CCODECVT_H line.
+	* include/Makefile.in: Regenerate.
Index: libstdc++-v3/include/ext/concurrence.h
===================================================================
--- libstdc++-v3/include/ext/concurrence.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ libstdc++-v3/include/ext/concurrence.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -3,6 +3,8 @@
 // Copyright (C) 2003, 2004, 2005, 2006
 // Free Software Foundation, Inc.
 //
+// Copyright (C) 2009 STMicroelectronics
+//
 // This file is part of the GNU ISO C++ Library.  This library is free
 // software; you can redistribute it and/or modify it under the
 // terms of the GNU General Public License as published by the
@@ -104,16 +106,71 @@
 #endif
   }
 
+#if __GTHREADS
+  class __mutex_type
+  {
+  public:
+    __gthread_mutex_t _M_mutex;
+    __gthread_once_t  _M_once;
+
+    __mutex_type() : _M_once(__GTHREAD_ONCE_INIT) { }
+
+    ~__mutex_type() { }
+
+    void
+    _M_create()
+    {
+#if defined __GTHREAD_MUTEX_INIT
+      __gthread_mutex_t __tmp = __GTHREAD_MUTEX_INIT;
+      _M_mutex = __tmp;
+#else
+      __GTHREAD_MUTEX_INIT_FUNCTION(&_M_mutex);
+#endif
+    }
+  };
+
+  __mutex_type _M_device __attribute__ ((weak));
+
+  class __scoped_gmutex_lock
+  {
+  private:
+    __scoped_gmutex_lock(const __scoped_gmutex_lock&);
+    __scoped_gmutex_lock& operator=(const __scoped_gmutex_lock&);
+
+    static void
+    __M_device_create()
+    { _M_device._M_create(); }
+
+  public:
+    explicit __scoped_gmutex_lock()
+    {
+      // Do not need to check __gthread_active_p() as assume already
+      // checked before a sentry is created.
+      __gthread_once(&_M_device._M_once, __M_device_create);
+
+      if (__gthread_mutex_lock(&_M_device._M_mutex) != 0)
+	__throw_concurrence_lock_error();
+    }
+
+    ~__scoped_gmutex_lock() throw()
+    {
+      if (__gthread_mutex_unlock(&_M_device._M_mutex) != 0)
+	__throw_concurrence_unlock_error();
+    }
+  };
+#endif
+
   class __mutex 
   {
   private:
     __gthread_mutex_t _M_mutex;
+    bool	      _M_init;
 
     __mutex(const __mutex&);
     __mutex& operator=(const __mutex&);
 
   public:
-    __mutex() 
+    __mutex() : _M_init(false)
     { 
 #if __GTHREADS
       if (__gthread_active_p())
@@ -124,6 +181,7 @@
 #else
 	  __GTHREAD_MUTEX_INIT_FUNCTION(&_M_mutex); 
 #endif
+          _M_init = true;
 	}
 #endif 
     }
@@ -133,6 +191,21 @@
 #if __GTHREADS
       if (__gthread_active_p())
 	{
+	  if (__builtin_expect(_M_init == false, false))
+	    {
+	      __scoped_gmutex_lock sentry;
+	      if (_M_init == false)
+		{
+#if defined __GTHREAD_MUTEX_INIT
+		  __gthread_mutex_t __tmp = __GTHREAD_MUTEX_INIT;
+		  _M_mutex = __tmp;
+#else
+		  __GTHREAD_MUTEX_INIT_FUNCTION(&_M_mutex);
+#endif
+		  _M_init = true;
+		}
+	    }
+
 	  if (__gthread_mutex_lock(&_M_mutex) != 0)
 	    __throw_concurrence_lock_error();
 	}
@@ -155,12 +228,13 @@
   {
   private:
     __gthread_recursive_mutex_t _M_mutex;
+    bool			_M_init;
 
     __recursive_mutex(const __recursive_mutex&);
     __recursive_mutex& operator=(const __recursive_mutex&);
 
   public:
-    __recursive_mutex() 
+    __recursive_mutex() : _M_init(false)
     { 
 #if __GTHREADS
       if (__gthread_active_p())
@@ -171,6 +245,7 @@
 #else
 	  __GTHREAD_RECURSIVE_MUTEX_INIT_FUNCTION(&_M_mutex); 
 #endif
+          _M_init = true;
 	}
 #endif 
     }
@@ -180,6 +255,22 @@
 #if __GTHREADS
       if (__gthread_active_p())
 	{
+	  if (__builtin_expect(_M_init == false, false))
+	    {
+	      __scoped_gmutex_lock sentry;
+	      if (_M_init == false)
+		{
+#if defined __GTHREAD_RECURSIVE_MUTEX_INIT
+		  __gthread_recursive_mutex_t __tmp =
+		    __GTHREAD_RECURSIVE_MUTEX_INIT;
+		  _M_mutex = __tmp;
+#else
+		  __GTHREAD_RECURSIVE_MUTEX_INIT_FUNCTION(&_M_mutex);
+#endif
+		  _M_init = true;
+		}
+	    }
+
 	  if (__gthread_recursive_mutex_lock(&_M_mutex) != 0)
 	    __throw_concurrence_lock_error();
 	}
Index: libstdc++-v3/include/bits/concurrence.h
===================================================================
--- libstdc++-v3/include/bits/concurrence.h	(.../vendor/tags/4.2.4)	(revision 0)
+++ libstdc++-v3/include/bits/concurrence.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,102 @@
+// Support for concurrent programing -*- C++ -*-
+
+// Copyright (C) 2003, 2004
+// Free Software Foundation, Inc.
+//
+// Copyright (C) 2007 STMicroelectronics
+//
+// This file is part of the GNU ISO C++ Library.  This library is free
+// software; you can redistribute it and/or modify it under the
+// terms of the GNU General Public License as published by the
+// Free Software Foundation; either version 2, or (at your option)
+// any later version.
+
+// This library is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+// GNU General Public License for more details.
+
+// You should have received a copy of the GNU General Public License along
+// with this library; see the file COPYING.  If not, write to the Free
+// Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301,
+// USA.
+
+// As a special exception, you may use this file as part of a free software
+// library without restriction.  Specifically, if other files instantiate
+// templates or use macros or inline functions from this file, or you compile
+// this file and link it with other files to produce an executable, this
+// file does not by itself cause the resulting executable to be covered by
+// the GNU General Public License.  This exception does not however
+// invalidate any other reasons why the executable file might be covered by
+// the GNU General Public License.
+
+/** @file concurrence.h
+ *  This is an internal header file, included by other library headers.
+ *  You should not attempt to use it directly.
+ */
+
+#ifndef _CONCURRENCE_H
+#define _CONCURRENCE_H 1
+
+// GCC's thread abstraction layer
+#include "bits/gthr.h"
+
+#if __GTHREADS
+
+# ifdef __GTHREAD_MUTEX_INIT
+#  define __glibcxx_mutex_type __gthread_mutex_t
+#  define __glibcxx_mutex_define_initialized(NAME) \
+__gthread_mutex_t NAME = __GTHREAD_MUTEX_INIT
+#  define __glibcxx_mutex_lock(NAME) \
+__gthread_mutex_lock(&NAME)
+# else
+// Implies __GTHREAD_MUTEX_INIT_FUNCTION
+struct __glibcxx_mutex : public __gthread_mutex_t
+{
+   __glibcxx_mutex() { __GTHREAD_MUTEX_INIT_FUNCTION(this); }
+};
+
+#  define __glibcxx_mutex_type __glibcxx_mutex
+#  define __glibcxx_mutex_define_initialized(NAME) \
+__glibcxx_mutex NAME __attribute__((init_priority(30000)))
+# define __glibcxx_mutex_lock(NAME) \
+__gthread_mutex_lock(&NAME)
+# endif
+
+# define __glibcxx_mutex_unlock(NAME) __gthread_mutex_unlock(&NAME)
+
+#else
+
+# define __glibcxx_mutex_type __gthread_mutex_t
+# define __glibcxx_mutex_define_initialized(NAME) __gthread_mutex_t NAME
+# define __glibcxx_mutex_lock(NAME)
+# define __glibcxx_mutex_unlock(NAME)
+
+#endif
+
+namespace __gnu_cxx
+{
+  typedef __glibcxx_mutex_type mutex_type;
+  
+  /// @brief  Scoped lock idiom.
+  // Acquire the mutex here with a constructor call, then release with
+  // the destructor call in accordance with RAII style.
+  class lock
+  {
+    // Externally defined and initialized.
+    mutex_type& device;
+
+  public:
+    explicit lock(mutex_type& name) : device(name)
+    { __glibcxx_mutex_lock(device); }
+
+    ~lock() throw()
+    { __glibcxx_mutex_unlock(device); }
+
+  private:
+    lock(const lock&);
+    lock& operator=(const lock&);
+  };
+}
+
+#endif
Index: libstdc++-v3/include/Makefile.in
===================================================================
--- libstdc++-v3/include/Makefile.in	(.../vendor/tags/4.2.4)	(revision 920)
+++ libstdc++-v3/include/Makefile.in	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1337,6 +1337,7 @@
 
 # Host includes static.
 # XXX Missing dependency info for {host_headers_extra}
+# CCODECVT_H is no longer defined; we don't want to symlink / copy the netire directory!
 stamp-host: ${host_headers} ${host_headers_noinst} stamp-${host_alias}
 	@if [ ! -f stamp-host ]; then \
 	  (cd ${host_builddir} ;\
Index: libstdc++-v3/include/Makefile.am
===================================================================
--- libstdc++-v3/include/Makefile.am	(.../vendor/tags/4.2.4)	(revision 920)
+++ libstdc++-v3/include/Makefile.am	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -954,6 +954,7 @@
 
 # Host includes static.
 # XXX Missing dependency info for {host_headers_extra}
+# CCODECVT_H is no longer defined; we don't want to symlink / copy the netire directory!
 stamp-host: ${host_headers} ${host_headers_noinst} stamp-${host_alias}
 	@if [ ! -f stamp-host ]; then \
 	  (cd ${host_builddir} ;\
Index: libstdc++-v3/libsupc++/eh_globals.cc
===================================================================
--- libstdc++-v3/libsupc++/eh_globals.cc	(.../vendor/tags/4.2.4)	(revision 920)
+++ libstdc++-v3/libsupc++/eh_globals.cc	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2,6 +2,8 @@
 // Copyright (C) 2001, 2002, 2003, 2004, 2005, 2006
 // Free Software Foundation, Inc.
 //
+// Copyright (c) 2009  STMicroelectronics.
+//
 // This file is part of GCC.
 //
 // GCC is free software; you can redistribute it and/or modify
@@ -34,6 +36,7 @@
 #include "cxxabi.h"
 #include "unwind-cxx.h"
 #include "bits/gthr.h"
+#include <ext/concurrence.h>
 
 #if _GLIBCXX_HOSTED
 using std::free;
@@ -67,7 +70,6 @@
 __cxxabiv1::__cxa_get_globals() throw()
 { return get_global(); }
 
-
 #else
 
 // Single-threaded fallback buffer.
@@ -97,12 +99,10 @@
 {
   __gthread_key_t  	_M_key;
   bool 			_M_init;
+  __gthread_once_t	_M_once;
 
-  __eh_globals_init() : _M_init(false)
-  { 
-    if (__gthread_active_p())
-      _M_init = __gthread_key_create(&_M_key, eh_globals_dtor) == 0; 
-  }
+  __eh_globals_init() : _M_init(false), _M_once(__GTHREAD_ONCE_INIT)
+  { }
 
   ~__eh_globals_init()
   {
@@ -110,14 +110,23 @@
       __gthread_key_delete(_M_key);
     _M_init = false;
   }
+
+  inline void
+  _M_create()
+  { _M_init = __gthread_key_create(&_M_key, eh_globals_dtor) == 0; }
 };
 
 static __eh_globals_init init;
 
+static void
+init_create()
+{ init._M_create(); }
+
 extern "C" __cxa_eh_globals*
 __cxxabiv1::__cxa_get_globals_fast() throw()
 {
   __cxa_eh_globals* g;
+
   if (init._M_init)
     g = static_cast<__cxa_eh_globals*>(__gthread_getspecific(init._M_key));
   else
@@ -129,6 +138,11 @@
 __cxxabiv1::__cxa_get_globals() throw()
 {
   __cxa_eh_globals* g;
+
+  if (__builtin_expect(init._M_init == false, false)
+      && __gthread_active_p())
+    __gthread_once(&init._M_once, init_create);
+
   if (init._M_init)
     {
       g = static_cast<__cxa_eh_globals*>(__gthread_getspecific(init._M_key));
Index: libstdc++-v3/testsuite/lib/libstdc++.exp
===================================================================
--- libstdc++-v3/testsuite/lib/libstdc++.exp	(.../vendor/tags/4.2.4)	(revision 920)
+++ libstdc++-v3/testsuite/lib/libstdc++.exp	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -381,12 +381,17 @@
 	set version [exec ${cc} -dumpversion]
 	set machine [exec ${cc} -dumpmachine]
 	set comp_base_dir [file dirname [file dirname [file dirname [file dirname [file dirname [exec ${cc} --print-prog-name=cc1]]]]]]
+	if { ![file exists "${comp_base_dir}/${machine}"] } {
 	set includesbase "${comp_base_dir}/include/c++/${version}"
 	set includestarget "${includesbase}/${machine}"
-	set cc_final [concat $cc_final "-I$includesbase -I$includestarget"]
-
         set libsup "${comp_base_dir}/lib"
     } else {
+	    set includesbase "${comp_base_dir}/${machine}/include/c++/${version}"
+	    set includestarget "${includesbase}/${machine}"
+	    set libsup "${comp_base_dir}/${machine}/lib"
+	}
+	set cc_final [concat $cc_final "-I$includesbase -I$includestarget"]
+    } else {
         set libsup "${blddir}/libsupc++/.libs"
     }
 
Index: LAST_UPDATED
===================================================================
--- LAST_UPDATED	(.../vendor/tags/4.2.4)	(revision 920)
+++ LAST_UPDATED	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1 +1,2 @@
-Obtained from SVN: tags/gcc_4_2_4_release revision 135573
+Wed Feb 25 08:27:18 CET 2009
+Wed Feb 25 07:27:18 UTC 2009 (revision 808M)
Index: include/ChangeLog.STM
===================================================================
--- include/ChangeLog.STM	(.../vendor/tags/4.2.4)	(revision 0)
+++ include/ChangeLog.STM	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,8 @@
+2006-05-03  Andrew Stubbs  <andrew.stubbs@st.com>
+            J"orn Rennecke <joern.rennecke@st.com>
+
+	* libiberty.h (make_relative_prefix_ignore_links): Declare.
+
+2006-03-27  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* libiberty.h: Add support for cygpath.c.
Index: include/libiberty.h
===================================================================
--- include/libiberty.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ include/libiberty.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,6 +1,7 @@
 /* Function declarations for libiberty.
 
    Copyright 2001, 2002, 2005 Free Software Foundation, Inc.
+   Copyright (C) 2006 STMicroelectronics
    
    Note - certain prototypes declared in this header file are for
    functions whoes implementation copyright does not belong to the
@@ -197,6 +198,13 @@
 extern char *make_relative_prefix (const char *, const char *,
                                    const char *) ATTRIBUTE_MALLOC;
 
+/* Generate a relocated path to some installation directory without
+   attempting to follow any soft links.  Allocates
+   return value using malloc.  */
+
+extern char *make_relative_prefix_ignore_links (const char *, const char *,
+						const char *) ATTRIBUTE_MALLOC;
+
 /* Choose a temporary directory to use for scratch files.  */
 
 extern char *choose_temp_base (void) ATTRIBUTE_MALLOC;
@@ -617,5 +625,28 @@
 }
 #endif
 
+#ifdef __MINGW32__
+char *cygpath (const char *path);
+void cygpath_replace (char **path);
+
+/* The following macros are just to prevent putting #ifdef MINGW everywhere.
+   You still need to if you don't want to overwite the original pointer.  */
+
+/* Reassign the pointer PATH without freeing anything.  */
+#define CYGPATH(path) do {path = cygpath (path);} while(0)
+
+/* Free memory. Intended to be used in conjunction with CYGPATH().  */
+#define CYGPATH_FREE(path) free (path)
+
+/* Reassign the pointer PATH and free the previous content.  */
+#define CYGPATH_REPLACE(path) cygpath_replace (path)
+
+#else
+/* If these were properly empty statements then there might be warnings
+   which would kill a -Werror build.  */
+#define CYGPATH(path) do {} while (0)
+#define CYGPATH_FREE(path) do {} while (0)
+#define CYGPATH_REPLACE(path) do {} while (0)
+#endif
 
 #endif /* ! defined (LIBIBERTY_H) */
Index: libiberty/configure.ac
===================================================================
--- libiberty/configure.ac	(.../vendor/tags/4.2.4)	(revision 920)
+++ libiberty/configure.ac	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -182,6 +182,7 @@
   *-*-freebsd2.2.[[012]])	frag=mh-fbsd21 ;;
   i370-*-opened*)       frag=mh-openedition ;;
   i[[34567]]86-*-windows*)	frag=mh-windows ;;
+  *-*-mingw*)		frag=mh-mingw ;;
 esac
 
 if [[ -n "${frag}" ]]; then
Index: libiberty/make-relative-prefix.c
===================================================================
--- libiberty/make-relative-prefix.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ libiberty/make-relative-prefix.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,6 +1,7 @@
 /* Relative (relocatable) prefix support.
    Copyright (C) 1987, 1989, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of libiberty.
 
@@ -217,9 +218,9 @@
 
    If no relative prefix can be found, return NULL.  */
 
-char *
-make_relative_prefix (const char *progname,
-                      const char *bin_prefix, const char *prefix)
+static char *
+make_relative_prefix_1 (const char *progname, const char *bin_prefix,
+			const char *prefix, const int resolve_links)
 {
   char **prog_dirs, **bin_dirs, **prefix_dirs;
   int prog_num, bin_num, prefix_num;
@@ -289,9 +290,14 @@
 	}
     }
 
+  if ( resolve_links )
+    {
   full_progname = lrealpath (progname);
   if (full_progname == NULL)
     return NULL;
+    }
+  else
+    full_progname = strdup(progname);
 
   prog_dirs = split_directories (full_progname, &prog_num);
   bin_dirs = split_directories (bin_prefix, &bin_num);
@@ -387,3 +393,33 @@
 
   return ret;
 }
+
+
+/* Do the full job, including symlink resolution.
+   This path will find files installed in the same place as the
+   program even when a soft link has been made to the program
+   from somwhere else. */
+
+char *
+make_relative_prefix (progname, bin_prefix, prefix)
+     const char *progname;
+     const char *bin_prefix;
+     const char *prefix;
+{
+  return make_relative_prefix_1 (progname, bin_prefix, prefix, 1);
+}
+
+/* Make the relative pathname without attempting to resolve any links.
+   '..' etc may also be left in the pathname.
+   This will find the files the user meant the program to find if the
+   installation is patched together with soft links. */
+
+char *
+make_relative_prefix_ignore_links (progname, bin_prefix, prefix)
+     const char *progname;
+     const char *bin_prefix;
+     const char *prefix;
+{
+  return make_relative_prefix_1 (progname, bin_prefix, prefix, 0);
+}
+
Index: libiberty/cygpath.c
===================================================================
--- libiberty/cygpath.c	(.../vendor/tags/4.2.4)	(revision 0)
+++ libiberty/cygpath.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,358 @@
+/* Basic Cygwin pathname support for MinGW.
+
+   Copyright (C) 2006 STMicroelectronics
+
+   This file is part of the libiberty library.
+
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2 of the License, or
+   (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 51 Franklin Street - Fifth Floor,
+   Boston, MA 02110-1301, USA.
+
+
+   This file implements a limited amount of support for Cygwin paths.
+   It is intended for use by MinGW programs that must interact with Cygwin.
+
+   It is limited to absolute paths only.  I.e. Those beginning with Cygwin
+   mounts, such as /cygdrive/...  See the comment on cygpath() below.  */
+
+#include "libiberty.h"
+#include <string.h>
+#include <ctype.h>
+#include <windows.h>
+
+
+/* These are all the possible settings for the ST_CYGPATH_MODE
+   environment variable.  */
+static enum
+{
+  mode_unset,
+  mode_off,
+  mode_normal,
+  mode_full
+} mode = mode_unset;
+
+
+/* These are the values extracted from the registry.
+   They are extracted the first time cygpath is called.  */
+static const char *cygdrive = NULL;
+static struct mount
+{
+  /* The name of the Cygwin mount point.  E.g. "/usr/bin"  */
+  char *mount;
+
+  /* The actual Windows path that the mount translates to.  */
+  char *actual;
+
+  struct mount *next;
+} *mounts = NULL;
+
+
+/* Read a string from the Windows Registry.
+   KEY should be a valid handle from RegOpenKeyEx().
+   NAME should be the name of the value within the key.
+   The value should be of type REG_SZ.
+   If the value does not exist, is of the wrong typei, or another error
+   occurs, then NULL is returned.
+   Otherwise a malloced string is returned.  */
+static char *
+read_string_from_registry (HKEY key, const char *name)
+{
+  DWORD valuetype = REG_NONE;
+  DWORD valuesize = 0;
+  char *value = NULL;
+
+  if (RegQueryValueEx (key, name, NULL, &valuetype,
+		       NULL, &valuesize) == ERROR_SUCCESS
+      && valuetype == REG_SZ)
+    {
+      value = xmalloc (valuesize);
+      if (RegQueryValueEx (key, name, NULL, &valuetype, (unsigned char *)value,
+			   &valuesize) != ERROR_SUCCESS)
+	{
+	  free (value);
+	  value = NULL;
+	}
+    }
+
+  return value;
+}
+
+
+/* Fill in the mounts list (mounts is defined statically above).
+   All subkeys (not values) of KEY that contain a REG_SZ value named 'native'
+   are added to the start of the mounts list.  */
+static void
+read_mounts (HKEY key)
+{
+  int mountsize = 15;
+  char *mount = xmalloc (mountsize);
+  DWORD size = mountsize;
+  int index = 0;
+  int retval = 0;
+
+  /* For each subkey ...  */
+  while ((retval = RegEnumKeyEx (key, index, mount, &size, 0, NULL, 0, NULL))
+	 != ERROR_NO_MORE_ITEMS)
+    {
+      struct mount *newmount;
+      HKEY subkey;
+      char *actual;
+
+      switch (retval) {
+      case ERROR_MORE_DATA:
+	/* The buffer wasn't large enough for this key name.
+	   Unlike RegQueryValueEx, RegEnumKeyEx won't tell us how big it
+	   should be, so just make it bigger and try again.
+	   Note that this code path does NOT increment index.
+       	   Most of the time we will only be dealing with short strings.  */
+	mountsize += 10;
+	mount = xrealloc (mount, mountsize);
+	break;
+
+      case ERROR_SUCCESS:
+	/* Find the actual windows path.  */
+  	if (RegOpenKeyEx (key, mount, 0, KEY_READ, &subkey) != ERROR_SUCCESS)
+	  {
+	    index++;
+	    break;
+	  }
+	actual = read_string_from_registry (subkey, "native");	
+	RegCloseKey (subkey);
+	if (actual == NULL)
+	  {
+	    index++;
+	    break;
+	  }
+
+	/* Create the new entry in the mount table.  */
+	newmount = xmalloc (sizeof (struct mount));
+	newmount->mount = xstrdup (mount);
+	newmount->actual = actual;
+	newmount->next = mounts;
+	mounts = newmount;
+	index++;
+	break;
+
+      default:
+	/* Don't infinite loop should any other return value occur.  */
+        index++;
+      }
+
+      /* The last call to RegEnumKeyEx may have clobbered size.
+         Fix it before the next call.  */
+      size = mountsize;
+    }
+
+  free (mount);
+}
+
+
+/* The top level registry reading function.
+   Open the keys, call the above functions to get the right values,
+   and clean up.  */
+static void
+read_registry (void)
+{
+  HKEY hcu_key, hlm_key;
+
+  /* Get key handles for the two places cygwin keeps its registry data.  */
+  if (RegOpenKeyEx (HKEY_CURRENT_USER,
+		    "Software\\Cygnus Solutions\\Cygwin\\mounts v2",
+		    0, KEY_READ, &hcu_key) != ERROR_SUCCESS)
+    hcu_key = NULL;
+
+  if (RegOpenKeyEx (HKEY_LOCAL_MACHINE,
+		    "SOFTWARE\\Cygnus Solutions\\Cygwin\\mounts v2",
+		    0, KEY_READ, &hlm_key) != ERROR_SUCCESS)
+    hlm_key = NULL;
+
+  /* Get the virtual mount point used for windows drives.  */
+  if (hcu_key)
+    cygdrive = read_string_from_registry (hcu_key, "cygdrive prefix");
+  if (hlm_key && cygdrive == NULL)
+    cygdrive = read_string_from_registry (hlm_key, "cygdrive prefix");
+
+  /* Read the other mount points.
+     Read hlm before hcu to ensure hcu settings get used by preference
+     by being closer on the mounts stack.  */
+  if (hlm_key)
+    read_mounts (hlm_key);
+  if (hcu_key)
+    read_mounts (hcu_key);
+
+  if (hlm_key)
+    RegCloseKey (hlm_key);
+  if (hcu_key)
+    RegCloseKey (hcu_key);
+}
+
+
+/* Given a path of unknown variety, return the same path with any
+   Cygwin mount points substituted.
+   This function always returns a malloced string which should be
+   freed when the the caller is finished with it.
+
+   The mapping is affected by the ST_CYGPATH_MODE environment variable.
+   See the fprintf messages below for full information.
+
+   It can replace /cygdrive/<letter>/..... style pathnames, even if the
+   user has used 'mount -c' to an alternative string.
+
+   It can replace (if enabled) other Cygwin mount points, such as
+   the usual '/', '/usr/bin', '/usr/lib', as well as any other user defined
+   mount points.
+
+   It does NOT attempt to convert any pathnames that look like native Windows
+   names - such as those starting with '<letter>:' or double slash (UNC).
+
+   It does NOT handle relative pathnames passing through cygwin mounts
+   (e.g. '../cygdrive/c'), or absolute paths with repeated directory
+   separators or relative elements within the mount name
+   (e.g. '/usr/./bin').
+   
+   It does NOT allow backslash \ directory separators within the actual mount
+   path (e.g. '/usr\bin').  Cygwin does not always allow them there either.  */
+char *
+cygpath (const char *path)
+{
+  char *result = NULL;
+
+  if (path == NULL)
+    return NULL;
+
+  /* If this is the first time this function has been called then read the
+     environment and registry.  */
+  if (mode == mode_unset)
+    {
+      char *env = getenv ("ST_CYGPATH_MODE");
+
+      if (env == NULL || strcmp (env, "normal") == 0)
+    	mode = mode_normal;
+      else if (strcmp (env, "full") == 0)
+	mode = mode_full;
+      else if (strcmp (env, "off") == 0)
+	mode = mode_off;
+
+      if (mode != mode_off)
+	read_registry();
+
+      if (mode == mode_unset)
+	{
+	  /* The variable was set, but not to any known value.
+	     Set up a default and print an informational message
+	     for the user.  */
+	  mode = mode_normal;
+	  fprintf (stderr, "ST_CYGPATH_MODE should be one of:\n");
+	  fprintf (stderr, " off    - Disable all path translation.\n");
+	  fprintf (stderr, " normal - Translate %s only.\n", cygdrive);
+	  fprintf (stderr, " full   - Translate all Cygwin mounts.\n");
+	}
+    }
+
+  /* First, test if this can only be a windows (non-cygwin) path.
+     This includes paths that start with a drive letter or UNC double slash.  */
+  if ((isalpha (path[0]) && path[1] == ':')
+      || ((path[0] == '\\' || path[0] == '/')
+	  && (path[1] == '\\' || path[1] == '/')))
+    result = xstrdup (path);
+
+  /* Second, handle /cygdrive/<letter>/ (or whatever) paths.  */
+  if (!result && cygdrive != NULL && (mode == mode_normal || mode == mode_full))
+    {
+      int length = strlen (cygdrive);
+      /* Note that cygwin does not allow '\\' instead of '/' in cygdrive.  */
+      if (strncmp (cygdrive, path, length) == 0
+	  && (path[length] == '/' || path[length] == '\\'
+	      || path[length] == '\0')
+	  && isalpha (path[length+1]))
+        {
+	  result = xmalloc (strlen (path) - length+1 + 1);
+	  result[0] = path[length+1];
+	  result[1] = ':';
+	  strcpy (result + 2, path + length + 2);
+	}
+    }
+
+  /* Third, handle other types of cygwin path.  */
+  if (!result && mounts != NULL && mode == mode_full)
+    {
+      int matched = 0;
+      struct mount *foundat = NULL;
+      struct mount *mount = mounts;
+      /* Find the longest matching mount point.
+	 This is important. If we just used the first matching mount point
+	 it would probably always match '/' when '/usr/bin' is right.
+	 Use the first of equal length matches - this allows current-user
+	 mounts to override 'local machine' mounts (can this happen?).
+         It is a match only if the matching part is followed by a directory
+         separator or the end of the path, except for the root mount point.  */
+      while (mount != NULL)
+	{
+	  int length = strlen (mount->mount);
+	  if (strncmp (mount->mount, path, length) == 0
+	      && matched < length
+	      && (length == 1 /* Special case for root mount point '/'.  */
+		  || path[length] == '/' || path[length] == '\\'
+		  || path[length] == '\0'))
+	    {
+	      matched = length;
+	      foundat = mount;
+	    }
+	  mount = mount->next;
+	}
+      if (matched)
+	{
+	  /* There was a match so do the substitution.
+	     If matched is 1 then it can only be the root mount point, in
+	     which case we do not want to remove the matched part as it is the 
+	     directory separator.  */
+	  if (matched == 1)
+	    matched = 0;
+	  result = xmalloc (strlen (foundat->actual) + strlen (path) + 1
+			    - matched);
+	  strcpy (result, foundat->actual);
+	  strcat (result, path + matched);
+	}
+    }
+
+  if (result)
+    {
+      /* Ensure that the return is never just a drive letter.
+	 This is not a valid directory on Windows, but code often
+	 trims trailing slashes.  */
+      int length = strlen(result);
+      if (result[length-1] == ':')
+	{
+	  result = xrealloc (result, length+2);
+	  result[length] = '/';
+	  result[length+1] = '\0';
+	}
+      return result;
+    }
+
+  /* If we get here then it must have been some other kind of path.  */
+  return xstrdup (path);
+}
+
+
+/* This is just to make inserting the conversion more convenient.
+   The CYGPATH_REPLACE is conditionally compiled so it is harder to
+   add clean up code to go with it without this.  */
+void
+cygpath_replace (char **path)
+{
+  char *result = cygpath (*path);
+  free (*path);
+  *path = result;
+}
Index: libiberty/Makefile.in
===================================================================
--- libiberty/Makefile.in	(.../vendor/tags/4.2.4)	(revision 920)
+++ libiberty/Makefile.in	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2,6 +2,7 @@
 # Makefile
 #   Copyright (C) 1990, 91-99, 2000, 2001, 2002, 2003, 2004, 2005, 2006
 #   Free Software Foundation
+#   Copyright (C) 2006 STMicroelectronics
 #
 # This file is part of the libiberty library.
 # Libiberty is free software; you can redistribute it and/or
@@ -129,7 +130,7 @@
 CFILES = alloca.c argv.c asprintf.c atexit.c				\
 	basename.c bcmp.c bcopy.c bsearch.c bzero.c			\
 	calloc.c choose-temp.c clock.c concat.c cp-demangle.c		\
-	 cp-demint.c cplus-dem.c					\
+	 cp-demint.c cplus-dem.c cygpath.c				\
 	dyn-string.c							\
 	fdmatch.c ffs.c fibheap.c floatformat.c fnmatch.c		\
 	 fopen_unlocked.c						\
@@ -186,7 +187,7 @@
 # maint-missing" and "make check".
 CONFIGURED_OFILES = ./asprintf.o ./atexit.o				\
 	./basename.o ./bcmp.o ./bcopy.o ./bsearch.o ./bzero.o		\
-	./calloc.o ./clock.o ./copysign.o				\
+	./calloc.o ./clock.o ./copysign.o cygpath.o			\
 	./_doprnt.o							\
 	./ffs.o								\
 	./getcwd.o ./getpagesize.o ./gettimeofday.o			\
@@ -607,6 +608,12 @@
 	else true; fi
 	$(COMPILE.c) $(srcdir)/cplus-dem.c $(OUTPUT_OPTION)
 
+./cygpath.o: $(srcdir)/cygpath.c $(INCDIR)/ansidecl.h $(INCDIR)/libiberty.h
+	if [ x"$(PICFLAG)" != x ]; then \
+	  $(COMPILE.c) $(PICFLAG) $(srcdir)/cygpath.c -o pic/$@; \
+	else true; fi
+	$(COMPILE.c) $(srcdir)/cygpath.c $(OUTPUT_OPTION)
+
 ./dyn-string.o: $(srcdir)/dyn-string.c config.h $(INCDIR)/ansidecl.h \
 	$(INCDIR)/dyn-string.h $(INCDIR)/libiberty.h
 	if [ x"$(PICFLAG)" != x ]; then \
Index: libiberty/config/mh-mingw
===================================================================
--- libiberty/config/mh-mingw	(.../vendor/tags/4.2.4)	(revision 0)
+++ libiberty/config/mh-mingw	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1 @@
+EXTRA_OFILES=cygpath.o
Index: libiberty/configure
===================================================================
--- libiberty/configure	(.../vendor/tags/4.2.4)	(revision 920)
+++ libiberty/configure	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -3599,6 +3599,7 @@
   *-*-freebsd2.2.[012])	frag=mh-fbsd21 ;;
   i370-*-opened*)       frag=mh-openedition ;;
   i[34567]86-*-windows*)	frag=mh-windows ;;
+  *-*-mingw*)		frag=mh-mingw ;;
 esac
 
 if [ -n "${frag}" ]; then
Index: libiberty/ChangeLog.STM
===================================================================
--- libiberty/ChangeLog.STM	(.../vendor/tags/4.2.4)	(revision 0)
+++ libiberty/ChangeLog.STM	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,22 @@
+2006-05-15  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* cygpath.c (cygpath): Convert pathnames consisting only of a
+	drive specifier to a valid directory (e.g 'c:' -> 'c:/').
+
+2006-05-03  Andrew Stubbs  <andrew.stubbs@st.com>
+            J"orn Rennecke <joern.rennecke@st.com>
+
+	* make-relative-prefix.c (make_relative_prefix_1): New function,
+	broken out of make_relative_prefix.  Make link resolution dependent
+	on new parameter.
+	(make_relative_prefix): Use make_relative_prefix_1.
+	(make_relative_prefix_ignore_links): New function.
+
+2006-03-27  Andrew Stubbs  <andrew.stubbs@st.com>
+
+libiberty/
+	* cygpath.c: New file.
+	* config/mh-mingw: New file.
+	* configure.ac: Add mh-mingw makefile fragment when host is MinGW.
+	* configure: Regenerate.
+	* Makefile.in: Add cygpath.[co] .
Index: configure.in
===================================================================
--- configure.in	(.../vendor/tags/4.2.4)	(revision 920)
+++ configure.in	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -750,7 +750,7 @@
       i[[3456789]]86-*-msdosdjgpp*) ;; # don't add gprof back in
       *) skipdirs=`echo " ${skipdirs} " | sed -e 's/ gprof / /'` ;;
     esac
-    noconfigdirs="$noconfigdirs target-libgloss ${libgcj}"
+    noconfigdirs="$noconfigdirs ${libgcj}"
     ;;
   sparc-*-elf*)
     noconfigdirs="$noconfigdirs ${libgcj}"
@@ -1660,12 +1660,19 @@
     ;;
 esac
 
-alphaieee_frag=/dev/null
+ieee_frag=/dev/null
 case $target in
-  alpha*-*-*)
+  alpha*-*-* | sh*-*-*)
     # This just makes sure to use the -mieee option to build target libs.
     # This should probably be set individually by each library.
-    alphaieee_frag="config/mt-alphaieee"
+    ieee_frag="config/mt-ieee"
+    ;;
+esac
+
+relax_frag=/dev/null
+case $target in
+  sh-superh-elf)
+    relax_frag="config/mt-relax"
     ;;
 esac
 
@@ -2170,7 +2177,7 @@
 esac
 
 # Makefile fragments.
-for frag in host_makefile_frag target_makefile_frag alphaieee_frag ospace_frag;
+for frag in host_makefile_frag target_makefile_frag ieee_frag ospace_frag relax_frag;
 do
   eval fragval=\$$frag
   if test $fragval != /dev/null; then
@@ -2179,8 +2186,9 @@
 done
 AC_SUBST_FILE(host_makefile_frag)
 AC_SUBST_FILE(target_makefile_frag)
-AC_SUBST_FILE(alphaieee_frag)
+AC_SUBST_FILE(ieee_frag)
 AC_SUBST_FILE(ospace_frag)
+AC_SUBST_FILE(relax_frag)
 
 # Miscellanea: directories, flags, etc.
 AC_SUBST(RPATH_ENVVAR)
Index: config/mt-alphaieee
===================================================================
--- config/mt-alphaieee	(.../vendor/tags/4.2.4)	(revision 920)
+++ config/mt-alphaieee	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,2 +0,0 @@
-CFLAGS_FOR_TARGET += -mieee
-CXXFLAGS_FOR_TARGET += -mieee
Index: config/mh-interix
===================================================================
Index: config/warnings.m4
===================================================================
Index: config/gettext-sister.m4
===================================================================
Index: config/depstand.m4
===================================================================
Index: config/gxx-include-dir.m4
===================================================================
Index: config/mt-ieee
===================================================================
--- config/mt-ieee	(.../vendor/tags/4.2.4)	(revision 0)
+++ config/mt-ieee	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,2 @@
+CFLAGS_FOR_TARGET += -mieee
+CXXFLAGS_FOR_TARGET += -mieee
Index: config/mt-relax
===================================================================
--- config/mt-relax	(.../vendor/tags/4.2.4)	(revision 0)
+++ config/mt-relax	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,2 @@
+CFLAGS_FOR_TARGET += -mrelax
+# CXXFLAGS_FOR_TARGET += -mrelax
Index: config/mt-ospace
===================================================================
--- config/mt-ospace	(.../vendor/tags/4.2.4)	(revision 920)
+++ config/mt-ospace	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,3 +1,3 @@
 # Build libraries optimizing for space, not speed.
- CFLAGS_FOR_TARGET = -g -Os
- CXXFLAGS_FOR_TARGET = -g -Os
+CFLAGS_FOR_TARGET += -g -Os
+CXXFLAGS_FOR_TARGET += -g -Os
Index: config/acx.m4
===================================================================
Index: config/mh-decstation
===================================================================
Index: config/acinclude.m4
===================================================================
Index: configure
===================================================================
--- configure	(.../vendor/tags/4.2.4)	(revision 920)
+++ configure	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1590,7 +1590,7 @@
       i[3456789]86-*-msdosdjgpp*) ;; # don't add gprof back in
       *) skipdirs=`echo " ${skipdirs} " | sed -e 's/ gprof / /'` ;;
     esac
-    noconfigdirs="$noconfigdirs target-libgloss ${libgcj}"
+    noconfigdirs="$noconfigdirs ${libgcj}"
     ;;
   sparc-*-elf*)
     noconfigdirs="$noconfigdirs ${libgcj}"
@@ -2947,12 +2947,19 @@
     ;;
 esac
 
-alphaieee_frag=/dev/null
+ieee_frag=/dev/null
 case $target in
-  alpha*-*-*)
+  alpha*-*-* | sh*-*-*)
     # This just makes sure to use the -mieee option to build target libs.
     # This should probably be set individually by each library.
-    alphaieee_frag="config/mt-alphaieee"
+    ieee_frag="config/mt-ieee"
+    ;;
+esac
+
+relax_frag=/dev/null
+case $target in
+  sh-superh-elf)
+    relax_frag="config/mt-relax"
     ;;
 esac
 
@@ -3463,7 +3470,7 @@
 esac
 
 # Makefile fragments.
-for frag in host_makefile_frag target_makefile_frag alphaieee_frag ospace_frag;
+for frag in host_makefile_frag target_makefile_frag ieee_frag ospace_frag relax_frag;
 do
   eval fragval=\$$frag
   if test $fragval != /dev/null; then
@@ -7642,10 +7649,12 @@
 s%@host_makefile_frag@%%g
 /@target_makefile_frag@/r $target_makefile_frag
 s%@target_makefile_frag@%%g
-/@alphaieee_frag@/r $alphaieee_frag
-s%@alphaieee_frag@%%g
+/@ieee_frag@/r $ieee_frag
+s%@ieee_frag@%%g
 /@ospace_frag@/r $ospace_frag
 s%@ospace_frag@%%g
+/@relax_frag@/r $relax_frag
+s%@relax_frag@%%g
 s%@RPATH_ENVVAR@%$RPATH_ENVVAR%g
 s%@tooldir@%$tooldir%g
 s%@build_tooldir@%$build_tooldir%g
Index: ChangeLog.STM
===================================================================
--- ChangeLog.STM	(.../vendor/tags/4.2.4)	(revision 0)
+++ ChangeLog.STM	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,28 @@
+2009-02-03  Christian Bruel  <christian.bruel@st.com>
+
+	* config/mt-ospace: Don't overwrite CFLAGS_FOR_TARGET.
+	* config/mt-relax: Not supported for c++.
+
+2008-05-26  Christian Bruel  <christian.bruel@st.com>
+
+	* config/mt-relax: New file.
+	* Makefile.in: Add relax fragment.
+	* Makefile.tpl: Likewise
+	* configure.in: Likewise.
+	* configure: Regenerate.
+
+2008-09-30  Christian Bruel  <christian.bruel@st.com>
+
+	* configure.in: Allow libgloss configure for sh.
+	* configure: Regenerate.
+
+2008-05-26  Christian Bruel  <christian.bruel@st.com>
+
+	* config/mt-alphaieee: Removed.
+	* config/mt-ieee: Renamed from mt-alphaieee.
+	* Makefile.in: alphaieee_frag renamed ieee_frag.
+	ieee_frag must be included after ospace_frag.
+	* Makefile.tpl: Likewise
+	* configure.in: Likewise. Enable for sh.
+	* configure: Regenerate.
+
Index: gcc/doc/tm.texi
===================================================================
--- gcc/doc/tm.texi	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/doc/tm.texi	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2102,6 +2102,16 @@
 @code{CCmode} is incomplete.
 @end defmac
 
+@deftypefn {Target Hook} int TARGET_MATCH_ADJUST (rtx @var{x}, int @var{regno})
+@var{regno} contains the register number of the first hard register in @var{x}.
+Return a register number suitable to test if two registers are considered
+matching for the purposes of a matching constraint.
+This is neeed to get useful results when matching registers of different
+sizes for big endian targets.
+The default is to point to the least significant hard register for
+scalar integer modes, but to the first register for any other mode.
+@end deftypefn
+
 @node Leaf Functions
 @subsection Handling Leaf Functions
 
@@ -8766,10 +8776,10 @@
 @code{num_modes_for_mode_switching[@var{entity}] - 1}.
 @end defmac
 
-@defmac EMIT_MODE_SET (@var{entity}, @var{mode}, @var{hard_regs_live})
+@defmac EMIT_MODE_SET (@var{entity}, @var{mode}, @var{flip}, @var{hard_regs_live})
 Generate one or more insns to set @var{entity} to @var{mode}.
 @var{hard_reg_live} is the set of hard registers live at the point where
-the insn(s) are to be inserted.
+the insn(s) are to be inserted. @var{flip} is a boolean to indicate that current mode can be flipped.
 @end defmac
 
 @node Target Attributes
Index: gcc/doc/gcc.1
===================================================================
--- gcc/doc/gcc.1	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/doc/gcc.1	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -719,7 +719,7 @@
 \&\-m5\-compact  \-m5\-compact\-nofpu 
 \&\-mb  \-ml  \-mdalign  \-mrelax 
 \&\-mbigtable  \-mfmovd  \-mhitachi \-mrenesas \-mno\-renesas \-mnomacsave 
-\&\-mieee  \-misize  \-mpadstruct  \-mspace 
+\&\-mieee  \-misize  \-mpadstruct 
 \&\-mprefergot  \-musermode \-multcost=\fR\fInumber\fR \fB\-mdiv=\fR\fIstrategy\fR 
 \&\fB\-mdivsi3_libfunc=\fR\fIname\fR  
 \&\fB\-madjust\-unroll \-mindexed\-addressing \-mgettrcost=\fR\fInumber\fR \fB\-mpt\-fixed 
@@ -11733,9 +11733,6 @@
 .IX Item "-mpadstruct"
 This option is deprecated.  It pads structures to multiple of 4 bytes,
 which is incompatible with the \s-1SH\s0 \s-1ABI\s0.
-.IP "\fB\-mspace\fR" 4
-.IX Item "-mspace"
-Optimize for space instead of speed.  Implied by \fB\-Os\fR.
 .IP "\fB\-mprefergot\fR" 4
 .IX Item "-mprefergot"
 When generating position-independent code, emit function calls using
Index: gcc/doc/invoke.texi
===================================================================
--- gcc/doc/invoke.texi	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/doc/invoke.texi	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,5 +1,6 @@
 @c Copyright (C) 1988, 1989, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,
 @c 2000, 2001, 2002, 2003, 2004, 2005, 2006 Free Software Foundation, Inc.
+@c Copyright (c) 2009  STMicroelectronics.
 @c This is part of the GCC manual.
 @c For copying conditions, see the file gcc.texi.
 
@@ -322,7 +323,7 @@
 -fno-inline  -fno-math-errno  -fno-peephole  -fno-peephole2 @gol
 -funsafe-math-optimizations  -funsafe-loop-optimizations  -ffinite-math-only @gol
 -fno-toplevel-reorder -fno-trapping-math  -fno-zero-initialized-in-bss @gol
--fomit-frame-pointer  -foptimize-register-move @gol
+-fomit-frame-pointer  -foptimize-register-move -foptimize-related-values @gol
 -foptimize-sibling-calls  -fprefetch-loop-arrays @gol
 -fprofile-generate -fprofile-use @gol
 -fregmove  -frename-registers @gol
@@ -705,17 +706,20 @@
 @emph{SH Options}
 @gccoptlist{-m1  -m2  -m2e  -m3  -m3e @gol
 -m4-nofpu  -m4-single-only  -m4-single  -m4 @gol
+-m4-300-nofpu  -m4-300-single-only  -m4-300-single  -m4-300 @gol
 -m4a-nofpu -m4a-single-only -m4a-single -m4a -m4al @gol
 -m5-64media  -m5-64media-nofpu @gol
 -m5-32media  -m5-32media-nofpu @gol
 -m5-compact  -m5-compact-nofpu @gol
 -mb  -ml  -mdalign  -mrelax @gol
 -mbigtable  -mfmovd  -mhitachi -mrenesas -mno-renesas -mnomacsave @gol
--mieee  -misize  -mpadstruct  -mspace @gol
+-mieee  -misize  -mpadstruct @gol
 -mprefergot  -musermode -multcost=@var{number} -mdiv=@var{strategy} @gol
 -mdivsi3_libfunc=@var{name}  @gol
 -madjust-unroll -mindexed-addressing -mgettrcost=@var{number} -mpt-fixed @gol
- -minvalid-symbols}
+-minvalid-symbols @gol
+-malign-small-blocks=@var{block-size} @gol
+}
 
 @emph{SPARC Options}
 @gccoptlist{-mcpu=@var{cpu-type} @gol
@@ -4612,7 +4616,7 @@
 -fpeephole2 @gol
 -fschedule-insns  -fschedule-insns2 @gol
 -fsched-interblock  -fsched-spec @gol
--fregmove @gol
+-fregmove -foptimize-related-values @gol
 -fstrict-aliasing -fstrict-overflow @gol
 -fdelete-null-pointer-checks @gol
 -freorder-blocks  -freorder-functions @gol
@@ -5047,6 +5051,15 @@
 
 Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
 
+@item -foptimize-related-values
+@opindex foptimize-related-values
+For targets with auto-increment addressing modes, attempt to re-arrange
+register + offset calculations and memory acesses in order to reduce
+the overall instruction count and register pressure.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}, unless
+-foptimize-register-move is disabled.
+
 @item -fdelayed-branch
 @opindex fdelayed-branch
 If supported for the target machine, attempt to reorder instructions
@@ -12406,6 +12419,24 @@
 @opindex m4
 Generate code for the SH4.
 
+@item -m4-300-nofpu
+@opindex m4-300-nofpu
+Generate code for the ST40-300 without a floating-point unit.
+
+@item -m4-300-single-only
+@opindex m4-300-single-only
+Generate code for the ST40-300 with a floating-point unit that only
+supports single-precision arithmetic.
+
+@item -m4-300-single
+@opindex m4-300-single
+Generate code for the ST40-300 assuming the floating-point unit is in
+single-precision mode by default.
+
+@item -m4-300
+@opindex m4-300
+Generate code for the ST40-300.
+
 @item -m4a-nofpu
 @opindex m4a-nofpu
 Generate code for the SH4al-dsp, or for a SH4a in such a way that the
@@ -12496,10 +12527,6 @@
 This option is deprecated.  It pads structures to multiple of 4 bytes,
 which is incompatible with the SH ABI@.
 
-@item -mspace
-@opindex mspace
-Optimize for space instead of speed.  Implied by @option{-Os}.
-
 @item -mprefergot
 @opindex mprefergot
 When generating position-independent code, emit function calls using
@@ -12569,6 +12596,11 @@
 This option only has an effect if the gcc code base supports the
 TARGET_ADJUST_UNROLL_MAX target hook.
 
+@item -malign-small-blocks=@var{number}
+@opindex align-small-blocks=@var{number}
+Set the size among which basic blocks are aligned on cache line boundaries. 
+Default is 16 bytes. 0 means default alignment.
+
 @item -mindexed-addressing
 @opindex mindexed-addressing
 Enable the use of the indexed addressing mode for SHmedia32/SHcompact.
Index: gcc/doc/g++.1
===================================================================
--- gcc/doc/g++.1	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/doc/g++.1	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -719,7 +719,7 @@
 \&\-m5\-compact  \-m5\-compact\-nofpu 
 \&\-mb  \-ml  \-mdalign  \-mrelax 
 \&\-mbigtable  \-mfmovd  \-mhitachi \-mrenesas \-mno\-renesas \-mnomacsave 
-\&\-mieee  \-misize  \-mpadstruct  \-mspace 
+\&\-mieee  \-misize  \-mpadstruct 
 \&\-mprefergot  \-musermode \-multcost=\fR\fInumber\fR \fB\-mdiv=\fR\fIstrategy\fR 
 \&\fB\-mdivsi3_libfunc=\fR\fIname\fR  
 \&\fB\-madjust\-unroll \-mindexed\-addressing \-mgettrcost=\fR\fInumber\fR \fB\-mpt\-fixed 
@@ -11733,9 +11733,6 @@
 .IX Item "-mpadstruct"
 This option is deprecated.  It pads structures to multiple of 4 bytes,
 which is incompatible with the \s-1SH\s0 \s-1ABI\s0.
-.IP "\fB\-mspace\fR" 4
-.IX Item "-mspace"
-Optimize for space instead of speed.  Implied by \fB\-Os\fR.
 .IP "\fB\-mprefergot\fR" 4
 .IX Item "-mprefergot"
 When generating position-independent code, emit function calls using
Index: gcc/doc/loop.texi
===================================================================
--- gcc/doc/loop.texi	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/doc/loop.texi	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -400,13 +400,15 @@
 @cindex Number of iterations analysis
 
 Both on GIMPLE and on RTL, there are functions available to determine
-the number of iterations of a loop, with a similar interface.  In many
-cases, it is not possible to determine number of iterations
-unconditionally -- the determined number is correct only if some
-assumptions are satisfied.  The analysis tries to verify these
-conditions using the information contained in the program; if it fails,
-the conditions are returned together with the result.  The following
-information and conditions are provided by the analysis:
+the number of iterations of a loop, with a similar interface.  The
+number of iterations of a loop in GCC is defined as the number of
+executions of the loop latch.  In many cases, it is not possible to
+determine the number of iterations unconditionally -- the determined
+number is correct only if some assumptions are satisfied.  The analysis
+tries to verify these conditions using the information contained in the
+program; if it fails, the conditions are returned together with the
+result.  The following information and conditions are provided by the
+analysis:
 
 @itemize
 @item @code{assumptions}: If this condition is false, the rest of
@@ -434,16 +436,16 @@
 @code{find_simple_exit} on RTL.  Finally, there are functions that
 provide the same information, but additionally cache it, so that
 repeated calls to number of iterations are not so costly --
-@code{number_of_iterations_in_loop} on GIMPLE and
-@code{get_simple_loop_desc} on RTL.
+@code{number_of_latch_executions} on GIMPLE and @code{get_simple_loop_desc}
+on RTL.
 
 Note that some of these functions may behave slightly differently than
 others -- some of them return only the expression for the number of
 iterations, and fail if there are some assumptions.  The function
-@code{number_of_iterations_in_loop} works only for single-exit loops,
-and it returns the value for number of iterations higher by one with
-respect to all other functions (i.e., it returns number of executions of
-the exit statement, not of the loop latch).
+@code{number_of_latch_executions} works only for single-exit loops.
+The function @code{number_of_cond_exit_executions} can be used to
+determine number of executions of the exit condition of a single-exit
+loop (i.e., the @code{number_of_latch_executions} increased by one).
 
 @node Dependency analysis
 @section Data Dependency Analysis
Index: gcc/doc/gcc.info
===================================================================
--- gcc/doc/gcc.info	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/doc/gcc.info	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -905,7 +905,7 @@
           -m5-compact  -m5-compact-nofpu
           -mb  -ml  -mdalign  -mrelax
           -mbigtable  -mfmovd  -mhitachi -mrenesas -mno-renesas -mnomacsave
-          -mieee  -misize  -mpadstruct  -mspace
+          -mieee  -misize  -mpadstruct  
           -mprefergot  -musermode -multcost=NUMBER -mdiv=STRATEGY
           -mdivsi3_libfunc=NAME
           -madjust-unroll -mindexed-addressing -mgettrcost=NUMBER -mpt-fixed
@@ -11852,9 +11852,6 @@
      This option is deprecated.  It pads structures to multiple of 4
      bytes, which is incompatible with the SH ABI.
 
-`-mspace'
-     Optimize for space instead of speed.  Implied by `-Os'.
-
 `-mprefergot'
      When generating position-independent code, emit function calls
      using the Global Offset Table instead of the Procedure Linkage
@@ -31538,7 +31535,6 @@
 * msoft-quad-float:                      SPARC Options.      (line   45)
 * msoft-reg-count:                       M68hc1x Options.    (line   43)
 * mspace <1>:                            V850 Options.       (line   30)
-* mspace:                                SH Options.         (line  110)
 * mspe:                                  RS/6000 and PowerPC Options.
                                                              (line  196)
 * mspecld-anomaly:                       Blackfin Options.   (line   14)
Index: gcc/dwarf2asm.c
===================================================================
--- gcc/dwarf2asm.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/dwarf2asm.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,5 +1,6 @@
 /* Dwarf2 assembler output helper routines.
    Copyright (C) 2001, 2002, 2003, 2004, 2005, 2007 Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -514,7 +515,8 @@
 
   va_start (ap, comment);
 
-#ifdef HAVE_AS_LEB128
+  if (TARGET_USES_LEB128)
+    {
   fprintf (asm_out_file, "\t.uleb128 " HOST_WIDE_INT_PRINT_HEX , value);
 
   if (flag_debug_asm && comment)
@@ -522,7 +524,8 @@
       fprintf (asm_out_file, "\t%s ", ASM_COMMENT_START);
       vfprintf (asm_out_file, comment, ap);
     }
-#else
+    }
+  else
   {
     unsigned HOST_WIDE_INT work = value;
     const char *byte_op = targetm.asm_out.byte_op;
@@ -559,7 +562,7 @@
 	}
     }
   }
-#endif
+
   fputc ('\n', asm_out_file);
 
   va_end (ap);
@@ -638,14 +641,15 @@
 
   va_start (ap, comment);
 
-#ifdef HAVE_AS_LEB128
+  if (TARGET_USES_LEB128)
+    {
   fputs ("\t.uleb128 ", asm_out_file);
   assemble_name (asm_out_file, lab1);
   fputc ('-', asm_out_file);
   assemble_name (asm_out_file, lab2);
-#else
+    }
+  else
   gcc_unreachable ();
-#endif
 
   if (flag_debug_asm && comment)
     {
@@ -668,14 +672,15 @@
 
   va_start (ap, comment);
 
-#ifdef HAVE_AS_LEB128
+  if (TARGET_USES_LEB128)
+    {
   fputs ("\t.sleb128 ", asm_out_file);
   assemble_name (asm_out_file, lab1);
   fputc ('-', asm_out_file);
   assemble_name (asm_out_file, lab2);
-#else
+    }
+  else
   gcc_unreachable ();
-#endif
 
   if (flag_debug_asm && comment)
     {
Index: gcc/targhooks.c
===================================================================
--- gcc/targhooks.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/targhooks.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -65,6 +65,7 @@
 #include "reload.h"
 #include "optabs.h"
 #include "recog.h"
+#include "regs.h"
 
 
 void
@@ -180,6 +181,27 @@
   return long_long_integer_type_node;
 }
 
+/*  Given an rtx and its regno, return a regno value that shall be used for
+    purposes of comparison in operands_match_p.
+    Generally, we say that integer registers are subject to big-endian
+    adjustment.  This default target hook should generally work if the mode
+    of a register is a sufficient indication if this adjustment is to take
+    place; this will not work when software floating point is done in integer
+    registers.  */
+int
+default_match_adjust (rtx x, int regno)
+{
+  /* On a WORDS_BIG_ENDIAN machine, point to the last register of a
+     multiple hard register group of scalar integer registers, so that
+     for example (reg:DI 0) and (reg:SI 1) will be considered the same
+     register.  */
+  if (WORDS_BIG_ENDIAN && GET_MODE_SIZE (GET_MODE (x)) > UNITS_PER_WORD
+      && SCALAR_INT_MODE_P (GET_MODE (x))
+      && regno < FIRST_PSEUDO_REGISTER)
+    regno += hard_regno_nregs[regno][GET_MODE (x)] - 1;
+  return regno;
+}
+
 
 /* Returns the size of the cookie to use when allocating an array
    whose elements have the indicated TYPE.  Assumes that it is already
Index: gcc/targhooks.h
===================================================================
--- gcc/targhooks.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/targhooks.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -79,5 +79,6 @@
 						enum machine_mode,
 						secondary_reload_info *);
 extern void hook_void_bitmap (bitmap);
-
+extern int default_match_adjust (rtx, int);
 extern int default_reloc_rw_mask (void);
+
Index: gcc/gengtype.c
===================================================================
--- gcc/gengtype.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/gengtype.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1271,7 +1271,7 @@
 
       for (i = 0; i < NUM_BASE_FILES; i++)
 	if (memcmp (basename, lang_dir_names[i], strlen (lang_dir_names[i])) == 0
-	    && basename[strlen(lang_dir_names[i])] == '/')
+	    && IS_DIR_SEPARATOR(basename[strlen(lang_dir_names[i])]))
 	  return base_files[i];
 
       output_name = "gtype-desc.c";
@@ -1310,8 +1310,11 @@
   for (of = output_files; of; of = of->next)
     {
       FILE * newfile;
+      char * ofname = of->name;
 
-      newfile = fopen (of->name, "r");
+      CYGPATH (ofname);
+
+      newfile = fopen (ofname, "r");
       if (newfile != NULL )
 	{
 	  int no_write_p;
@@ -1331,7 +1334,7 @@
 	    continue;
 	}
 
-      newfile = fopen (of->name, "w");
+      newfile = fopen (ofname, "w");
       if (newfile == NULL)
 	{
 	  perror ("opening output file");
@@ -1347,6 +1350,8 @@
 	  perror ("closing output file");
 	  exit (1);
 	}
+
+      CYGPATH_FREE (ofname);
     }
 }
 
Index: gcc/optabs.c
===================================================================
--- gcc/optabs.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/optabs.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2,6 +2,7 @@
    Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007
    Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -3544,6 +3545,9 @@
 {
   do
     {
+      if (purpose == ccp_jump
+	  && cbranch_optab->handlers[(int) mode].insn_code != CODE_FOR_nothing)
+	return 1;
       if (cmp_optab->handlers[(int) mode].insn_code != CODE_FOR_nothing)
 	{
 	  if (purpose == ccp_jump)
@@ -3554,9 +3558,6 @@
 	    /* There's only one cmov entry point, and it's allowed to fail.  */
 	    return 1;
 	}
-      if (purpose == ccp_jump
-	  && cbranch_optab->handlers[(int) mode].insn_code != CODE_FOR_nothing)
-	return 1;
       if (purpose == ccp_cmov
 	  && cmov_optab->handlers[(int) mode].insn_code != CODE_FOR_nothing)
 	return 1;
@@ -4292,19 +4293,41 @@
 gen_add3_insn (rtx r0, rtx r1, rtx c)
 {
   int icode = (int) add_optab->handlers[(int) GET_MODE (r0)].insn_code;
+  int mcode;
+  rtx s;
 
   if (icode == CODE_FOR_nothing
       || !(insn_data[icode].operand[0].predicate
-	   (r0, insn_data[icode].operand[0].mode))
-      || !(insn_data[icode].operand[1].predicate
+	   (r0, insn_data[icode].operand[0].mode)))
+    return NULL_RTX;
+
+  if ((insn_data[icode].operand[1].predicate
 	   (r1, insn_data[icode].operand[1].mode))
-      || !(insn_data[icode].operand[2].predicate
+      && (insn_data[icode].operand[2].predicate
 	   (c, insn_data[icode].operand[2].mode)))
+    return GEN_FCN (icode) (r0, r1, c);
+  
+  mcode = (int) mov_optab->handlers[(int) GET_MODE (r0)].insn_code;
+  if (REGNO (r0) == REGNO (r1)
+      || !(insn_data[icode].operand[1].predicate
+	   (r0, insn_data[icode].operand[1].mode))
+      || !(insn_data[icode].operand[2].predicate
+	   (r1, insn_data[icode].operand[2].mode))
+      || !(insn_data[mcode].operand[0].predicate
+	   (r0, insn_data[mcode].operand[0].mode))
+      || !(insn_data[mcode].operand[1].predicate
+	   (c, insn_data[mcode].operand[1].mode)))
     return NULL_RTX;
 
-  return GEN_FCN (icode) (r0, r1, c);
+  start_sequence ();
+  emit_insn (GEN_FCN (mcode) (r0, c));
+  emit_insn (GEN_FCN (icode) (r0, r0, r1));
+  s = get_insns ();
+  end_sequence ();
+  return s;
 }
 
+
 int
 have_add2_insn (rtx x, rtx y)
 {
Index: gcc/DATESTAMP
===================================================================
--- gcc/DATESTAMP	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/DATESTAMP	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1 +1 @@
-20080519
+20090602
Index: gcc/postreload.c
===================================================================
--- gcc/postreload.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/postreload.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,6 +1,7 @@
 /* Perform simple optimizations to clean up the result of reload.
    Copyright (C) 1987, 1988, 1989, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005, 2007 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -888,6 +889,23 @@
 		{
 		  rtx *np;
 
+		  /* For every new use of REG_SUM, we have to record the use
+		     of BASE therein.  */
+		  for (i = reg_state[regno].use_index;
+		       i < RELOAD_COMBINE_MAX_USES; i++)
+		    {
+		      rtx *basep = &XEXP (*reg_state[regno].reg_use[i].usep, 1);
+		      rtx use_insn = reg_state[regno].reg_use[i].insn;
+
+		      if (*basep != base)
+			abort ();
+		      reload_combine_note_use (basep, use_insn);
+		    }
+		  if (reg_state[REGNO (base)].use_ruid
+		      > reg_state[regno].use_ruid)
+		    reg_state[REGNO (base)].use_ruid
+		      = reg_state[regno].use_ruid;
+
 		  /* Delete the reg-reg addition.  */
 		  delete_insn (insn);
 
Index: gcc/defaults.h
===================================================================
--- gcc/defaults.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/defaults.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -3,6 +3,7 @@
    2005, 2007
    Free Software Foundation, Inc.
    Contributed by Ron Guilmette (rfg@monkeys.com)
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -273,6 +274,15 @@
 #define TARGET_USES_WEAK_UNWIND_INFO 0
 #endif
 
+/* Use leb128 encoding based on command line options.  */
+#ifndef TARGET_USES_LEB128
+#ifdef HAVE_AS_LEB128
+#define TARGET_USES_LEB128 1
+#else
+#define TARGET_USES_LEB128 0
+#endif
+#endif
+
 /* By default, there is no prefix on user-defined symbols.  */
 #ifndef USER_LABEL_PREFIX
 #define USER_LABEL_PREFIX ""
Index: gcc/tree-tailcall.c
===================================================================
--- gcc/tree-tailcall.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/tree-tailcall.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -120,7 +120,6 @@
    accumulator.  */
 static tree m_acc, a_acc;
 
-static bool suitable_for_tail_opt_p (void);
 static bool optimize_tail_call (struct tailcall *, bool);
 static void eliminate_tail_call (struct tailcall *);
 static void find_tail_calls (basic_block, struct tailcall **);
@@ -156,7 +155,7 @@
    This test must pass in addition to suitable_for_tail_opt_p in order to make
    tail call discovery happen.  */
 
-static bool
+bool
 suitable_for_tail_call_opt_p (void)
 {
   tree param;
Index: gcc/reload.c
===================================================================
--- gcc/reload.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/reload.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2176,14 +2176,8 @@
 	 multiple hard register group of scalar integer registers, so that
 	 for example (reg:DI 0) and (reg:SI 1) will be considered the same
 	 register.  */
-      if (WORDS_BIG_ENDIAN && GET_MODE_SIZE (GET_MODE (x)) > UNITS_PER_WORD
-	  && SCALAR_INT_MODE_P (GET_MODE (x))
-	  && i < FIRST_PSEUDO_REGISTER)
-	i += hard_regno_nregs[i][GET_MODE (x)] - 1;
-      if (WORDS_BIG_ENDIAN && GET_MODE_SIZE (GET_MODE (y)) > UNITS_PER_WORD
-	  && SCALAR_INT_MODE_P (GET_MODE (y))
-	  && j < FIRST_PSEUDO_REGISTER)
-	j += hard_regno_nregs[j][GET_MODE (y)] - 1;
+      i = targetm.match_adjust (x, i);
+      j = targetm.match_adjust (y, j);
 
       return i == j;
     }
Index: gcc/tree-scalar-evolution.c
===================================================================
--- gcc/tree-scalar-evolution.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/tree-scalar-evolution.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -468,19 +468,14 @@
 	{
 	  struct loop *inner_loop = 
 	    current_loops->parray[CHREC_VARIABLE (evolution_fn)];
-	  tree nb_iter = number_of_iterations_in_loop (inner_loop);
+
+ 	  tree nb_iter = number_of_latch_executions (inner_loop);
 
 	  if (nb_iter == chrec_dont_know)
 	    return chrec_dont_know;
 	  else
 	    {
 	      tree res;
-	      tree type = chrec_type (nb_iter);
-
-	      /* Number of iterations is off by one (the ssa name we
-		 analyze must be defined before the exit).  */
-	      nb_iter = chrec_fold_minus (type, nb_iter,
-					  build_int_cst (type, 1));
 	      
 	      /* evolution_fn is the evolution function in LOOP.  Get
 		 its value in the nb_iter-th iteration.  */
@@ -510,7 +505,7 @@
 chrec_is_positive (tree chrec, bool *value)
 {
   bool value0, value1, value2;
-  tree type, end_value, nb_iter;
+  tree end_value, nb_iter;
   
   switch (TREE_CODE (chrec))
     {
@@ -533,15 +528,12 @@
       if (!evolution_function_is_affine_p (chrec))
 	return false;
 
-      nb_iter = number_of_iterations_in_loop
+      nb_iter = number_of_latch_executions
 	(current_loops->parray[CHREC_VARIABLE (chrec)]);
 
       if (chrec_contains_undetermined (nb_iter))
 	return false;
 
-      type = chrec_type (nb_iter);
-      nb_iter = chrec_fold_minus (type, nb_iter, build_int_cst (type, 1));
-
 #if 0
       /* TODO -- If the test is after the exit, we may decrease the number of
 	 iterations by one.  */
@@ -894,19 +886,6 @@
 set_nb_iterations_in_loop (struct loop *loop, 
 			   tree res)
 {
-  tree type = chrec_type (res);
-
-  res = chrec_fold_plus (type, res, build_int_cst (type, 1));
-
-  /* FIXME HWI: However we want to store one iteration less than the
-     count of the loop in order to be compatible with the other
-     nb_iter computations in loop-iv.  This also allows the
-     representation of nb_iters that are equal to MAX_INT.  */
-  if (TREE_CODE (res) == INTEGER_CST
-      && (TREE_INT_CST_LOW (res) == 0
-	  || TREE_OVERFLOW (res)))
-    res = chrec_dont_know;
-  
   if (dump_file && (dump_flags & TDF_DETAILS))
     {
       fprintf (dump_file, "  (set_nb_iterations_in_loop = ");
@@ -2468,7 +2447,7 @@
    the loop body has been executed 6 times.  */
 
 tree 
-number_of_iterations_in_loop (struct loop *loop)
+number_of_latch_executions (struct loop *loop)
 {
   tree res, type;
   edge exit;
@@ -2503,6 +2482,33 @@
   return set_nb_iterations_in_loop (loop, res);
 }
 
+/* Returns the number of executions of the exit condition of LOOP,
+   i.e., the number by one higher than number_of_latch_executions.
+   Note that unline number_of_latch_executions, this number does
+   not necessarily fit in the unsigned variant of the type of
+   the control variable -- if the number of iterations is a constant,
+   we return chrec_dont_know if adding one to number_of_latch_executions
+   overflows; however, in case the number of iterations is symbolic
+   expression, the caller is responsible for dealing with this
+   the possible overflow.  */
+
+tree 
+number_of_exit_cond_executions (struct loop *loop)
+{
+  tree ret = number_of_latch_executions (loop);
+  tree type = chrec_type (ret);
+
+  if (chrec_contains_undetermined (ret))
+    return ret;
+
+  ret = chrec_fold_plus (type, ret, build_int_cst (type, 1));
+  if (TREE_CODE (ret) == INTEGER_CST
+      && TREE_OVERFLOW (ret))
+    return chrec_dont_know;
+
+  return ret;
+}
+
 /* One of the drivers for testing the scalar evolutions analysis.
    This function computes the number of iterations for all the loops
    from the EXIT_CONDITIONS array.  */
@@ -2517,7 +2523,7 @@
   
   for (i = 0; VEC_iterate (tree, *exit_conditions, i, cond); i++)
     {
-      tree res = number_of_iterations_in_loop (loop_containing_stmt (cond));
+      tree res = number_of_latch_executions (loop_containing_stmt (cond));
       if (chrec_contains_undetermined (res))
 	nb_chrec_dont_know_loops++;
       else
@@ -2963,7 +2969,7 @@
       if (!exit)
 	continue;
 
-      niter = number_of_iterations_in_loop (loop);
+      niter = number_of_latch_executions (loop);
       if (niter == chrec_dont_know
 	  /* If computing the number of iterations is expensive, it may be
 	     better not to introduce computations involving it.  */
Index: gcc/tree-scalar-evolution.h
===================================================================
--- gcc/tree-scalar-evolution.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/tree-scalar-evolution.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -21,7 +21,8 @@
 #ifndef GCC_TREE_SCALAR_EVOLUTION_H
 #define GCC_TREE_SCALAR_EVOLUTION_H
 
-extern tree number_of_iterations_in_loop (struct loop *);
+extern tree number_of_latch_executions (struct loop *);
+extern tree number_of_exit_cond_executions (struct loop *);
 extern tree get_loop_exit_condition (struct loop *);
 
 extern void scev_initialize (struct loops *loops);
Index: gcc/target.h
===================================================================
--- gcc/target.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/target.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -749,6 +749,10 @@
 				      enum machine_mode,
 				      struct secondary_reload_info *);
 
+   /* Take an rtx and its regno, and return the regno for purposes of
+      checking a matching constraint.  */
+   int (*match_adjust) (rtx, int);
+
   /* Functions specific to the C++ frontend.  */
   struct cxx {
     /* Return the integer type used for guard variables.  */
Index: gcc/configure
===================================================================
--- gcc/configure	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/configure	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -309,7 +309,8 @@
 # include <unistd.h>
 #endif"
 
-ac_subst_vars='SHELL PATH_SEPARATOR PACKAGE_NAME PACKAGE_TARNAME PACKAGE_VERSION PACKAGE_STRING PACKAGE_BUGREPORT exec_prefix prefix program_transform_name bindir sbindir libexecdir datadir sysconfdir sharedstatedir localstatedir libdir includedir oldincludedir infodir mandir build_alias host_alias target_alias DEFS ECHO_C ECHO_N ECHO_T LIBS build build_cpu build_vendor build_os host host_cpu host_vendor host_os target target_cpu target_vendor target_os target_noncanonical build_libsubdir build_subdir host_subdir target_subdir GENINSRC CC CFLAGS LDFLAGS CPPFLAGS ac_ct_CC EXEEXT OBJEXT NO_MINUS_C_MINUS_O OUTPUT_OPTION CPP EGREP strict1_warn cxx_compat_warn warn_cflags WERROR nocommon_flag TREEBROWSER valgrind_path valgrind_path_defines valgrind_command coverage_flags enable_multilib enable_decimal_float enable_shared TARGET_SYSTEM_ROOT TARGET_SYSTEM_ROOT_DEFINE CROSS_SYSTEM_HEADER_DIR onestep datarootdir docdir htmldir SET_MAKE AWK LN_S LN RANLIB ac_ct_RANLIB ranlib_flags INSTALL INSTALL_PROGRAM INSTALL_DATA make_compare_target have_mktemp_command MAKEINFO BUILD_INFO GENERATED_MANPAGES FLEX BISON NM AR stage1_cflags COLLECT2_LIBS GNAT_LIBEXC LDEXP_LIB TARGET_GETGROUPS_T LIBICONV LTLIBICONV LIBICONV_DEP manext objext gthread_flags extra_modes_file extra_opt_files USE_NLS LIBINTL LIBINTL_DEP INCINTL XGETTEXT GMSGFMT POSUB CATALOGS DATADIRNAME INSTOBJEXT GENCAT CATOBJEXT host_cc_for_libada CROSS ALL SYSTEM_HEADER_DIR inhibit_libc CC_FOR_BUILD BUILD_CFLAGS STMP_FIXINC STMP_FIXPROTO collect2 gcc_cv_as ORIGINAL_AS_FOR_TARGET gcc_cv_ld ORIGINAL_LD_FOR_TARGET gcc_cv_nm ORIGINAL_NM_FOR_TARGET gcc_cv_objdump libgcc_visibility GGC zlibdir zlibinc MAINT gcc_tooldir dollar slibdir objdir subdirs srcdir all_boot_languages all_compilers all_gtfiles all_gtfiles_files_langs all_gtfiles_files_files all_lang_makefrags all_lang_makefiles all_languages all_selected_languages all_stagestuff build_exeext build_install_headers_dir build_xm_file_list build_xm_include_list build_xm_defines build_file_translate check_languages cc_set_by_configure quoted_cc_set_by_configure cpp_install_dir xmake_file tmake_file extra_gcc_objs extra_headers_list extra_objs extra_parts extra_passes extra_programs float_h_file gcc_config_arguments gcc_gxx_include_dir host_exeext host_xm_file_list host_xm_include_list host_xm_defines out_host_hook_obj install lang_opt_files lang_specs_files lang_tree_files local_prefix md_file objc_boehm_gc out_file out_object_file stage_prefix_set_by_configure quoted_stage_prefix_set_by_configure thread_file tm_file_list tm_include_list tm_defines tm_p_file_list tm_p_include_list xm_file_list xm_include_list xm_defines c_target_objs cxx_target_objs target_cpu_default GMPLIBS GMPINC LIBOBJS LTLIBOBJS'
+ac_subst_vars='SHELL PATH_SEPARATOR PACKAGE_NAME PACKAGE_TARNAME PACKAGE_VERSION PACKAGE_STRING PACKAGE_BUGREPORT exec_prefix prefix program_transform_name bindir sbindir libexecdir datadir sysconfdir sharedstatedir localstatedir libdir includedir oldincludedir infodir mandir build_alias host_alias target_alias DEFS ECHO_C ECHO_N ECHO_T LIBS build build_cpu build_vendor build_os host host_cpu host_vendor host_os target target_cpu target_vendor target_os target_noncanonical build_libsubdir build_subdir host_subdir target_subdir GENINSRC CC CFLAGS LDFLAGS CPPFLAGS ac_ct_CC EXEEXT OBJEXT NO_MINUS_C_MINUS_O OUTPUT_OPTION CPP EGREP strict1_warn cxx_compat_warn warn_cflags WERROR nocommon_flag TREEBROWSER valgrind_path valgrind_path_defines valgrind_command coverage_flags enable_multilib enable_decimal_float enable_shared TARGET_SYSTEM_ROOT TARGET_SYSTEM_ROOT_DEFINE CROSS_SYSTEM_HEADER_DIR onestep datarootdir docdir htmldir SET_MAKE AWK LN_S LN RANLIB ac_ct_RANLIB ranlib_flags INSTALL INSTALL_PROGRAM INSTALL_DATA make_compare_target have_mktemp_command MAKEINFO BUILD_INFO GENERATED_MANPAGES FLEX BISON NM AR stage1_cflags COLLECT2_LIBS GNAT_LIBEXC LDEXP_LIB TARGET_GETGROUPS_T LIBICONV LTLIBICONV LIBICONV_DEP manext objext gthread_flags extra_modes_file extra_opt_files USE_NLS LIBINTL LIBINTL_DEP INCINTL XGETTEXT GMSGFMT POSUB CATALOGS DATADIRNAME INSTOBJEXT GENCAT CATOBJEXT host_cc_for_libada CROSS ALL SYSTEM_HEADER_DIR inhibit_libc CC_FOR_BUILD BUILD_CFLAGS STMP_FIXINC STMP_FIXPROTO collect2 gcc_cv_as ORIGINAL_AS_FOR_TARGET gcc_cv_ld ORIGINAL_LD_FOR_TARGET gcc_cv_nm ORIGINAL_NM_FOR_TARGET gcc_cv_objdump libgcc_visibility GGC zlibdir zlibinc MAINT gcc_tooldir dollar slibdir objdir subdirs srcdir all_boot_languages all_compilers all_gtfiles all_gtfiles_files_langs all_gtfiles_files_files all_lang_makefrags all_lang_makefiles all_languages all_selected_languages all_stagestuff build_exeext build_install_headers_dir build_xm_file_list build_xm_include_list build_xm_defines build_file_translate check_languages cc_set_by_configure quoted_cc_set_by_configure cpp_install_dir xmake_file tmake_file extra_gcc_objs extra_headers_list extra_libgcc_srcs extra_objs extra_parts extra_passes extra_programs float_h_file gcc_config_arguments gcc_gxx_include_dir host_exeext host_xm_file_list host_xm_include_list host_xm_defines out_host_hook_obj install lang_opt_files lang_specs_files lang_tree_files local_prefix md_file objc_boehm_gc out_file out_object_file stage_prefix_set_by_configure quoted_stage_prefix_set_by_configure thread_file tm_file_list tm_include_list tm_defines tm_p_file_list tm_p_include_list xm_file_list xm_include_list xm_defines c_target_objs cxx_target_objs target_cpu_default GMPLIBS GMPINC LIBOBJS LTLIBOBJS'
+
 ac_subst_files='language_hooks'
 
 # Initialize some variables set by options.
@@ -12411,7 +12412,7 @@
     target_thread_file='single'
     ;;
   aix | dce | gnat | irix | posix | posix95 | rtems | \
-  single | solaris | vxworks | win32 )
+  single | solaris | vxworks | win32 | generic)
     target_thread_file=${enable_threads}
     ;;
   *)
@@ -12432,6 +12433,14 @@
   rm -f gthr-default.h
   echo "#include \"gthr-${thread_file}.h\"" > gthr-default.h
   gthread_flags=-DHAVE_GTHR_DEFAULT
+  if test $thread_file != posix; then
+    if test -f $srcdir/gthr-${thread_file}.c; then
+      extra_libgcc_srcs=$srcdir/gthr-${thread_file}.c
+    fi
+    if test -f $srcdir/gthr-objc-${thread_file}.c; then
+      extra_libgcc_srcs="${extra_libgcc_srcs} $srcdir/gthr-objc-${thread_file}.c"
+    fi
+  fi
 fi
 
 
@@ -12738,6 +12747,7 @@
 	esac
 	saved_CFLAGS="${CFLAGS}"
 	CC="${CC_FOR_BUILD}" CFLAGS="${CFLAGS_FOR_BUILD}" \
+	CONFIG_SITE="" \
 	${realsrcdir}/configure \
 		--enable-languages=${enable_languages-all} \
 		--target=$target_alias --host=$build_alias --build=$build_alias
@@ -13193,6 +13203,16 @@
 	CROSS="-DCROSS_COMPILE"
 	ALL=all.cross
 	SYSTEM_HEADER_DIR=$build_system_header_dir
+
+	# For builds with an in-tree newlib, then the headers are not
+	# copied to build_system_header_dir, so things like limits.h
+	# won't work unless we point at the real headers.
+	if test "$with_newlib" = yes \
+		&& (test -z "$with_headers" || test "$with_headers" = yes) \
+		&& test -d $srcdir/../newlib/libc/include; then
+	  SYSTEM_HEADER_DIR="\$(abs_srcdir)/../newlib/libc/include"
+	fi
+
 	case "$host","$target" in
 	# Darwin crosses can use the host system's libraries and headers,
 	# because of the fat library support.  Of course, it must be the
@@ -14045,7 +14065,7 @@
   # arbitrary sections are supported and try the test.
   as_ver=`$gcc_cv_as --version 2>/dev/null | sed 1q`
   if echo "$as_ver" | grep GNU > /dev/null; then
-    as_ver=`echo $as_ver | sed -e 's/GNU assembler \([0-9.][0-9.]*\).*/\1/'`
+    as_ver=`echo $as_ver | sed -e 's/GNU assembler [^0-9.]*\([0-9.][0-9.]*\).*/\1/'`
     as_major=`echo $as_ver | sed 's/\..*//'`
     as_minor=`echo $as_ver | sed 's/[^.]*\.\([0-9]*\).*/\1/'`
     if test $as_major -eq 2 && test $as_minor -lt 11
@@ -17302,6 +17322,7 @@
 s,@tmake_file@,$tmake_file,;t t
 s,@extra_gcc_objs@,$extra_gcc_objs,;t t
 s,@extra_headers_list@,$extra_headers_list,;t t
+s,@extra_libgcc_srcs@,$extra_libgcc_srcs,;t t
 s,@extra_objs@,$extra_objs,;t t
 s,@extra_parts@,$extra_parts,;t t
 s,@extra_passes@,$extra_passes,;t t
Index: gcc/final.c
===================================================================
--- gcc/final.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/final.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2,6 +2,7 @@
    Copyright (C) 1987, 1988, 1989, 1992, 1993, 1994, 1995, 1996, 1997,
    1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007
    Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -378,6 +379,20 @@
     }
 }
 
+static void realloc_insn_lengths (int uid, char **varying_length)
+{
+  int max_uid = get_max_uid ();
+
+  gcc_assert (insn_lengths);
+
+  insn_lengths = XRESIZEVEC (int, insn_lengths, max_uid);
+  insn_lengths[uid] = 0;
+  insn_lengths_max_uid = max_uid;
+  insn_lengths[uid] = 0;
+  *varying_length = XRESIZEVEC (char, *varying_length, max_uid);
+  (*varying_length)[uid] = 0;
+}
+
 /* Obtain the current length of an insn.  If branch shortening has been done,
    get its actual length.  Otherwise, use FALLBACK_FN to calculate the
    length.  */
@@ -424,7 +439,7 @@
 	  length = asm_insn_count (body) * fallback_fn (insn);
 	else if (GET_CODE (body) == SEQUENCE)
 	  for (i = 0; i < XVECLEN (body, 0); i++)
-	    length += get_attr_length (XVECEXP (body, 0, i));
+	    length += get_attr_length_1 (XVECEXP (body, 0, i), fallback_fn);
 	else
 	  length = fallback_fn (insn);
 	break;
@@ -434,7 +449,7 @@
       }
 
 #ifdef ADJUST_INSN_LENGTH
-  ADJUST_INSN_LENGTH (insn, length);
+  ADJUST_INSN_LENGTH (insn, length, 0);
 #endif
   return length;
 #else /* not HAVE_ATTR_length */
@@ -919,6 +934,11 @@
     }
 #ifdef HAVE_ATTR_length
 
+  gcc_assert (insn_lengths == 0);
+
+  /* New insn might have been created by insn_length_adjustment.  */
+  max_uid = get_max_uid ();
+
   /* Allocate the rest of the arrays.  */
   insn_lengths = XNEWVEC (int, max_uid);
   insn_lengths_max_uid = max_uid;
@@ -1098,7 +1118,12 @@
 
       /* If needed, do any adjustment.  */
 #ifdef ADJUST_INSN_LENGTH
-      ADJUST_INSN_LENGTH (insn, insn_lengths[uid]);
+      ADJUST_INSN_LENGTH (insn, insn_lengths[uid], 0);
+      if (max_uid != get_max_uid ())
+	{
+	  realloc_insn_lengths (max_uid, &varying_length);
+	  max_uid = get_max_uid ();
+	}
       if (insn_lengths[uid] < 0)
 	fatal_insn ("negative insn length", insn);
 #endif
@@ -1277,11 +1302,23 @@
 		    }
 		}
 	      else
+		{
+#ifdef ADJUST_INSN_LENGTH
+		  /* If needed, do any adjustment for non varying insn.  */
+		  ADJUST_INSN_LENGTH (insn, insn_lengths[uid], 1);
+		  if (max_uid != get_max_uid ())
+		    if (max_uid != get_max_uid ())
+		      {
+			realloc_insn_lengths (max_uid, &varying_length);
+			max_uid = get_max_uid ();
+		      }
+#endif
 		insn_current_address += insn_lengths[uid];
-
+		}
 	      continue;
 	    }
 
+	  /* Varying_length.  */
 	  if (NONJUMP_INSN_P (insn) && GET_CODE (PATTERN (insn)) == SEQUENCE)
 	    {
 	      int i;
@@ -1303,11 +1340,22 @@
 		  else
 		    inner_length = insn_current_length (inner_insn);
 
+#ifdef ADJUST_INSN_LENGTH
+		  /* If needed, do any adjustment.  */
+		  ADJUST_INSN_LENGTH (inner_insn, inner_length, 0);
+		  if (max_uid != get_max_uid ())
+		    {
+		      realloc_insn_lengths (max_uid, &varying_length);
+		      max_uid = get_max_uid ();
+		    }
+#endif
+
 		  if (inner_length != insn_lengths[inner_uid])
 		    {
 		      insn_lengths[inner_uid] = inner_length;
 		      something_changed = 1;
 		    }
+
 		  insn_current_address += insn_lengths[inner_uid];
 		  new_length += inner_length;
 		}
@@ -1321,8 +1369,13 @@
 #ifdef ADJUST_INSN_LENGTH
 	  /* If needed, do any adjustment.  */
 	  tmp_length = new_length;
-	  ADJUST_INSN_LENGTH (insn, new_length);
+	  ADJUST_INSN_LENGTH (insn, new_length, 0);
 	  insn_current_address += (new_length - tmp_length);
+	  if (max_uid != get_max_uid ())
+	    {
+	      realloc_insn_lengths (max_uid, &varying_length);
+	      max_uid = get_max_uid ();
+	    }
 #endif
 
 	  if (new_length != insn_lengths[uid])
@@ -1357,9 +1410,16 @@
   else
     template = decode_asm_operands (body, NULL, NULL, NULL, NULL);
 
+  if (! *template)
+    return 0;
+
+#ifdef TARGET_ASM_COUNT
+  count = TARGET_ASM_COUNT (template, 0);
+#else
   for (; *template; template++)
     if (IS_ASM_LOGICAL_LINE_SEPARATOR (*template) || *template == '\n')
       count++;
+#endif
 
   return count;
 }
@@ -1847,6 +1907,10 @@
 
 	  if (align && NEXT_INSN (insn))
 	    {
+#ifdef FINAL_PRESCAN_INSN
+	FINAL_PRESCAN_INSN (insn, recog_data.operand, recog_data.n_operands);
+#endif
+
 #ifdef ASM_OUTPUT_MAX_SKIP_ALIGN
 	      ASM_OUTPUT_MAX_SKIP_ALIGN (file, align, max_skip);
 #else
@@ -2155,6 +2219,7 @@
 		   split.  */
 		do
 		  insn = final_scan_insn (insn, file, 0, 1, seen);
+
 		while (insn != next);
 	      }
 #ifdef DBR_OUTPUT_SEQEND
Index: gcc/gcc.c
===================================================================
--- gcc/gcc.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/gcc.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1471,25 +1471,33 @@
 #define MD_STARTFILE_PREFIX_1 ""
 #endif
 
+/* These directories are locations set at configure-time based on the
+   --prefix option provided to configure.  Their initializers are
+   defined in Makefile.in.  These paths are not *directly* used when
+   gcc_exec_prefix is set because, in that case, we know where the
+   compiler has been installed, and use paths relative to that
+   location instead.  */
 static const char *const standard_exec_prefix = STANDARD_EXEC_PREFIX;
+static const char *const standard_libexec_prefix = STANDARD_LIBEXEC_PREFIX;
+static const char *const standard_bindir_prefix = STANDARD_BINDIR_PREFIX;
+static const char *const standard_startfile_prefix = STANDARD_STARTFILE_PREFIX;
+
+/* For native compilers, these are well-known paths containing
+   components that may be provided by the system.  For cross
+   compilers, these paths are not used.  */
 static const char *const standard_exec_prefix_1 = "/usr/libexec/gcc/";
 static const char *const standard_exec_prefix_2 = "/usr/lib/gcc/";
 static const char *md_exec_prefix = MD_EXEC_PREFIX;
-
 static const char *md_startfile_prefix = MD_STARTFILE_PREFIX;
 static const char *md_startfile_prefix_1 = MD_STARTFILE_PREFIX_1;
-static const char *const standard_startfile_prefix = STANDARD_STARTFILE_PREFIX;
 static const char *const standard_startfile_prefix_1
   = STANDARD_STARTFILE_PREFIX_1;
 static const char *const standard_startfile_prefix_2
   = STANDARD_STARTFILE_PREFIX_2;
 
+/* A relative path to be used in finding the location of tools
+   relative to the driver.  */
 static const char *const tooldir_base_prefix = TOOLDIR_BASE_PREFIX;
-static const char *tooldir_prefix;
-
-static const char *const standard_bindir_prefix = STANDARD_BINDIR_PREFIX;
-
-static const char *standard_libexec_prefix = STANDARD_LIBEXEC_PREFIX;
 
 /* Subdirectory to use for locating libraries.  Set by
    set_multilib_dir based on the compilation options.  */
@@ -2017,6 +2025,7 @@
   char *buffer;
   char *p;
 
+  CYGPATH (filename);
   buffer = load_specs (filename);
 
   /* Scan BUFFER for specs, putting them in the vector.  */
@@ -2239,6 +2248,8 @@
 
   if (link_command_spec == 0)
     fatal ("spec file has no spec for linking");
+
+  CYGPATH_FREE (filename);
 }
 
 /* Record the names of temporary files we tell compilers to write,
@@ -2420,15 +2431,17 @@
     {
       size_t multi_dir_len = 0;
       size_t multi_os_dir_len = 0;
-      size_t suffix_len;
-      size_t just_suffix_len;
+      size_t suffix_len = 0;
+      size_t just_suffix_len = 0;
       size_t len;
 
       if (multi_dir)
 	multi_dir_len = strlen (multi_dir);
       if (multi_os_dir)
 	multi_os_dir_len = strlen (multi_os_dir);
+      if (multi_suffix)
       suffix_len = strlen (multi_suffix);
+      if (just_multi_suffix)
       just_suffix_len = strlen (just_multi_suffix);
 
       if (path == NULL)
@@ -2447,7 +2460,7 @@
 	  memcpy (path, pl->prefix, len);
 
 	  /* Look first in MACHINE/VERSION subdirectory.  */
-	  if (!skip_multi_dir)
+	  if (!skip_multi_dir && multi_suffix)
 	    {
 	      memcpy (path + len, multi_suffix, suffix_len + 1);
 	      ret = callback (path, callback_info);
@@ -2457,7 +2470,7 @@
 
 	  /* Some paths are tried with just the machine (ie. target)
 	     subdir.  This is used for finding as, ld, etc.  */
-	  if (!skip_multi_dir
+	  if (!skip_multi_dir && just_multi_suffix
 	      && pl->require_machine_suffix == 2)
 	    {
 	      memcpy (path + len, just_multi_suffix, just_suffix_len + 1);
@@ -2662,7 +2675,10 @@
 	     bool do_multi)
 {
   struct file_at_path_info info;
+  char *temp;
+
 
+  CYGPATH (name);
 #ifdef DEFAULT_ASSEMBLER
   if (! strcmp (name, "as") && access (DEFAULT_ASSEMBLER, mode) == 0)
     return xstrdup (DEFAULT_ASSEMBLER);
@@ -2678,8 +2694,13 @@
   if (IS_ABSOLUTE_PATH (name))
     {
       if (access (name, mode) == 0)
-	return xstrdup (name);
+	{
+	  temp =  xstrdup (name);
+	  CYGPATH_FREE (name);
+	  return temp;
+	}
 
+      CYGPATH_FREE (name);
       return NULL;
     }
 
@@ -2689,8 +2710,10 @@
   info.suffix_len = strlen (info.suffix);
   info.mode = mode;
 
-  return for_each_path (pprefix, do_multi, info.name_len + info.suffix_len,
+  temp = for_each_path (pprefix, do_multi, info.name_len + info.suffix_len,
 			file_at_path, &info);
+  CYGPATH_FREE (name);
+  return temp;  
 }
 
 /* Ranking of prefixes in the sort list. -B prefixes are put before
@@ -2748,6 +2771,7 @@
 }
 
 /* Same as add_prefix, but prepending target_system_root to prefix.  */
+/* The target_system_root prefix has been relocated by gcc_exec_prefix.  */
 static void
 add_sysrooted_prefix (struct path_prefix *pprefix, const char *prefix,
 		      const char *component,
@@ -3277,6 +3301,7 @@
   int is_modify_target_name;
   unsigned int j;
 #endif
+  const char *tooldir_prefix;
 
   GET_ENVIRONMENT (gcc_exec_prefix, "GCC_EXEC_PREFIX");
 
@@ -3364,11 +3389,38 @@
   /* FIXME: make_relative_prefix doesn't yet work for VMS.  */
   if (!gcc_exec_prefix)
     {
+      /* argv[0] may be a soft link. In this case it may be correct to ignore it and treat it
+	 as the real thing, or we may have to follow the link and find the real installation.
+	 We decide which to do based on whether we can find the target directory (its name
+	 is in spec_machine) without following the links.
+	 If so then assume ALL the files have been linked. In this case it would be the wrong
+	 thing to try and be clever, because the user may be pasting the installation
+	 together with links. Some simple package managers do do this.  */
+      char *temp;
+      gcc_libexec_prefix = make_relative_prefix_ignore_links (argv[0],
+							      standard_bindir_prefix,
+							      standard_libexec_prefix);
+      if (gcc_libexec_prefix)
+	add_prefix (&exec_prefixes, gcc_libexec_prefix, "GCC",
+		    PREFIX_PRIORITY_LAST, 0, 0);
+      if ((temp = find_a_file (&exec_prefixes, spec_machine, R_OK, 0)) == NULL)
+	{
+	  /* the directory was not found so follow the links and hope that solves the problem. */
       gcc_exec_prefix = make_relative_prefix (argv[0], standard_bindir_prefix,
 					      standard_exec_prefix);
       gcc_libexec_prefix = make_relative_prefix (argv[0],
 						 standard_bindir_prefix,
 						 standard_libexec_prefix);
+	}
+      else
+	{
+	  /* the directory was found so ignore the links and assume the headers/libraries/programs
+	     are all linked from the same place as required.  */
+	  free(temp);
+	  gcc_exec_prefix = make_relative_prefix_ignore_links (argv[0], standard_bindir_prefix,
+							       standard_exec_prefix);
+	}
+
       if (gcc_exec_prefix)
 	putenv (concat ("GCC_EXEC_PREFIX=", gcc_exec_prefix, NULL));
     }
@@ -3379,14 +3431,22 @@
 	 / (which is ignored by make_relative_prefix), so append a
 	 program name.  */
       char *tmp_prefix = concat (gcc_exec_prefix, "gcc", NULL);
-      gcc_libexec_prefix = make_relative_prefix (tmp_prefix,
+      gcc_libexec_prefix = make_relative_prefix_ignore_links (tmp_prefix,
 						 standard_exec_prefix,
 						 standard_libexec_prefix);
+ 
+      /* The path is unrelocated, so fallback to the original setting.  */
+      if (!gcc_libexec_prefix)
+ 	gcc_libexec_prefix = standard_libexec_prefix;
+ 
       free (tmp_prefix);
     }
 #else
 #endif
 
+   /* From this point onward, gcc_exec_prefix is non-null if the toolchain
+      is relocated. The toolchain was either relocated using GCC_EXEC_PREFIX
+      or an automatically created GCC_EXEC_PREFIX from argv[0].  */
   if (gcc_exec_prefix)
     {
       int len = strlen (gcc_exec_prefix);
@@ -3763,6 +3823,8 @@
 		else
 		  value = p + 1;
 
+		CYGPATH (value);
+
 		len = strlen (value);
 
 		/* Catch the case where the user has forgotten to append a
@@ -3935,12 +3997,15 @@
       use_pipes = 0;
     }
 
-  /* Set up the search paths before we go looking for config files.  */
+  /* Set up the search paths.  We add directories that we expect to
+     contain GNU Toolchain components before directories specified by
+     the machine description so that we will find GNU components (like
+     the GNU assembler) before those of the host system.  */ 
 
-  /* These come before the md prefixes so that we will find gcc's subcommands
-     (such as cpp) rather than those of the host system.  */
-  /* Use 2 as fourth arg meaning try just the machine as a suffix,
-     as well as trying the machine and the version.  */
+  /* If we don't know where the toolchain has been installed, use the
+     configured-in locations.  */
+  if (!gcc_exec_prefix)
+    {
 #ifndef OS2
   add_prefix (&exec_prefixes, standard_libexec_prefix, "GCC",
 	      PREFIX_PRIORITY_LAST, 1, 0);
@@ -3948,51 +4013,36 @@
 	      PREFIX_PRIORITY_LAST, 2, 0);
   add_prefix (&exec_prefixes, standard_exec_prefix, "BINUTILS",
 	      PREFIX_PRIORITY_LAST, 2, 0);
+#endif
+      add_prefix (&startfile_prefixes, standard_exec_prefix, "BINUTILS",
+		  PREFIX_PRIORITY_LAST, 1, 0);
+    }
+
+  /* If not cross-compiling, search well-known system locations.  */
+  if (*cross_compile == '0')
+    {
+#ifndef OS2
   add_prefix (&exec_prefixes, standard_exec_prefix_1, "BINUTILS",
 	      PREFIX_PRIORITY_LAST, 2, 0);
   add_prefix (&exec_prefixes, standard_exec_prefix_2, "BINUTILS",
 	      PREFIX_PRIORITY_LAST, 2, 0);
 #endif
-
-  add_prefix (&startfile_prefixes, standard_exec_prefix, "BINUTILS",
-	      PREFIX_PRIORITY_LAST, 1, 0);
   add_prefix (&startfile_prefixes, standard_exec_prefix_2, "BINUTILS",
 	      PREFIX_PRIORITY_LAST, 1, 0);
+    }
 
+  gcc_assert (!IS_ABSOLUTE_PATH (tooldir_base_prefix));
   tooldir_prefix = concat (tooldir_base_prefix, spec_machine,
 			   dir_separator_str, NULL);
 
-  /* If tooldir is relative, base it on exec_prefixes.  A relative
-     tooldir lets us move the installed tree as a unit.
-
-     If GCC_EXEC_PREFIX is defined, then we want to add two relative
-     directories, so that we can search both the user specified directory
-     and the standard place.  */
-
-  if (!IS_ABSOLUTE_PATH (tooldir_prefix))
-    {
-      if (gcc_exec_prefix)
-	{
-	  char *gcc_exec_tooldir_prefix
-	    = concat (gcc_exec_prefix, spec_machine, dir_separator_str,
+  /* Look for tools relative to the location from which the driver is
+     running, or, if that is not available, the configured prefix.  */
+  tooldir_prefix
+    = concat (gcc_exec_prefix ? gcc_exec_prefix : standard_exec_prefix,
+	      spec_machine, dir_separator_str,
 		      spec_version, dir_separator_str, tooldir_prefix, NULL);
 
 	  add_prefix (&exec_prefixes,
-		      concat (gcc_exec_tooldir_prefix, "bin",
-			      dir_separator_str, NULL),
-		      NULL, PREFIX_PRIORITY_LAST, 0, 0);
-	  add_prefix (&startfile_prefixes,
-		      concat (gcc_exec_tooldir_prefix, "lib",
-			      dir_separator_str, NULL),
-		      NULL, PREFIX_PRIORITY_LAST, 0, 1);
-	}
-
-      tooldir_prefix = concat (standard_exec_prefix, spec_machine,
-			       dir_separator_str, spec_version,
-			       dir_separator_str, tooldir_prefix, NULL);
-    }
-
-  add_prefix (&exec_prefixes,
 	      concat (tooldir_prefix, "bin", dir_separator_str, NULL),
 	      "BINUTILS", PREFIX_PRIORITY_LAST, 0, 0);
   add_prefix (&startfile_prefixes,
@@ -4217,6 +4267,7 @@
 #ifdef HAVE_TARGET_OBJECT_SUFFIX
 	  argv[i] = convert_filename (argv[i], 0, access (argv[i], F_OK));
 #endif
+	  CYGPATH (argv[i]);
 
 	  if (strcmp (argv[i], "-") != 0 && access (argv[i], F_OK) < 0)
 	    {
@@ -6313,18 +6364,16 @@
 			      PREFIX_PRIORITY_LAST, 0, 1);
       else if (*cross_compile == '0')
 	{
-	  if (gcc_exec_prefix)
-	    add_prefix (&startfile_prefixes,
-			concat (gcc_exec_prefix, machine_suffix,
-				standard_startfile_prefix, NULL),
-			NULL, PREFIX_PRIORITY_LAST, 0, 1);
 	  add_prefix (&startfile_prefixes,
-		      concat (standard_exec_prefix,
+		      concat (gcc_exec_prefix 
+			      ? gcc_exec_prefix : standard_exec_prefix, 
 			      machine_suffix,
 			      standard_startfile_prefix, NULL),
 		      NULL, PREFIX_PRIORITY_LAST, 0, 1);
 	}
 
+      /* Sysrooted prefixes are relocated because target_system_root is
+	 also relocated by gcc_exec_prefix.  */
       if (*standard_startfile_prefix_1)
  	add_sysrooted_prefix (&startfile_prefixes,
 			      standard_startfile_prefix_1, "BINUTILS",
@@ -6368,7 +6417,9 @@
 
   if (print_search_dirs)
     {
-      printf (_("install: %s%s\n"), standard_exec_prefix, machine_suffix);
+      printf (_("install: %s%s\n"),
+	      gcc_exec_prefix ? gcc_exec_prefix : standard_exec_prefix,
+	      gcc_exec_prefix ? "" : machine_suffix);
       printf (_("programs: %s\n"),
 	      build_search_list (&exec_prefixes, "", false, false));
       printf (_("libraries: %s\n"),
Index: gcc/gensupport.c
===================================================================
--- gcc/gensupport.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/gensupport.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -119,6 +119,18 @@
 static void init_predicate_table (void);
 static void record_insn_name (int, const char *);
 
+static FILE *
+cygpath_fopen (const char *file, const char *mode)
+{
+  FILE *f;
+
+  CYGPATH (file);
+  f = fopen (file, mode);
+  CYGPATH_FREE (file);
+
+  return f;
+}
+
 void
 message_with_line (int lineno, const char *msg, ...)
 {
@@ -220,7 +232,7 @@
 	  static const char sep[2] = { DIR_SEPARATOR, '\0' };
 
 	  pathname = concat (stackp->fname, sep, filename, NULL);
-	  input_file = fopen (pathname, "r");
+	  input_file = cygpath_fopen (pathname, "r");
 	  if (input_file != NULL)
 	    goto success;
 	  free (pathname);
@@ -231,7 +243,7 @@
     pathname = concat (base_dir, filename, NULL);
   else
     pathname = xstrdup (filename);
-  input_file = fopen (pathname, "r");
+  input_file = cygpath_fopen (pathname, "r");
   if (input_file == NULL)
     {
       free (pathname);
@@ -1030,7 +1042,7 @@
 
       read_rtx_filename = in_fname;
       read_rtx_lineno = 1;
-      input_file = fopen (in_fname, "r");
+      input_file = cygpath_fopen (in_fname, "r");
       if (input_file == 0)
 	{
 	  perror (in_fname);
Index: gcc/fold-const.c
===================================================================
--- gcc/fold-const.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/fold-const.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2,6 +2,7 @@
    Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,
    2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008
    Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -2027,7 +2028,13 @@
   if (! overflow)
     {
       tree lt = TYPE_MIN_VALUE (type);
-      REAL_VALUE_TYPE l = real_value_from_int_cst (NULL_TREE, lt);
+      REAL_VALUE_TYPE l;
+
+      if (TYPE_PRECISION (type) >= 32)
+	l = real_value_from_int_cst (NULL_TREE, lt);
+      else
+	real_from_integer (&l, VOIDmode, (HOST_WIDE_INT) -1 << 31,
+			   (HOST_WIDE_INT) -1, 0);
       if (REAL_VALUES_LESS (r, l))
 	{
 	  overflow = 1;
@@ -2041,7 +2048,14 @@
       tree ut = TYPE_MAX_VALUE (type);
       if (ut)
 	{
-	  REAL_VALUE_TYPE u = real_value_from_int_cst (NULL_TREE, ut);
+	  REAL_VALUE_TYPE u;
+
+	  if (TYPE_PRECISION (type) >= 32)
+	    u = real_value_from_int_cst (NULL_TREE, ut);
+	  else
+	    real_from_integer (&u, VOIDmode,
+			       (HOST_WIDE_INT) 32767 << 16 | 65535L,
+			       (HOST_WIDE_INT) 0, 0);
 	  if (REAL_VALUES_LESS (u, r))
 	    {
 	      overflow = 1;
@@ -6610,11 +6624,6 @@
       && (TREE_SIDE_EFFECTS (arg0) || TREE_SIDE_EFFECTS (arg1)))
     return 0;
 
-  if (DECL_P (arg1))
-    return 0;
-  if (DECL_P (arg0))
-    return 1;
-
   /* It is preferable to swap two SSA_NAME to ensure a canonical form
      for commutative and comparison operators.  Ensuring a canonical
      form allows the optimizers to find additional redundancies without
@@ -6624,6 +6633,18 @@
       && SSA_NAME_VERSION (arg0) > SSA_NAME_VERSION (arg1))
     return 1;
 
+  /* Put SSA_NAMEs last.  */
+  if (TREE_CODE (arg1) == SSA_NAME)
+    return 0;
+  if (TREE_CODE (arg0) == SSA_NAME)
+    return 1;
+
+  /* Put variables last.  */
+  if (DECL_P (arg1))
+    return 0;
+  if (DECL_P (arg0))
+    return 1;
+
   return 0;
 }
 
@@ -7968,6 +7989,143 @@
   return NULL_TREE;
 }
 
+/* Helper that tries to canonicalize the comparison ARG0 CODE ARG1
+   by changing CODE to reduce the magnitude of constants involved in
+   ARG0 of the comparison.
+   Returns a canonicalized comparison tree if a simplification was
+   possible, otherwise returns NULL_TREE.
+   Set *STRICT_OVERFLOW_P to true if the canonicalization is only
+   valid if signed overflow is undefined.  */
+
+static tree
+maybe_canonicalize_comparison_1 (enum tree_code code, tree type,
+				 tree arg0, tree arg1,
+				 bool *strict_overflow_p)
+{
+  enum tree_code code0 = TREE_CODE (arg0);
+  tree t, cst0 = NULL_TREE;
+  int sgn0;
+  bool swap = false;
+
+  /* Match A +- CST code arg1 and CST code arg1.  */
+  if (!(((code0 == MINUS_EXPR
+          || code0 == PLUS_EXPR)
+         && TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST)
+	|| code0 == INTEGER_CST))
+    return NULL_TREE;
+
+  /* Identify the constant in arg0 and its sign.  */
+  if (code0 == INTEGER_CST)
+    cst0 = arg0;
+  else
+    cst0 = TREE_OPERAND (arg0, 1);
+  sgn0 = tree_int_cst_sgn (cst0);
+
+  /* Overflowed constants and zero will cause problems.  */
+  if (integer_zerop (cst0)
+      || TREE_OVERFLOW (cst0))
+    return NULL_TREE;
+
+  /* See if we can reduce the magnitude of the constant in
+     arg0 by changing the comparison code.  */
+  if (code0 == INTEGER_CST)
+    {
+      /* CST <= arg1  ->  CST-1 < arg1.  */
+      if (code == LE_EXPR && sgn0 == 1)
+	code = LT_EXPR;
+      /* -CST < arg1  ->  -CST-1 <= arg1.  */
+      else if (code == LT_EXPR && sgn0 == -1)
+	code = LE_EXPR;
+      /* CST > arg1  ->  CST-1 >= arg1.  */
+      else if (code == GT_EXPR && sgn0 == 1)
+	code = GE_EXPR;
+      /* -CST >= arg1  ->  -CST-1 > arg1.  */
+      else if (code == GE_EXPR && sgn0 == -1)
+	code = GT_EXPR;
+      else
+        return NULL_TREE;
+      /* arg1 code' CST' might be more canonical.  */
+      swap = true;
+    }
+  else
+    {
+      /* A - CST < arg1  ->  A - CST-1 <= arg1.  */
+      if (code == LT_EXPR
+	  && code0 == ((sgn0 == -1) ? PLUS_EXPR : MINUS_EXPR))
+	code = LE_EXPR;
+      /* A + CST > arg1  ->  A + CST-1 >= arg1.  */
+      else if (code == GT_EXPR
+	       && code0 == ((sgn0 == -1) ? MINUS_EXPR : PLUS_EXPR))
+	code = GE_EXPR;
+      /* A + CST <= arg1  ->  A + CST-1 < arg1.  */
+      else if (code == LE_EXPR
+	       && code0 == ((sgn0 == -1) ? MINUS_EXPR : PLUS_EXPR))
+	code = LT_EXPR;
+      /* A - CST >= arg1  ->  A - CST-1 > arg1.  */
+      else if (code == GE_EXPR
+	       && code0 == ((sgn0 == -1) ? PLUS_EXPR : MINUS_EXPR))
+	code = GT_EXPR;
+      else
+	return NULL_TREE;
+      *strict_overflow_p = true;
+    }
+
+  /* Now build the constant reduced in magnitude.  */
+  t = int_const_binop (sgn0 == -1 ? PLUS_EXPR : MINUS_EXPR,
+  		       cst0, build_int_cst (TREE_TYPE (cst0), 1), 0);
+  if (code0 != INTEGER_CST)
+    t = fold_build2 (code0, TREE_TYPE (arg0), TREE_OPERAND (arg0, 0), t);
+
+  /* If swapping might yield to a more canonical form, do so.  */
+  if (swap)
+    return fold_build2 (swap_tree_comparison (code), type, arg1, t);
+  else
+    return fold_build2 (code, type, t, arg1);
+}
+
+/* Canonicalize the comparison ARG0 CODE ARG1 with type TYPE with undefined
+   overflow further.  Try to decrease the magnitude of constants involved
+   by changing LE_EXPR and GE_EXPR to LT_EXPR and GT_EXPR or vice versa
+   and put sole constants at the second argument position.
+   Returns the canonicalized tree if changed, otherwise NULL_TREE.  */
+
+static tree
+maybe_canonicalize_comparison (enum tree_code code, tree type,
+			       tree arg0, tree arg1)
+{
+  tree t;
+  bool strict_overflow_p;
+  const char * const warnmsg = G_("assuming signed overflow does not occur "
+				  "when reducing constant in comparison");
+
+  /* In principle pointers also have undefined overflow behavior,
+     but that causes problems elsewhere.  */
+  if (!TYPE_OVERFLOW_UNDEFINED (TREE_TYPE (arg0))
+      || POINTER_TYPE_P (TREE_TYPE (arg0)))
+    return NULL_TREE;
+
+  /* Try canonicalization by simplifying arg0.  */
+  strict_overflow_p = false;
+  t = maybe_canonicalize_comparison_1 (code, type, arg0, arg1,
+				       &strict_overflow_p);
+  if (t)
+    {
+      if (strict_overflow_p)
+	fold_overflow_warning (warnmsg, WARN_STRICT_OVERFLOW_MAGNITUDE);
+      return t;
+    }
+
+  /* Try canonicalization by simplifying arg1 using the swapped
+     comparison.  */
+  code = swap_tree_comparison (code);
+  strict_overflow_p = false;
+  t = maybe_canonicalize_comparison_1 (code, type, arg1, arg0,
+				       &strict_overflow_p);
+  if (t && strict_overflow_p)
+    fold_overflow_warning (warnmsg, WARN_STRICT_OVERFLOW_MAGNITUDE);
+  return t;
+}
+
 /* Return whether BASE + OFFSET may wrap around the address space.
    This is used to avoid issuing overflow warnings for expressions
    like &p->x which can not wrap.  */
@@ -8052,6 +8210,39 @@
 
       lhs = fold_build2 (lhs_add ? PLUS_EXPR : MINUS_EXPR,
 			 TREE_TYPE (arg1), const2, const1);
+      /* If the constant operation overflowed this can be
+	 simplified as a comparison against INT_MAX/INT_MIN.  */
+      if (TREE_CODE (lhs) == INTEGER_CST
+	  && TREE_OVERFLOW (lhs))
+	{
+	  int const1_sgn = tree_int_cst_sgn (const1);
+	  enum tree_code code2 = code;
+
+	  /* Get the sign of the constant on the lhs if the
+	     operation were VARIABLE + CONST1.  */
+	  if (TREE_CODE (arg0) == MINUS_EXPR)
+	    const1_sgn = -const1_sgn;
+
+	  /* The sign of the constant determines if we overflowed
+	     INT_MAX (const1_sgn == -1) or INT_MIN (const1_sgn == 1).
+	     Canonicalize to the INT_MIN overflow by swapping the comparison
+	     if necessary.  */
+	  if (const1_sgn == -1)
+	    code2 = swap_tree_comparison (code);
+
+	  /* We now can look at the canonicalized case
+	       VARIABLE + 1  CODE2  INT_MIN
+	     and decide on the result.  */
+	  if (code2 == LT_EXPR
+	      || code2 == LE_EXPR
+	      || code2 == EQ_EXPR)
+	    return omit_one_operand (type, boolean_false_node, variable);
+	  else if (code2 == NE_EXPR
+		   || code2 == GE_EXPR
+		   || code2 == GT_EXPR)
+	    return omit_one_operand (type, boolean_true_node, variable);
+	}
+
       if (TREE_CODE (lhs) == TREE_CODE (arg1)
 	  && (TREE_CODE (lhs) != INTEGER_CST
 	      || !TREE_OVERFLOW (lhs)))
@@ -8064,6 +8255,110 @@
 	}
     }
 
+  /* For comparisons of pointers we can decompose it to a compile time
+     comparison of the base objects and the offsets into the object.
+     This requires at least one operand being an ADDR_EXPR to do more
+     than the operand_equal_p test below.  */
+  if (POINTER_TYPE_P (TREE_TYPE (arg0))
+      && (TREE_CODE (arg0) == ADDR_EXPR
+	  || TREE_CODE (arg1) == ADDR_EXPR))
+    {
+      tree base0, base1, offset0 = NULL_TREE, offset1 = NULL_TREE;
+      HOST_WIDE_INT bitsize, bitpos0 = 0, bitpos1 = 0;
+      enum machine_mode mode;
+      int volatilep, unsignedp;
+      bool indirect_base0 = false;
+
+      /* Get base and offset for the access.  Strip ADDR_EXPR for
+	 get_inner_reference, but put it back by stripping INDIRECT_REF
+	 off the base object if possible.  */
+      base0 = arg0;
+      if (TREE_CODE (arg0) == ADDR_EXPR)
+	{
+	  base0 = get_inner_reference (TREE_OPERAND (arg0, 0),
+				       &bitsize, &bitpos0, &offset0, &mode,
+				       &unsignedp, &volatilep, false);
+	  if (TREE_CODE (base0) == INDIRECT_REF)
+	    base0 = TREE_OPERAND (base0, 0);
+	  else
+	    indirect_base0 = true;
+	}
+
+      base1 = arg1;
+      if (TREE_CODE (arg1) == ADDR_EXPR)
+	{
+	  base1 = get_inner_reference (TREE_OPERAND (arg1, 0),
+				       &bitsize, &bitpos1, &offset1, &mode,
+				       &unsignedp, &volatilep, false);
+	  /* We have to make sure to have an indirect/non-indirect base1
+	     just the same as we did for base0.  */
+	  if (TREE_CODE (base1) == INDIRECT_REF
+	      && !indirect_base0)
+	    base1 = TREE_OPERAND (base1, 0);
+	  else if (!indirect_base0)
+	    base1 = NULL_TREE;
+	}
+      else if (indirect_base0)
+	base1 = NULL_TREE;
+
+      /* If we have equivalent bases we might be able to simplify.  */
+      if (base0 && base1
+	  && operand_equal_p (base0, base1, 0))
+	{
+	  /* We can fold this expression to a constant if the non-constant
+	     offset parts are equal.  */
+	  if (offset0 == offset1
+	      || (offset0 && offset1
+		  && operand_equal_p (offset0, offset1, 0)))
+	    {
+	      switch (code)
+		{
+		case EQ_EXPR:
+		  return build_int_cst (boolean_type_node, bitpos0 == bitpos1);
+		case NE_EXPR:
+		  return build_int_cst (boolean_type_node, bitpos0 != bitpos1);
+		case LT_EXPR:
+		  return build_int_cst (boolean_type_node, bitpos0 < bitpos1);
+		case LE_EXPR:
+		  return build_int_cst (boolean_type_node, bitpos0 <= bitpos1);
+		case GE_EXPR:
+		  return build_int_cst (boolean_type_node, bitpos0 >= bitpos1);
+		case GT_EXPR:
+		  return build_int_cst (boolean_type_node, bitpos0 > bitpos1);
+		default:;
+		}
+	    }
+	  /* We can simplify the comparison to a comparison of the variable
+	     offset parts if the constant offset parts are equal.
+	     Be careful to use signed size type here because otherwise we
+	     mess with array offsets in the wrong way.  This is possible
+	     because pointer arithmetic is restricted to retain within an
+	     object and overflow on pointer differences is undefined as of
+	     6.5.6/8 and /9 with respect to the signed ptrdiff_t.  */
+	  else if (bitpos0 == bitpos1)
+	    {
+	      tree signed_size_type_node;
+	      signed_size_type_node = signed_type_for (size_type_node);
+
+	      /* By converting to signed size type we cover middle-end pointer
+	         arithmetic which operates on unsigned pointer types of size
+	         type size and ARRAY_REF offsets which are properly sign or
+	         zero extended from their type in case it is narrower than
+	         size type.  */
+	      if (offset0 == NULL_TREE)
+		offset0 = build_int_cst (signed_size_type_node, 0);
+	      else
+		offset0 = fold_convert (signed_size_type_node, offset0);
+	      if (offset1 == NULL_TREE)
+		offset1 = build_int_cst (signed_size_type_node, 0);
+	      else
+		offset1 = fold_convert (signed_size_type_node, offset1);
+
+	      return fold_build2 (code, type, offset0, offset1);
+	    }
+	}
+    }
+
   /* If this is a comparison of two exprs that look like an ARRAY_REF of the
      same object, then we can fold this to a comparison of the two offsets in
      signed size type.  This is possible because pointer arithmetic is
@@ -8118,6 +8413,89 @@
 	}
     }
 
+  /* Transform comparisons of the form X +- C1 CMP Y +- C2 to
+     X CMP Y +- C2 +- C1 for signed X, Y.  This is valid if
+     the resulting offset is smaller in absolute value than the
+     original one.  */
+  if (TYPE_OVERFLOW_UNDEFINED (TREE_TYPE (arg0))
+      && (TREE_CODE (arg0) == PLUS_EXPR || TREE_CODE (arg0) == MINUS_EXPR)
+      && (TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST
+	  && !TREE_OVERFLOW (TREE_OPERAND (arg0, 1)))
+      && (TREE_CODE (arg1) == PLUS_EXPR || TREE_CODE (arg1) == MINUS_EXPR)
+      && (TREE_CODE (TREE_OPERAND (arg1, 1)) == INTEGER_CST
+	  && !TREE_OVERFLOW (TREE_OPERAND (arg1, 1))))
+    {
+      tree const1 = TREE_OPERAND (arg0, 1);
+      tree const2 = TREE_OPERAND (arg1, 1);
+      tree variable1 = TREE_OPERAND (arg0, 0);
+      tree variable2 = TREE_OPERAND (arg1, 0);
+      tree cst;
+      const char * const warnmsg = G_("assuming signed overflow does not "
+				      "occur when combining constants around "
+				      "a comparison");
+
+      /* Put the constant on the side where it doesn't overflow and is
+	 of lower absolute value than before.  */
+      cst = int_const_binop (TREE_CODE (arg0) == TREE_CODE (arg1)
+			     ? MINUS_EXPR : PLUS_EXPR,
+			     const2, const1, 0);
+      if (!TREE_OVERFLOW (cst)
+	  && tree_int_cst_compare (const2, cst) == tree_int_cst_sgn (const2))
+	{
+	  fold_overflow_warning (warnmsg, WARN_STRICT_OVERFLOW_COMPARISON);
+	  return fold_build2 (code, type,
+			      variable1,
+			      fold_build2 (TREE_CODE (arg1), TREE_TYPE (arg1),
+					   variable2, cst));
+	}
+
+      cst = int_const_binop (TREE_CODE (arg0) == TREE_CODE (arg1)
+			     ? MINUS_EXPR : PLUS_EXPR,
+			     const1, const2, 0);
+      if (!TREE_OVERFLOW (cst)
+	  && tree_int_cst_compare (const1, cst) == tree_int_cst_sgn (const1))
+	{
+	  fold_overflow_warning (warnmsg, WARN_STRICT_OVERFLOW_COMPARISON);
+	  return fold_build2 (code, type,
+			      fold_build2 (TREE_CODE (arg0), TREE_TYPE (arg0),
+					   variable1, cst),
+			      variable2);
+	}
+    }
+
+  /* Transform comparisons of the form X * C1 CMP 0 to X CMP 0 in the
+     signed arithmetic case.  That form is created by the compiler
+     often enough for folding it to be of value.  One example is in
+     computing loop trip counts after Operator Strength Reduction.  */
+  if (TYPE_OVERFLOW_UNDEFINED (TREE_TYPE (arg0))
+      && TREE_CODE (arg0) == MULT_EXPR
+      && (TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST
+          && !TREE_OVERFLOW (TREE_OPERAND (arg0, 1)))
+      && integer_zerop (arg1))
+    {
+      tree const1 = TREE_OPERAND (arg0, 1);
+      tree const2 = arg1;                       /* zero */
+      tree variable1 = TREE_OPERAND (arg0, 0);
+      enum tree_code cmp_code = code;
+
+      gcc_assert (!integer_zerop (const1));
+
+      fold_overflow_warning (("assuming signed overflow does not occur when "
+			      "eliminating multiplication in comparison "
+			      "with zero"),
+			     WARN_STRICT_OVERFLOW_COMPARISON);
+
+      /* If const1 is negative we swap the sense of the comparison.  */
+      if (tree_int_cst_sgn (const1) < 0)
+        cmp_code = swap_tree_comparison (cmp_code);
+
+      return fold_build2 (cmp_code, type, variable1, const2);
+    }
+
+  tem = maybe_canonicalize_comparison (code, type, op0, op1);
+  if (tem)
+    return tem;
+
   if (FLOAT_TYPE_P (TREE_TYPE (arg0)))
     {
       tree targ0 = strip_float_extensions (arg0);
@@ -9092,7 +9470,7 @@
 	 Also note that operand_equal_p is always false if an operand
 	 is volatile.  */
 
-      if ((! FLOAT_TYPE_P (type) || flag_unsafe_math_optimizations)
+      if ((!FLOAT_TYPE_P (type) || !HONOR_NANS (TYPE_MODE (type)))
 	  && operand_equal_p (arg0, arg1, 0))
 	return fold_convert (type, integer_zero_node);
 
Index: gcc/genautomata.c
===================================================================
--- gcc/genautomata.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/genautomata.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -8974,6 +8974,7 @@
 		strlen (STANDARD_OUTPUT_DESCRIPTION_FILE_SUFFIX) + 1);
   obstack_1grow (&irp, '\0');
   output_description_file_name = obstack_base (&irp);
+  CYGPATH (output_description_file_name);
   obstack_finish (&irp);
 }
 
Index: gcc/tree-gimple.c
===================================================================
--- gcc/tree-gimple.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/tree-gimple.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -508,3 +508,48 @@
       gcc_unreachable ();
    }
 }
+
+/* Canonicalize a tree T for use in a COND_EXPR as conditional.  Returns
+   a canonicalized tree that is valid for a COND_EXPR or NULL_TREE, if
+   we failed to create one.  */
+
+tree
+canonicalize_cond_expr_cond (tree t)
+{
+  /* For (bool)x use x != 0.  */
+  if (TREE_CODE (t) == NOP_EXPR
+      && TREE_TYPE (t) == boolean_type_node)
+    {
+      tree top0 = TREE_OPERAND (t, 0);
+      t = build2 (NE_EXPR, TREE_TYPE (t),
+		  top0, build_int_cst (TREE_TYPE (top0), 0));
+    }
+  /* For !x use x == 0.  */
+  else if (TREE_CODE (t) == TRUTH_NOT_EXPR)
+    {
+      tree top0 = TREE_OPERAND (t, 0);
+      t = build2 (EQ_EXPR, TREE_TYPE (t),
+		  top0, build_int_cst (TREE_TYPE (top0), 0));
+    }
+  /* For cmp ? 1 : 0 use cmp.  */
+  else if (TREE_CODE (t) == COND_EXPR
+	   && COMPARISON_CLASS_P (TREE_OPERAND (t, 0))
+	   && integer_onep (TREE_OPERAND (t, 1))
+	   && integer_zerop (TREE_OPERAND (t, 2)))
+    {
+      tree top0 = TREE_OPERAND (t, 0);
+      t = build2 (TREE_CODE (top0), TREE_TYPE (t),
+		  TREE_OPERAND (top0, 0), TREE_OPERAND (top0, 1));
+    }
+
+  /* A valid conditional for a COND_EXPR is either a gimple value
+     or a comparison with two gimple value operands.  */
+  if (is_gimple_val (t)
+      || (COMPARISON_CLASS_P (t)
+	  && is_gimple_val (TREE_OPERAND (t, 0))
+	  && is_gimple_val (TREE_OPERAND (t, 1))))
+    return t;
+
+  return NULL_TREE;
+}
+
Index: gcc/tree-gimple.h
===================================================================
--- gcc/tree-gimple.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/tree-gimple.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -133,6 +133,7 @@
 struct gimplify_omp_ctx;
 extern void omp_firstprivatize_variable (struct gimplify_omp_ctx *, tree);
 extern tree gimple_boolify (tree);
+extern tree canonicalize_cond_expr_cond (tree);
 
 /* In omp-low.c.  */
 extern void diagnose_omp_structured_block_errors (tree);
Index: gcc/DEV-PHASE
===================================================================
--- gcc/DEV-PHASE	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/DEV-PHASE	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1 @@
+snapshot
Index: gcc/testsuite/gcc.target/sh/sh.exp
===================================================================
--- gcc/testsuite/gcc.target/sh/sh.exp	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/testsuite/gcc.target/sh/sh.exp	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,41 @@
+#   Copyright (C) 2007 Free Software Foundation, Inc.
+
+# This program is free software; you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 2 of the License, or
+# (at your option) any later version.
+# 
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
+# 
+# You should have received a copy of the GNU General Public License
+# along with this program; if not, write to the Free Software
+# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.  
+
+# GCC testsuite that uses the `gcc-dg.exp' driver, just a single option, no
+# looping over tests.
+
+# Exit immediately if this isn't a IS target.
+if ![istarget sh-*-*] then {
+  return
+}
+
+# Load support procs.
+load_lib gcc-dg.exp
+
+# If a testcase doesn't have special options, use these.
+global DEFAULT_CFLAGS
+if ![info exists DEFAULT_CFLAGS] then {
+    set DEFAULT_CFLAGS " -ansi -pedantic-errors"
+}
+
+# Initialize `dg'.
+dg-init
+
+# Main loop.
+dg-runtest [lsort [glob -nocomplain $srcdir/$subdir/*.\[cS\]]] "" $DEFAULT_CFLAGS
+
+# All done.
+dg-finish
Index: gcc/testsuite/gcc.target/sh/fpchg1.c
===================================================================
--- gcc/testsuite/gcc.target/sh/fpchg1.c	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/testsuite/gcc.target/sh/fpchg1.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,24 @@
+/* { dg-do run } */
+/* { dg-options "-O1 -m4-300" } */
+
+/* Check that the fpchg instruction is not moved in a delay slot if the
+   fallthru block uses the mode.  */
+
+__attribute__ ((weak))
+void barrier(void)
+{
+}
+
+float f;
+int i;
+double d;
+
+int main()
+{
+  i = 4;
+
+  barrier();
+
+  i = (f + (i && f && d));
+  return i;
+}
Index: gcc/testsuite/gcc.target/sh/fpchg2.c
===================================================================
--- gcc/testsuite/gcc.target/sh/fpchg2.c	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/testsuite/gcc.target/sh/fpchg2.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,16 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -m4-300" } */
+
+/* Make sure that fpchg is preferred over lfd.s fpscr.  */
+/* { dg-final { scan-assembler "fpchg" } } */
+/* { dg-final { scan-assembler-not "fpscr" } } */
+
+extern float c;
+
+void
+foo(int j)
+{
+  while (j--)
+    c++;
+
+}
Index: gcc/testsuite/gcc.dg/nested-func-4.c
===================================================================
--- gcc/testsuite/gcc.dg/nested-func-4.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/testsuite/gcc.dg/nested-func-4.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,4 +1,4 @@
-/* { dg-do run } */
+/* { dg-do run { xfail sh-superh-elf } } */ 
 /* { dg-options "-pg" } */
 /* { dg-options "-pg -static" { target hppa*-*-hpux* } } */
 /* { dg-require-profiling "-pg" } */
Index: gcc/testsuite/gcc.dg/packed-array.c
===================================================================
--- gcc/testsuite/gcc.dg/packed-array.c	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/testsuite/gcc.dg/packed-array.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,40 @@
+/* { dg-do run } */
+/* { dg-options "-O2 -fno-inline" } */
+
+struct usb_interface_descriptor {
+ unsigned short wMaxPacketSize;
+  char e;
+} __attribute__ ((packed));
+
+struct usb_device {
+ int devnum;
+ struct usb_interface_descriptor if_desc[2];
+};
+
+extern int printf (const char *, ...);
+
+void foo (unsigned short a)
+{
+  printf ("%d\n", a);
+}
+
+struct usb_device ndev;
+
+void usb_set_maxpacket(int n)
+{
+  int i;
+
+  for(i=0; i<n;i++)
+    foo((&ndev)->if_desc[i].wMaxPacketSize);
+}
+
+int
+main()
+{
+  usb_set_maxpacket(2);
+  return 0;
+}
+
+
+
+
Index: gcc/testsuite/gcc.dg/attr-isr.c
===================================================================
--- gcc/testsuite/gcc.dg/attr-isr.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/testsuite/gcc.dg/attr-isr.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -16,3 +16,4 @@
 /* { dg-final { scan-assembler-times "\[^f\]r\[0-9\]\[ \t\]*," 8 } } */
 /* { dg-final { scan-assembler-not "\[^f\]r1\[0-3\]" } } */
 /* { dg-final { scan-assembler-times "macl" 2} } */
+/* { dg-final { scan-assembler-not "rte.*\n.*r15\[+\],r\[0-7\]\n" } } */
Index: gcc/testsuite/gcc.dg/long-long-compare-1.c
===================================================================
--- gcc/testsuite/gcc.dg/long-long-compare-1.c	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/testsuite/gcc.dg/long-long-compare-1.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,126 @@
+/* Problem only noticed on SH for -mcbranchdi DImode comparison with constants.
+ * Target dependant failure but test valid for alls.  */
+/* { dg-do run } */
+/* { dg-options "-O0" } */
+/* { dg-options "-O0 -mcbranchdi" { target sh4-*-* } } */
+
+extern void abort(void);
+extern void exit(int);
+
+int test2(long long n)
+{
+  if (n < 2)
+    return 1;
+  return 0;
+}
+
+int test1(long long n)
+{
+  if (n < 1)
+    return 1;
+  return 0;
+}
+
+int test0(long long n)
+{
+  if (n < 0)
+    return 1;
+  return 0;
+}
+
+int test1n(long long n)
+{
+  if (n < -1LL)
+    return 1;
+  return 0;
+}
+
+int test2n(long long n)
+{
+  if (n < -2LL)
+    return 1;
+  return 0;
+}
+
+int main()
+{
+  if (test2n (-1LL))
+    abort ();
+
+  if (test2n (-2LL))
+    abort ();
+
+  if (test2n (0LL))
+    abort ();
+
+  if (test2n (1LL))
+    abort ();
+
+  if (test2n (2LL))
+    abort ();
+ 
+  if (test1n (-1LL))
+    abort ();
+
+  if (!test1n (-2LL))
+    abort ();
+
+  if (test1n (0LL))
+    abort ();
+
+  if (test1n (1LL))
+    abort ();
+
+  if (test1n (2LL))
+    abort ();
+
+  if (!test0 (-1LL))
+    abort ();
+
+  if (!test0 (-2LL))
+    abort ();
+
+  if (test0 (0LL))
+    abort ();
+
+  if (test0 (1LL))
+    abort ();
+
+  if (test0 (2LL))
+    abort ();
+
+  if (!test2 (-1LL))
+    abort ();
+
+  if (!test2 (-2LL))
+    abort ();
+
+  if (!test2 (0LL))
+    abort ();
+
+  if (!test2 (1LL))
+    abort ();
+
+  if (test2 (2LL))
+    abort ();
+
+  if (!test1 (-1LL))
+    abort ();
+
+  if (!test1 (-2LL))
+    abort ();
+
+  if (!test1 (0LL))
+    abort ();
+
+  if (test1 (1LL))
+    abort ();
+
+  if (test1 (2LL))
+    abort ();
+
+  exit (0);
+}
+
+
+
Index: gcc/testsuite/gcc.dg/fold-sub.c
===================================================================
--- gcc/testsuite/gcc.dg/fold-sub.c	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/testsuite/gcc.dg/fold-sub.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-options "-ffinite-math-only -fdump-tree-gimple" } */
+
+float f(float x)
+{
+  return x - x;
+}
+
+/* Substraction should be turned into 0.  */
+
+/* { dg-final { scan-tree-dump-not " - " "gimple" } } */
+/* { dg-final { cleanup-tree-dump "gimple" } } */
Index: gcc/testsuite/gcc.dg/const-weak.c
===================================================================
--- gcc/testsuite/gcc.dg/const-weak.c	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/testsuite/gcc.dg/const-weak.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,16 @@
+/* weak constants can be replaced at link time. */
+
+/* { dg-do compile } */
+/* { dg-options "-O2 -fdump-tree-ccp" } */
+
+const int wconst __attribute__((weak)) = 2;
+
+int f(void)
+{
+  return wconst;
+}
+
+/* { dg-final { scan-tree-dump-not "return 2" "ccp"} } */
+/* { dg-final { cleanup-tree-dump "ccp" } } */
+
+	
Index: gcc/testsuite/gcc.dg/pr32450.c
===================================================================
--- gcc/testsuite/gcc.dg/pr32450.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/testsuite/gcc.dg/pr32450.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,6 +1,6 @@
 /* Contributed by Joost VandeVondele  <jv244@cam.ac.uk> */
 
-/* { dg-do run } */
+/* { dg-do run { xfail sh-*-* } } */
 /* { dg-require-profiling "-pg" } */
 /* { dg-options "-O2 -pg" } */
 /* { dg-options "-O2 -pg -static" { target hppa*-*-hpux* } } */
Index: gcc/testsuite/gcc.dg/nest.c
===================================================================
--- gcc/testsuite/gcc.dg/nest.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/testsuite/gcc.dg/nest.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,5 +1,5 @@
 /* PR 5967, PR 7114 */
-/* { dg-do run } */
+/* { dg-do run { xfail sh-superh-elf } } */ 
 /* { dg-require-profiling "-pg" } */
 /* { dg-options "-O2 -pg" } */
 /* { dg-options "-O2 -pg -static" { target hppa*-*-hpux* } } */
Index: gcc/testsuite/gcc.dg/tree-ssa/loop-17.c
===================================================================
--- gcc/testsuite/gcc.dg/tree-ssa/loop-17.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/testsuite/gcc.dg/tree-ssa/loop-17.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -15,5 +15,5 @@
   return i;
 }
 
-/* { dg-final { scan-tree-dump "set_nb_iterations_in_loop = 2" "sccp" } } */
+/* { dg-final { scan-tree-dump "set_nb_iterations_in_loop = 1" "sccp" } } */
 /* { dg-final { cleanup-tree-dump "sccp" } } */
Index: gcc/testsuite/gcc.dg/cpp/trad/include.c
===================================================================
--- gcc/testsuite/gcc.dg/cpp/trad/include.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/testsuite/gcc.dg/cpp/trad/include.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -4,6 +4,5 @@
 
 /* { dg-do preprocess } */
 
-#define __STDC__ 1	/* Stop complaints about non-ISO compilers.  */
 #define stdio 1
 #include <stdio.h>		/* { dg-bogus "o such file or directory" } */
Index: gcc/testsuite/gcc.dg/strict-overflow-5.c
===================================================================
--- gcc/testsuite/gcc.dg/strict-overflow-5.c	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/testsuite/gcc.dg/strict-overflow-5.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,19 @@
+/* { dg-do compile } */
+/* { dg-options "-fstrict-overflow -O2 -fdump-tree-optimized" } */
+
+/* We can only unroll when using strict overflow semantics.  */
+
+int foo (int i)
+{
+  int index;
+  int r=0;
+ 
+  for (index = i; index <= i+4; index+=2) 
+    r++;
+ 
+  return r;
+}
+
+/* { dg-final { scan-tree-dump "return 3" "optimized" } } */
+/* { dg-final { cleanup-tree-dump "optimized" } } */
+
Index: gcc/testsuite/ChangeLog.STM
===================================================================
--- gcc/testsuite/ChangeLog.STM	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/testsuite/ChangeLog.STM	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,48 @@
+2009-05-27  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/cpp/trad/include.c: Don't force __STDC__.
+
+2008-12-15  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/nest.c: xfail for sh-superh-elf.
+	* gcc.db/nested-func4.c: Likewise.
+	* gcc.db/pr32450.c: Likewise.
+
+2006-10-13  Carlos O'Donell  <carlos@codesourcery.com>
+
+	* lib/c-torture.exp: Use target-libpath.exp.
+	* lib/target-libpath.exp (set_ld_library_path_env_vars): If present,
+	set GCC_EXEC_PREFIX env var from global variable of same name.
+
+2008-05-06  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/28671
+	* gcc.dg/const-weak.c: New testcase. 
+
+2008-04-17  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/28594
+	* gcc.dg/long-long-compare-1.c: New testcase. 
+	
+2006-12-10  Zdenek Dvorak <dvorakz@suse.cz>
+
+	* gcc.dg/tree-ssa/loop-17.c: Update outcome.
+
+2008-01-28  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=3313
+	* gcc.dg/packed-array.c: New testcase. 
+
+2007-08-21  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/fold-sub.c: New test.
+
+2007-06-20  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/attr-isr.c: Test delay slot content.
+
+2007-02-16  Richard Guenther  <rguenther@suse.de>
+	    Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/strict-overflow-5.c: New testcase.
+
Index: gcc/testsuite/lib/c-torture.exp
===================================================================
--- gcc/testsuite/lib/c-torture.exp	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/testsuite/lib/c-torture.exp	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -17,6 +17,7 @@
 # This file was written by Rob Savoye. (rob@cygnus.com)
 
 load_lib file-format.exp
+load_lib target-libpath.exp
 
 # The default option list can be overridden by
 # TORTURE_OPTIONS="{ { list1 } ... { listN } }"
@@ -40,6 +41,19 @@
 	{ -Os } ]
 }
 
+global GCC_UNDER_TEST
+if ![info exists GCC_UNDER_TEST] {
+    set GCC_UNDER_TEST "[find_gcc]"
+}
+
+global orig_environment_saved
+
+# This file may be sourced, so don't override environment settings
+# that have been previously setup.
+if { $orig_environment_saved == 0 } {
+    append ld_library_path [gcc-set-multilib-library-path $GCC_UNDER_TEST]
+    set_ld_library_path_env_vars
+}
 
 # Split TORTURE_OPTIONS into two choices: one for testcases with loops and
 # one for testcases without loops.
Index: gcc/testsuite/lib/target-libpath.exp
===================================================================
--- gcc/testsuite/lib/target-libpath.exp	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/testsuite/lib/target-libpath.exp	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -50,6 +50,12 @@
   global orig_ld_library_path_32
   global orig_ld_library_path_64
   global orig_dyld_library_path
+  global GCC_EXEC_PREFIX
+
+  # Set the relocated compiler prefix, but only if the user hasn't specified one.
+  if { [info exists GCC_EXEC_PREFIX] && ![info exists env(GCC_EXEC_PREFIX)] } {
+    setenv GCC_EXEC_PREFIX "$GCC_EXEC_PREFIX"
+  }
 
   # Setting the ld library path causes trouble when testing cross-compilers.
   if { [is_remote target] } {
Index: gcc/testsuite/gfortran.dg/actual_array_constructor_2.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/pure_dummy_length_1.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/used_types_4.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/assumed_charlen_function_1.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/assumed_charlen_function_2.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/assumed_charlen_function_3.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/derived_init_2.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/alloc_comp_basics_2.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/char_pointer_dummy.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/used_types_9.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/matmul_3.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/external_procedures_1.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/alloc_comp_constructor_1.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/parent_result_ref_1.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/array_initializer_1.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/parent_result_ref_2.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/namelist_4.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/char_pointer_assign.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/parent_result_ref_3.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/actual_array_substr_1.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/proc_assign_1.f90
===================================================================
Index: gcc/testsuite/gfortran.dg/parent_result_ref_4.f90
===================================================================
Index: gcc/cp/decl.c
===================================================================
--- gcc/cp/decl.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/cp/decl.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -5503,6 +5503,32 @@
   return decl;
 }
 
+/* Returns the type for the argument to "__cxa_atexit" (or "atexit",
+   if "__cxa_atexit" is not being used) corresponding to the function
+   to be called when the program exits.  */
+
+static tree
+get_atexit_fn_ptr_type (void)
+{
+  tree arg_types;
+  tree fn_type;
+
+  if (!atexit_fn_ptr_type_node)
+    {
+      if (flag_use_cxa_atexit)
+	/* The parameter to "__cxa_atexit" is "void (*)(void *)".  */
+	arg_types = tree_cons (NULL_TREE, ptr_type_node, void_list_node);
+      else
+	/* The parameter to "atexit" is "void (*)(void)".  */
+	arg_types = void_list_node;
+      
+      fn_type = build_function_type (void_type_node, arg_types);
+      atexit_fn_ptr_type_node = build_pointer_type (fn_type);
+    }
+
+  return atexit_fn_ptr_type_node;
+}
+
 /* Returns a pointer to the `atexit' function.  Note that if
    FLAG_USE_CXA_ATEXIT is nonzero, then this will actually be the new
    `__cxa_atexit' function specified in the IA64 C++ ABI.  */
@@ -5533,8 +5559,7 @@
       /* First, build the pointer-to-function type for the first
 	 argument.  */
       arg_types = tree_cons (NULL_TREE, ptr_type_node, void_list_node);
-      fn_type = build_function_type (void_type_node, arg_types);
-      fn_ptr_type = build_pointer_type (fn_type);
+      fn_ptr_type = get_atexit_fn_ptr_type ();
       /* Then, build the rest of the argument types.  */
       arg_types = tree_cons (NULL_TREE, ptr_type_node, void_list_node);
       if (use_aeabi_atexit)
@@ -5605,7 +5630,6 @@
 start_cleanup_fn (void)
 {
   char name[32];
-  tree parmtypes;
   tree fntype;
   tree fndecl;
 
@@ -5614,19 +5638,10 @@
   /* No need to mangle this.  */
   push_lang_context (lang_name_c);
 
-  /* Build the parameter-types.  */
-  parmtypes = void_list_node;
-  /* Functions passed to __cxa_atexit take an additional parameter.
-     We'll just ignore it.  After we implement the new calling
-     convention for destructors, we can eliminate the use of
-     additional cleanup functions entirely in the -fnew-abi case.  */
-  if (flag_use_cxa_atexit)
-    parmtypes = tree_cons (NULL_TREE, ptr_type_node, parmtypes);
-  /* Build the function type itself.  */
-  fntype = build_function_type (void_type_node, parmtypes);
   /* Build the name of the function.  */
   sprintf (name, "__tcf_%d", start_cleanup_cnt++);
   /* Build the function declaration.  */
+  fntype = TREE_TYPE (get_atexit_fn_ptr_type ());
   fndecl = build_lang_decl (FUNCTION_DECL, get_identifier (name), fntype);
   /* It's a function with internal linkage, generated by the
      compiler.  */
@@ -5678,12 +5693,35 @@
   tree compound_stmt;
   tree args;
   tree fcall;
+  tree type;
+  bool use_dtor;
 
-  if (TYPE_HAS_TRIVIAL_DESTRUCTOR (TREE_TYPE (decl)))
+  type = TREE_TYPE (decl);
+  if (TYPE_HAS_TRIVIAL_DESTRUCTOR (type))
     return void_zero_node;
 
+  /* If we're using "__cxa_atexit" (or "__aeabi_atexit"), and DECL is
+     a class object, we can just pass the destructor to
+     "__cxa_atexit"; we don't have to build a temporary function to do
+     the cleanup.  */
+  use_dtor = (flag_use_cxa_atexit 
+	      /*	      && !targetm.cxx.use_atexit_for_cxa_atexit () */
+	      && CLASS_TYPE_P (type));
+  if (use_dtor)
+    {
+      int idx;
+
+      /* Find the destructor.  */
+      idx = lookup_fnfields_1 (type, complete_dtor_identifier);
+      gcc_assert (idx >= 0);
+      cleanup = VEC_index (tree, CLASSTYPE_METHOD_VEC (type), idx);
+      /* Make sure it is accessible.  */
+      perform_or_defer_access_check (TYPE_BINFO (type), cleanup, cleanup);
+    }
+  else
+    {
   /* Call build_cleanup before we enter the anonymous function so that
-     any access checks will be done relative to the current scope,
+	 any anccess checks will be done relative to the current scope,
      rather than the scope of the anonymous function.  */
   build_cleanup (decl);
 
@@ -5704,26 +5742,43 @@
   finish_expr_stmt (fcall);
   finish_compound_stmt (compound_stmt);
   end_cleanup_fn ();
+    }
 
   /* Call atexit with the cleanup function.  */
-  cxx_mark_addressable (cleanup);
   mark_used (cleanup);
-  cleanup = build_unary_op (ADDR_EXPR, cleanup, 0);
+  cleanup = build_address (cleanup);
   if (flag_use_cxa_atexit)
     {
+      tree addr;
+
+      if (use_dtor)
+	{
+	  /* We must convert CLEANUP to the type that "__cxa_atexit"
+	     expects.  */
+	  cleanup = build_nop (get_atexit_fn_ptr_type (), cleanup);
+	  /* "__cxa_atexit" will pass the address of DECL to the
+	     cleanup function.  */
+	  mark_used (decl);
+	  addr = build_address (decl);
+	  /* The declared type of the parameter to "__cxa_atexit" is
+	     "void *".  For plain "T*", we could just let the
+	     machinery in build_function_call convert it -- but if the
+	     type is "cv-qualified T *", then we need to convert it
+	     before passing it in, to avoid spurious errors.  */
+	  addr = build_nop (ptr_type_node, addr);
+	}
+      else
+	/* Since the cleanup functions we build ignore the address
+	   they're given, there's no reason to pass the actual address
+	   in, and, in general, it's cheaper to pass NULL than any
+	   other value.  */
+	addr = null_pointer_node;
       args = tree_cons (NULL_TREE,
 			build_unary_op (ADDR_EXPR, get_dso_handle_node (), 0),
 			NULL_TREE);
-      if (targetm.cxx.use_aeabi_atexit ())
-	{
-	  args = tree_cons (NULL_TREE, cleanup, args);
-	  args = tree_cons (NULL_TREE, null_pointer_node, args);
-	}
-      else
-	{
-	  args = tree_cons (NULL_TREE, null_pointer_node, args);
+      args = tree_cons (NULL_TREE, addr, args);
 	  args = tree_cons (NULL_TREE, cleanup, args);
-	}
+
     }
   else
     args = tree_cons (NULL_TREE, cleanup, NULL_TREE);
@@ -11622,10 +11677,7 @@
       if (TREE_CODE (type) == ARRAY_TYPE)
 	rval = decl;
       else
-	{
-	  cxx_mark_addressable (decl);
-	  rval = build_unary_op (ADDR_EXPR, decl, 0);
-	}
+	rval = build_address (decl);
 
       /* Optimize for space over speed here.  */
       if (!has_vbases || flag_expensive_optimizations)
Index: gcc/cp/ChangeLog.STM
===================================================================
--- gcc/cp/ChangeLog.STM	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/cp/ChangeLog.STM	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,16 @@
+2009-03-26  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl27506
+        * name-lookup.c: (do_nonmember_using_decl): Fixed error handling.
+
+2007-05-31  Mark Mitchell  <mark@codesourcery.com>
+
+	* decl.c (get_atexit_fn_ptr_type): New function.
+	(get_atexit_node): Use it.
+	(start_cleanup_fn): Likewise.
+	(register_dtor_fn): Use the object's destructor, instead of a
+	separate cleanup function, where possible.
+	* cp-tree.h (CPTI_ATEXIT_FN_PTR_TYPE): New enumerator.
+	(atexit_fn_ptr_type_node): New macro.
+	* decl2.c (build_cleanup): Use build_address.
+
Index: gcc/cp/cp-tree.h
===================================================================
--- gcc/cp/cp-tree.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/cp/cp-tree.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -530,6 +530,7 @@
     CPTI_JCLASS,
     CPTI_TERMINATE,
     CPTI_CALL_UNEXPECTED,
+    CPTI_ATEXIT_FN_PTR_TYPE,
     CPTI_ATEXIT,
     CPTI_DSO_HANDLE,
     CPTI_DCAST,
@@ -619,6 +620,10 @@
 /* The declaration for "__cxa_call_unexpected".  */
 #define call_unexpected_node		cp_global_trees[CPTI_CALL_UNEXPECTED]
 
+/* The type of the function-pointer argument to "__cxa_atexit" (or
+   "std::atexit", if "__cxa_atexit" is not being used).  */
+#define atexit_fn_ptr_type_node         cp_global_trees[CPTI_ATEXIT_FN_PTR_TYPE]
+
 /* A pointer to `std::atexit'.  */
 #define atexit_node			cp_global_trees[CPTI_ATEXIT]
 
Index: gcc/cp/name-lookup.c
===================================================================
--- gcc/cp/name-lookup.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/cp/name-lookup.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2163,9 +2163,13 @@
     }
   else
     {
-      *newval = decls.value;
-      if (oldval && !decls_match (*newval, oldval))
+      if (oldval && !decls_match (decls.value, oldval))
+	{
 	error ("%qD is already declared in this scope", name);
+	  return;
+	}
+
+      *newval = decls.value;
     }
 
   *newtype = decls.type;
Index: gcc/cp/decl2.c
===================================================================
--- gcc/cp/decl2.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/cp/decl2.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2187,10 +2187,7 @@
   if (TREE_CODE (type) == ARRAY_TYPE)
     temp = decl;
   else
-    {
-      cxx_mark_addressable (decl);
-      temp = build1 (ADDR_EXPR, build_pointer_type (type), decl);
-    }
+    temp = build_address (decl);
   temp = build_delete (TREE_TYPE (temp), temp,
 		       sfk_complete_destructor,
 		       LOOKUP_NORMAL|LOOKUP_NONVIRTUAL|LOOKUP_DESTRUCTOR, 0);
Index: gcc/tree-ssa-ccp.c
===================================================================
--- gcc/tree-ssa-ccp.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/tree-ssa-ccp.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -344,6 +344,7 @@
 	   && TREE_READONLY (sym)
 	   && !MTAG_P (sym)
 	   && DECL_INITIAL (sym)
+	   && !DECL_WEAK (sym)
 	   && ccp_decl_initial_min_invariant (DECL_INITIAL (sym)))
     {
       /* Globals and static variables declared 'const' take their
Index: gcc/gthr-objc-generic.c
===================================================================
--- gcc/gthr-objc-generic.c	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/gthr-objc-generic.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,221 @@
+/* Threads compatibility routines for libobjc.  */
+/* Compile this one with gcc.  */
+/* Copyright (C) 1997, 1999, 2000, 2006 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA
+02110-1301, USA.  */
+
+/* As a special exception, if you link this library with other files,
+   some of which are compiled with GCC, to produce an executable,
+   this library does not by itself cause the resulting executable
+   to be covered by the GNU General Public License.
+   This exception does not however invalidate any other reasons why
+   the executable file might be covered by the GNU General Public License.  */
+
+#include "tconfig.h"
+
+#define __GTHR_WEAK __attribute__ ((weak))
+#define _LIBOBJC
+
+/* ??? The objc thread types are defined in ../libobjc/objc/thr.h,
+   but we don't want the gcc core to depend on libobjc.  */
+typedef void * objc_thread_t;
+typedef struct objc_mutex *objc_mutex_t;
+typedef struct objc_condition *objc_condition_t;
+#define OBJC_THREAD_INTERACTIVE_PRIORITY        2
+
+#include "gthr.h"
+
+#define UNUSED(x) x ATTRIBUTE_UNUSED
+
+/* Just provide compatibility for mutex handling.  */
+
+/* Thread local storage for a single thread */
+static void *thread_local_storage = 0;
+
+/* Backend initialization functions */
+
+/* Initialize the threads subsystem.  */
+int
+__generic_gxx_objc_init_thread_system (void)
+{
+  /* No thread support available */
+  return -1;
+}
+
+/* Close the threads subsystem.  */
+int
+__generic_gxx_objc_close_thread_system (void)
+{
+  /* No thread support available */
+  return -1;
+}
+
+/* Backend thread functions */
+
+/* Create a new thread of execution.  The thread starts executing by calling
+   FUNC with ARG as its only argument.
+   On success, a handle for the new thread is returned.
+   On failure, zero is returned.  */
+objc_thread_t
+__generic_gxx_objc_thread_detach (void UNUSED ((* func)(void *)),
+				  void * UNUSED(arg))
+{
+  /* No thread support available */
+  return 0;
+}
+
+/* Set the current thread's priority.  */
+int
+__generic_gxx_objc_thread_set_priority (int UNUSED(priority))
+{
+  /* No thread support available */
+  return -1;
+}
+
+/* Return the current thread's priority.  */
+int
+__generic_gxx_objc_thread_get_priority (void)
+{
+  return OBJC_THREAD_INTERACTIVE_PRIORITY;
+}
+
+/* Yield our process time to another thread.  */
+void
+__generic_gxx_objc_thread_yield (void)
+{
+  return;
+}
+
+/* Terminate the current thread.  */
+int
+__generic_gxx_objc_thread_exit (void)
+{
+  /* No thread support available */
+  /* Should we really exit the program */
+  /* exit (&__objc_thread_exit_status); */
+  return -1;
+}
+
+/* Returns an integer value which uniquely describes a thread.  */
+objc_thread_t
+__generic_gxx_objc_thread_id (void)
+{
+  /* No thread support, use 1.  */
+  return (objc_thread_t) 1;
+}
+
+/* Sets the thread's objc local storage pointer.  */
+int
+__generic_gxx_objc_thread_set_data (void *value)
+{
+  thread_local_storage = value;
+  return 0;
+}
+
+/* Returns the thread's objc local storage pointer.  */
+void *
+__generic_gxx_objc_thread_get_data (void)
+{
+  return thread_local_storage;
+}
+
+/* Backend mutex functions */
+
+/* Allocate a backend-specific mutex data in MUTEX->backend.  
+   Return 0 on success, -1 for failure.  */
+int
+__generic_gxx_objc_mutex_allocate (objc_mutex_t UNUSED(mutex))
+{
+  return 0;
+}
+
+/* Deallocate backend-specific mutex data in MUTEX->backend.
+   Return 0 on success, -1 for failure.  */
+int
+__generic_gxx_objc_mutex_deallocate (objc_mutex_t UNUSED(mutex))
+{
+  return 0;
+}
+
+/* Grab a lock on MUTEX.  Return 0 on success.  */
+int
+__generic_gxx_objc_mutex_lock (objc_mutex_t UNUSED(mutex))
+{
+  /* There can only be one thread, so we always get the lock */
+  return 0;
+}
+
+/* Try to grab a lock on MUTEX.  Return 0 on success.  */
+int
+__generic_gxx_objc_mutex_trylock (objc_mutex_t UNUSED(mutex))
+{
+  /* There can only be one thread, so we always get the lock */
+  return 0;
+}
+
+/* Unlock MUTEX.  Return 0 on success.  */
+int
+__generic_gxx_objc_mutex_unlock (objc_mutex_t UNUSED(mutex))
+{
+  return 0;
+}
+
+/* Backend condition mutex functions */
+
+/* Allocate backend-specific condition data in CONDITION->backend.
+   Return 0 on success, -1 for failure.  */
+int
+__generic_gxx_objc_condition_allocate (objc_condition_t UNUSED(condition))
+{
+  return 0;
+}
+
+/* Deallocate backend-specific condition data in CONDITION->backend.
+   Return 0 for success.  */
+int
+__generic_gxx_objc_condition_deallocate (objc_condition_t UNUSED(condition))
+{
+  return 0;
+}
+
+/* MUTEX is a locked mutex.  Atomically release MUTEX and wait on
+   CONDITION, i.e. so that no other thread can observe a state after
+   the release of MUTEX but before this thread has blocked.
+   Then re-acquire a lock on MUTEX.
+   Return 0 on success.  */
+int
+__generic_gxx_objc_condition_wait (objc_condition_t UNUSED(condition),
+				   objc_mutex_t UNUSED(mutex))
+{
+  return 0;
+}
+
+/* Wake up all threads waiting on CONDITION.  Return 0 on success.  */
+int
+__generic_gxx_objc_condition_broadcast (objc_condition_t UNUSED(condition))
+{
+  return 0;
+}
+
+/* Wake up one thread waiting on CONDITION.  Return 0 on success.  */
+int
+__generic_gxx_objc_condition_signal (objc_condition_t UNUSED(condition))
+{
+  return 0;
+}
Index: gcc/gengtype-lex.c
===================================================================
--- gcc/gengtype-lex.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/gengtype-lex.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -3731,7 +3731,11 @@
 void
 parse_file (const char *fname)
 {
-  yyin = fopen (fname, "r");
+  const char *s = fname;
+
+  CYGPATH (s);
+  yyin = fopen (s, "r");
+  CYGPATH_FREE (s);
   lexer_line.file = fname;
   lexer_line.line = 1;
   if (yyin == NULL)
Index: gcc/tree-ssa-loop-ivopts.c
===================================================================
--- gcc/tree-ssa-loop-ivopts.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/tree-ssa-loop-ivopts.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1432,10 +1432,13 @@
   return true;
 }
 
+static bool loop_offset_multiple_of (struct ivopts_data *, tree, tree);
+static tree iv_value (struct iv *, tree);
+
 /* Returns true if memory reference REF may be unaligned.  */
 
 static bool
-may_be_unaligned_p (tree ref)
+may_be_unaligned_p (struct ivopts_data *data, tree ref)
 {
   tree base;
   tree base_type;
@@ -1459,11 +1462,18 @@
   base_type = TREE_TYPE (base);
   base_align = TYPE_ALIGN (base_type);
 
-  if (mode != BLKmode
-      && (base_align < GET_MODE_ALIGNMENT (mode)
+  if (mode != BLKmode) {
+    if (base_align < GET_MODE_ALIGNMENT (mode)
 	  || bitpos % GET_MODE_ALIGNMENT (mode) != 0
-	  || bitpos % BITS_PER_UNIT != 0))
+	|| bitpos % BITS_PER_UNIT != 0)
+      return true;
+
+    if (toffset && contains_packed_reference (ref)) {
+      tree al = build_int_cst (NULL_TREE, GET_MODE_ALIGNMENT (mode) / BITS_PER_UNIT);
+      if (!loop_offset_multiple_of (data, toffset, al))
     return true;
+    } 
+  }
 
   return false;
 }
@@ -1521,7 +1531,7 @@
     goto fail;
 
   if (STRICT_ALIGNMENT
-      && may_be_unaligned_p (base))
+      && may_be_unaligned_p (data, base))
     goto fail;
 
   base = unshare_expr (base);
@@ -2621,6 +2631,56 @@
     }
 }
 
+/* Returns true if OFFSET is always a multiple of alignment AL
+   even if loop carried.  */
+
+static bool
+loop_offset_multiple_of (struct ivopts_data *data, tree offset, tree al)
+{
+  double_int mul;
+
+  STRIP_NOPS (offset);
+
+  if (TREE_CODE (offset) == SSA_NAME) {
+    if (constant_multiple_of (offset, al, &mul))
+      return true;
+  }
+
+  if (TREE_CODE (offset) == MULT_EXPR) {
+    tree op0 = TREE_OPERAND (offset, 0);
+    tree op1 = TREE_OPERAND (offset, 1);
+    STRIP_NOPS (op0);
+    STRIP_NOPS (op1);
+
+    if (TREE_CODE (op1) != INTEGER_CST)
+      return false;
+      
+    if (TREE_CODE (op0) == SSA_NAME) {
+      struct iv *civ = get_iv (data, op0);
+      if (!civ || zero_p (civ->step)
+	  || constant_multiple_of (iv_value (civ, op1), al, &mul))
+	return true;
+
+    }
+    else if (loop_offset_multiple_of (data, op0, al))
+      return true;
+  }
+
+  else if (TREE_CODE (offset) == PLUS_EXPR || TREE_CODE (offset) == MINUS_EXPR) {
+    tree op0 = TREE_OPERAND (offset, 0);
+    tree op1 = TREE_OPERAND (offset, 1);
+    if (loop_offset_multiple_of (data, op0, al) &&
+	loop_offset_multiple_of (data, op1, al))
+      return true;
+  }
+
+  else if (TREE_CODE (offset) == INTEGER_CST) {
+    return constant_multiple_of (offset, al, &mul);
+  }
+
+  return false;
+}
+
 /* Sets COMB to CST.  */
 
 static void
Index: gcc/mode-switching.c
===================================================================
--- gcc/mode-switching.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/mode-switching.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -95,6 +95,182 @@
 static void reg_becomes_live (rtx, rtx, void *);
 static void make_preds_opaque (basic_block, int);
 
+/* Bitmap to compute mode flipping.  */
+
+static sbitmap *mode_in_flip;  /* flip in mode status for each basic blocks.  */
+static sbitmap *mode_out_flip; /* flip out mode status for each basic blocks.  */
+/* To support mode switching, the algorithm cannot set the modes after
+   the insert and delete bitmaps are computed by pre_edge_lcm, because
+   'avin' is computed iteratively for each possible modes for each entity.
+   The mode emission will be done after all mode are processed.
+   (see commit_mode_sets).  */
+
+static int **modes_needed;  /* modes needs to be inserted on this edge.  */
+
+/* Indicates that edge mode information is unknown. Cannot use 'no_mode'
+   because its value depends of its entity */
+#define NO_MODE -1
+
+
+/* Return true when one of the predecessor edges of BB is marked with
+   EDGE_COMPLEX. (similar to bb_has_eh_pred in basic_block.h).  */
+static bool
+bb_has_complex_pred (basic_block bb)
+{
+  edge e;
+  edge_iterator ei;
+
+  FOR_EACH_EDGE (e, ei, bb->preds)
+    {
+      if (e->flags & EDGE_COMPLEX)
+	return true;
+    }
+  return false;
+}
+
+/* Test avin modes.  
+   if 'out' is 'true' we want to know if the mode out of the basic block
+   can be flipped. If 'in' is true we want to know if the mode entering the basic
+   block can be flipped.  */
+
+static int
+test_flip_status(int entity, basic_block bb, bool out)
+{
+  if (out)
+    return TEST_BIT (mode_out_flip[bb->index], entity);
+  else
+    return TEST_BIT (mode_in_flip[bb->index], entity);
+}
+
+/* Merges the avin modes.  */
+
+static void
+set_flip_status (sbitmap *avin, sbitmap *avout)
+{
+  basic_block bb;
+
+  FOR_EACH_BB (bb)
+    {
+      int i = bb->index;
+
+      /* Merge modes for each entity for each bb. 
+	 If multiple avin modes are set for the same bb, they are not
+	 exclusive and a flip may not be emitted.
+	 If more that 2 modes can be defined, flip may not be emitted.  */
+      if (! bb_has_complex_pred (bb))
+	{
+	  sbitmap_a_xor_b (mode_in_flip[i], mode_in_flip[i], avin[i]);
+	  sbitmap_a_xor_b (mode_out_flip[i], mode_out_flip[i], avout[i]);
+	}
+    }
+}
+
+/* Allocates and initializes modes_infos.  */
+
+static void
+init_modes_infos (int n_entities)
+{
+  int j;
+  int num_edges = 0;
+  basic_block bb;
+
+  /* How many edges do we have ?  */
+
+  FOR_BB_BETWEEN (bb, ENTRY_BLOCK_PTR, EXIT_BLOCK_PTR, next_bb)
+      num_edges += EDGE_COUNT (bb->succs);
+
+  modes_needed = XNEWVEC (int *, n_entities);
+
+  for (j = 0; j < n_entities; j++)
+    {
+      modes_needed[j] = XNEWVEC (int, num_edges);
+
+      /* Initial NO_MODE value is -1, because 0 is a value mode.  */
+      memset (modes_needed[j], NO_MODE, num_edges * sizeof (int));
+    }
+
+  /* Allocates bitmaps for modes.  */
+  mode_in_flip = sbitmap_vector_alloc (last_basic_block, n_entities);
+  mode_out_flip = sbitmap_vector_alloc (last_basic_block, n_entities);
+  sbitmap_vector_zero (mode_in_flip, last_basic_block);
+  sbitmap_vector_zero (mode_out_flip, last_basic_block);
+}
+
+/* frees memory used to hold the modes information.  */
+
+static void
+free_modes_infos (int n_entities)
+{
+  int j;
+
+  for (j = 0; j < n_entities; j++)
+    free (modes_needed[j]);
+
+  free (modes_needed);
+  sbitmap_vector_free (mode_in_flip);
+  sbitmap_vector_free (mode_out_flip);
+}
+
+/* records the mode associated with edge e for entity j.  */
+
+static void
+add_mode_set (int j, int e, int mode)
+{
+  modes_needed[j][e] = mode;
+}
+
+/* returns the mode needed on edge e for entity j. -1 if none.  */
+
+static int
+get_mode (int j, int e)
+{
+  return modes_needed[j][e];
+}
+
+/* Finally, after all the modes after been inserted after lcm, we can
+   process with the mode emission.  */
+
+static int
+commit_mode_sets (struct edge_list *edge_list, int j)
+{
+  int need_commit = 0;
+  int e;
+
+  for (e = 0; e < NUM_EDGES (edge_list); e++)
+    {
+      HARD_REG_SET live_at_edge;
+      edge eg = INDEX_EDGE (edge_list, e);
+      basic_block src_bb = eg->src;
+      int mode, prev_mode;
+      rtx mode_set;
+
+      if ((mode = get_mode (j, e)) == NO_MODE)
+	continue;
+
+      prev_mode = test_flip_status (j, src_bb, true);
+
+      REG_SET_TO_HARD_REG_SET (live_at_edge,
+			       src_bb->il.rtl->global_live_at_end);
+
+      start_sequence ();
+      EMIT_MODE_SET (entity_map[j], mode, prev_mode, live_at_edge);
+
+      mode_set = get_insns ();
+      end_sequence ();      
+
+      /* Do not bother to insert empty sequence.  */
+      if (mode_set == NULL_RTX)
+	continue;
+
+      /* We should not get an abnormal edge here.  */
+      gcc_assert (! (eg->flags & EDGE_ABNORMAL));
+	  
+      need_commit = 1;
+      insert_insn_on_edge (mode_set, eg);
+    }
+
+  return need_commit;
+}
 
 /* This function will allocate a new BBINFO structure, initialized
    with the MODE, INSN, and basic block BB parameters.  */
@@ -389,7 +565,6 @@
   basic_block bb;
   int need_commit = 0;
   sbitmap *kill;
-  struct edge_list *edge_list;
   static const int num_modes[] = NUM_MODES_FOR_MODE_SWITCHING;
 #define N_ENTITIES ARRAY_SIZE (num_modes)
   int entity_map[N_ENTITIES];
@@ -399,6 +574,8 @@
   int max_num_modes = 0;
   bool emited = false;
   basic_block post_entry ATTRIBUTE_UNUSED, pre_exit ATTRIBUTE_UNUSED;
+  sbitmap *avin, *avout;
+  struct edge_list *edge_list = 0;
 
   clear_bb_flags ();
 
@@ -435,6 +612,8 @@
   antic = sbitmap_vector_alloc (last_basic_block, n_entities);
   transp = sbitmap_vector_alloc (last_basic_block, n_entities);
   comp = sbitmap_vector_alloc (last_basic_block, n_entities);
+  avin = sbitmap_vector_alloc (last_basic_block, n_entities);
+  avout = sbitmap_vector_alloc (last_basic_block, n_entities);
 
   sbitmap_vector_ones (transp, last_basic_block);
 
@@ -537,6 +716,9 @@
     }
 
   kill = sbitmap_vector_alloc (last_basic_block, n_entities);
+
+  init_modes_infos (n_entities);
+
   for (i = 0; i < max_num_modes; i++)
     {
       int current_mode[N_ENTITIES];
@@ -546,6 +728,7 @@
       /* Set the anticipatable and computing arrays.  */
       sbitmap_vector_zero (antic, last_basic_block);
       sbitmap_vector_zero (comp, last_basic_block);
+
       for (j = n_entities - 1; j >= 0; j--)
 	{
 	  int m = current_mode[j] = MODE_PRIORITY_TO_MODE (entity_map[j], i);
@@ -566,8 +749,11 @@
 
       FOR_EACH_BB (bb)
 	sbitmap_not (kill[bb->index], transp[bb->index]);
-      edge_list = pre_edge_lcm (n_entities, transp, comp, antic,
-				kill, &insert, &delete);
+      edge_list = pre_edge_lcm_avs (n_entities, transp, comp, antic,
+				kill, avin, avout, &insert, &delete);
+
+      /* Merge modes for all entities.  */
+      set_flip_status (avin, avout);
 
       for (j = n_entities - 1; j >= 0; j--)
 	{
@@ -584,10 +770,6 @@
 	  for (e = NUM_EDGES (edge_list) - 1; e >= 0; e--)
 	    {
 	      edge eg = INDEX_EDGE (edge_list, e);
-	      int mode;
-	      basic_block src_bb;
-	      HARD_REG_SET live_at_edge;
-	      rtx mode_set;
 
 	      eg->aux = 0;
 
@@ -596,26 +778,8 @@
 
 	      eg->aux = (void *)1;
 
-	      mode = current_mode[j];
-	      src_bb = eg->src;
-
-	      REG_SET_TO_HARD_REG_SET (live_at_edge,
-				       src_bb->il.rtl->global_live_at_end);
-
-	      start_sequence ();
-	      EMIT_MODE_SET (entity_map[j], mode, live_at_edge);
-	      mode_set = get_insns ();
-	      end_sequence ();
-
-	      /* Do not bother to insert empty sequence.  */
-	      if (mode_set == NULL_RTX)
-		continue;
-
-	      /* We should not get an abnormal edge here.  */
-	      gcc_assert (! (eg->flags & EDGE_ABNORMAL));
-
-	      need_commit = 1;
-	      insert_insn_on_edge (mode_set, eg);
+	      /* Remember we need to emit it.  */
+	      add_mode_set(j, e, current_mode[j]);
 	    }
 
 	  FOR_EACH_BB_REVERSE (bb)
@@ -630,6 +794,9 @@
       sbitmap_vector_free (delete);
       sbitmap_vector_free (insert);
       clear_aux_for_edges ();
+
+      /* Keep an edge_list for later.  */
+      if (i != max_num_modes - 1)
       free_edge_list (edge_list);
     }
 
@@ -638,9 +805,16 @@
     {
       int no_mode = num_modes[entity_map[j]];
 
+      /* In case there was no mode inserted. the mode information on the edge
+	 might not be complete. 
+	 Update mode info on edges and commit pending mode sets.  */
+      need_commit |= commit_mode_sets (edge_list, j);
+
       FOR_EACH_BB_REVERSE (bb)
 	{
 	  struct seginfo *ptr, *next;
+	  int last_mode = test_flip_status (j, bb, false);
+
 	  for (ptr = bb_info[j][bb->index].seginfo; ptr; ptr = next)
 	    {
 	      next = ptr->next;
@@ -649,10 +823,14 @@
 		  rtx mode_set;
 
 		  start_sequence ();
-		  EMIT_MODE_SET (entity_map[j], ptr->mode, ptr->regs_live);
+		  EMIT_MODE_SET (entity_map[j], ptr->mode, last_mode,
+				 ptr->regs_live);
 		  mode_set = get_insns ();
 		  end_sequence ();
 
+		  /* modes are are localy set.  */
+		  last_mode = 1;
+
 		  /* Insert MODE_SET only if it is nonempty.  */
 		  if (mode_set != NULL_RTX)
 		    {
@@ -673,12 +851,17 @@
       free (bb_info[j]);
     }
 
+  free_edge_list (edge_list); 
+  free_modes_infos (n_entities);
+
   /* Finished. Free up all the things we've allocated.  */
 
   sbitmap_vector_free (kill);
   sbitmap_vector_free (antic);
   sbitmap_vector_free (transp);
   sbitmap_vector_free (comp);
+  sbitmap_vector_free (avout);
+  sbitmap_vector_free (avin);
 
   if (need_commit)
     commit_edge_insertions ();
Index: gcc/gengtype-lex.l
===================================================================
--- gcc/gengtype-lex.l	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/gengtype-lex.l	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -595,7 +595,11 @@
 void
 parse_file (const char *fname)
 {
-  yyin = fopen (fname, "r");
+  const char *s = fname;
+
+  CYGPATH (s);
+  yyin = fopen (s, "r");
+  CYGPATH_FREE (s);
   lexer_line.file = fname;
   lexer_line.line = 1;
   if (yyin == NULL)
Index: gcc/expr.c
===================================================================
--- gcc/expr.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/expr.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2,6 +2,7 @@
    Copyright (C) 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,
    2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007
    Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -53,6 +54,9 @@
 #include "target.h"
 #include "timevar.h"
 
+extern bool
+suitable_for_tail_call_opt_p (void);
+
 /* Decide whether a function's arguments should be processed
    from first to last or from last to first.
 
@@ -4050,7 +4054,7 @@
 /* Expand an assignment that stores the value of FROM into TO.  */
 
 void
-expand_assignment (tree to, tree from)
+expand_assignment (tree to, tree from, bool last_stmt_p)
 {
   rtx to_rtx = 0;
   rtx result;
@@ -4137,12 +4141,12 @@
 	  if (TREE_CODE (TREE_TYPE (from)) == COMPLEX_TYPE)
 	    {
 	      gcc_assert (bitpos == 0);
-	      result = store_expr (from, to_rtx, false);
+	      result = store_expr (from, to_rtx, false, false);
 	    }
 	  else
 	    {
 	      gcc_assert (bitpos == 0 || bitpos == GET_MODE_BITSIZE (mode1));
-	      result = store_expr (from, XEXP (to_rtx, bitpos != 0), false);
+	      result = store_expr (from, XEXP (to_rtx, bitpos != 0), false, false);
 	    }
 	}
       else
@@ -4275,7 +4279,7 @@
   /* Compute FROM and store the value in the rtx we got.  */
 
   push_temp_slots ();
-  result = store_expr (from, to_rtx, 0);
+  result = store_expr (from, to_rtx, false, last_stmt_p);
   preserve_temp_slots (result);
   free_temp_slots ();
   pop_temp_slots ();
@@ -4292,11 +4296,11 @@
    with no sequence point.  Will other languages need this to
    be more thorough?
 
-   If CALL_PARAM_P is nonzero, this is a store into a call param on the
+   If CALL_PARAM_P is true, this is a store into a call param on the
    stack, and block moves may need to be treated specially.  */
 
 rtx
-store_expr (tree exp, rtx target, int call_param_p)
+store_expr (tree exp, rtx target, bool call_param_p, bool last_stmt_p)
 {
   rtx temp;
   rtx alt_rtl = NULL_RTX;
@@ -4317,7 +4321,7 @@
 	 part.  */
       expand_expr (TREE_OPERAND (exp, 0), const0_rtx, VOIDmode,
 		   call_param_p ? EXPAND_STACK_PARM : EXPAND_NORMAL);
-      return store_expr (TREE_OPERAND (exp, 1), target, call_param_p);
+      return store_expr (TREE_OPERAND (exp, 1), target, call_param_p, false);
     }
   else if (TREE_CODE (exp) == COND_EXPR && GET_MODE (target) == BLKmode)
     {
@@ -4331,11 +4335,11 @@
       do_pending_stack_adjust ();
       NO_DEFER_POP;
       jumpifnot (TREE_OPERAND (exp, 0), lab1);
-      store_expr (TREE_OPERAND (exp, 1), target, call_param_p);
+      store_expr (TREE_OPERAND (exp, 1), target, call_param_p, false);
       emit_jump_insn (gen_jump (lab2));
       emit_barrier ();
       emit_label (lab1);
-      store_expr (TREE_OPERAND (exp, 2), target, call_param_p);
+      store_expr (TREE_OPERAND (exp, 2), target, call_param_p, false);
       emit_label (lab2);
       OK_DEFER_POP;
 
@@ -4484,8 +4488,10 @@
 	  if (GET_CODE (size) == CONST_INT
 	      && INTVAL (size) < TREE_STRING_LENGTH (exp))
 	    emit_block_move (target, temp, size,
-			     (call_param_p
-			      ? BLOCK_OP_CALL_PARM : BLOCK_OP_NORMAL));
+			     call_param_p
+			     ? BLOCK_OP_CALL_PARM
+			     : last_stmt_p ? BLOCK_OP_TAILCALL
+			     : BLOCK_OP_NORMAL);
 	  else
 	    {
 	      /* Compute the size of the data to copy from the string.  */
@@ -4503,8 +4509,10 @@
 	      copy_size_rtx = convert_to_mode (ptr_mode, copy_size_rtx,
 					       TYPE_UNSIGNED (sizetype));
 	      emit_block_move (target, temp, copy_size_rtx,
-			       (call_param_p
-				? BLOCK_OP_CALL_PARM : BLOCK_OP_NORMAL));
+			       call_param_p
+			       ? BLOCK_OP_CALL_PARM
+			       : last_stmt_p ? BLOCK_OP_TAILCALL
+			       : BLOCK_OP_NORMAL);
 
 	      /* Figure out how much is left in TARGET that we have to clear.
 		 Do all calculations in ptr_mode.  */
@@ -4547,8 +4555,9 @@
 			 int_size_in_bytes (TREE_TYPE (exp)));
       else if (GET_MODE (temp) == BLKmode)
 	emit_block_move (target, temp, expr_size (exp),
-			 (call_param_p
-			  ? BLOCK_OP_CALL_PARM : BLOCK_OP_NORMAL));
+			 call_param_p
+			 ? BLOCK_OP_CALL_PARM
+			 : last_stmt_p ? BLOCK_OP_TAILCALL : BLOCK_OP_NORMAL);
       else
 	{
 	  temp = force_operand (temp, target);
@@ -5242,7 +5251,7 @@
 		      = gen_reg_rtx (promote_mode (domain, DECL_MODE (index),
 						   &unsignedp, 0));
 		    SET_DECL_RTL (index, index_r);
-		    store_expr (lo_index, index_r, 0);
+		    store_expr (lo_index, index_r, false, false);
 		    
 		    /* Build the head of the loop.  */
 		    do_pending_stack_adjust ();
@@ -5269,7 +5278,7 @@
 		      store_constructor (value, xtarget, cleared,
 					 bitsize / BITS_PER_UNIT);
 		    else
-		      store_expr (value, xtarget, 0);
+		      store_expr (value, xtarget, false, false);
 
 		    /* Generate a conditional jump to exit the loop.  */
 		    exit_cond = build2 (LT_EXPR, integer_type_node,
@@ -5280,7 +5289,7 @@
 		       the loop.  */
 		    expand_assignment (index,
 				       build2 (PLUS_EXPR, TREE_TYPE (index),
-					       index, integer_one_node));
+					       index, integer_one_node), false);
 		    
 		    emit_jump (loop_start);
 		    
@@ -5311,7 +5320,7 @@
 					  expand_normal (position),
 					  highest_pow2_factor (position));
 		xtarget = adjust_address (xtarget, mode, 0);
-		store_expr (value, xtarget, 0);
+		store_expr (value, xtarget, false, false);
 	      }
 	    else
 	      {
@@ -5525,7 +5534,7 @@
       /* We're storing into a struct containing a single __complex.  */
 
       gcc_assert (!bitpos);
-      return store_expr (exp, target, 0);
+      return store_expr (exp, target, false, false);
     }
 
   /* If the structure is in a register or if the component
@@ -5626,7 +5635,7 @@
       if (!MEM_KEEP_ALIAS_SET_P (to_rtx) && MEM_ALIAS_SET (to_rtx) != 0)
 	set_mem_alias_set (to_rtx, alias_set);
 
-      return store_expr (exp, to_rtx, 0);
+      return store_expr (exp, to_rtx, false, false);
     }
 }
 
@@ -6802,6 +6811,54 @@
   return ret;
 }
 
+static bool
+may_tailcall(tree exp)
+{
+  edge e;
+  edge_iterator ei;
+  bool last_stmt_p = false;
+
+  if (!flag_optimize_sibling_calls || current_function_stdarg ||
+      ! suitable_for_tail_call_opt_p ())
+    return false;
+
+  FOR_EACH_EDGE (e, ei, EXIT_BLOCK_PTR->preds)
+    {
+      basic_block bb = e->src;
+
+      if (!single_succ_p (bb))
+	return false;
+
+      block_stmt_iterator bsi;
+      for (bsi = bsi_last (bb); !bsi_end_p (bsi); bsi_prev (&bsi))
+	{
+	  tree stmt = bsi_stmt (bsi);
+
+	  /* Ignore labels.  */
+	  if (TREE_CODE (stmt) == LABEL_EXPR)
+	    return false;
+
+	  if (TREE_CODE(stmt) == RETURN_EXPR)
+	    {
+	      tree lhs = TREE_OPERAND (exp, 0);
+	      if (TREE_CODE (lhs) == RESULT_DECL &&
+		  TREE_CODE (TREE_OPERAND (exp, 0)) == RESULT_DECL)
+		{
+		  last_stmt_p = true;
+		  continue;
+		}
+	      return false;
+	    }
+	  else if (stmt == exp)
+	      return true;
+	  else
+	    return false;
+	}
+    }
+
+  return last_stmt_p;
+}
+
 static rtx
 expand_expr_real_1 (tree exp, rtx target, enum machine_mode tmode,
 		    enum expand_modifier modifier, rtx *alt_rtl)
@@ -7775,7 +7832,7 @@
 	    /* Store data into beginning of memory target.  */
 	    store_expr (TREE_OPERAND (exp, 0),
 			adjust_address (target, TYPE_MODE (valtype), 0),
-			modifier == EXPAND_STACK_PARM);
+			modifier == EXPAND_STACK_PARM, false);
 
 	  else
 	    {
@@ -8622,13 +8679,13 @@
        op1 = gen_label_rtx ();
        jumpifnot (TREE_OPERAND (exp, 0), op0);
        store_expr (TREE_OPERAND (exp, 1), temp,
- 		  modifier == EXPAND_STACK_PARM);
+ 		  modifier == EXPAND_STACK_PARM, false);
 
        emit_jump_insn (gen_jump (op1));
        emit_barrier ();
        emit_label (op0);
        store_expr (TREE_OPERAND (exp, 2), temp,
- 		  modifier == EXPAND_STACK_PARM);
+ 		  modifier == EXPAND_STACK_PARM, false);
 
        emit_label (op1);
        OK_DEFER_POP;
@@ -8666,13 +8723,13 @@
 	    do_jump (TREE_OPERAND (rhs, 1),
 		     value ? label : 0,
 		     value ? 0 : label);
-	    expand_assignment (lhs, build_int_cst (TREE_TYPE (rhs), value));
+	    expand_assignment (lhs, build_int_cst (TREE_TYPE (rhs), value), false);
 	    do_pending_stack_adjust ();
 	    emit_label (label);
 	    return const0_rtx;
 	  }
 
-	expand_assignment (lhs, rhs);
+	expand_assignment (lhs, rhs, may_tailcall (exp));
 
 	return const0_rtx;
       }
Index: gcc/expr.h
===================================================================
--- gcc/expr.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/expr.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -465,13 +465,13 @@
 			    int, rtx, int, rtx, rtx, int, rtx);
 
 /* Expand an assignment that stores the value of FROM into TO.  */
-extern void expand_assignment (tree, tree);
+extern void expand_assignment (tree, tree, bool);
 
 /* Generate code for computing expression EXP,
    and storing the value into TARGET.
    If SUGGEST_REG is nonzero, copy the value through a register
    and return that register, if that is possible.  */
-extern rtx store_expr (tree, rtx, int);
+extern rtx store_expr (tree, rtx, bool, bool);
 
 /* Given an rtx that may include add and multiply operations,
    generate them as insns and return a pseudo-reg containing the value.
Index: gcc/longlong.h
===================================================================
--- gcc/longlong.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/longlong.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -846,6 +846,30 @@
 /* This is the same algorithm as __udiv_qrnnd_c.  */
 #define UDIV_NEEDS_NORMALIZATION 1
 
+#ifdef	DB_ST40300_BUG_WORKAROUND
+#define udiv_qrnnd(q, r, n1, n0, d) \
+  do {									\
+    extern UWtype __udiv_qrnnd_16 (UWtype, UWtype)			\
+                        __attribute__ ((visibility ("hidden")));	\
+    /* r0: rn r1: qn */ /* r0: n1 r4: n0 r5: d r6: d1 */ /* r2: __m */	\
+    __asm__ (								\
+"	.align	2\n"                                                    \
+"       mov%M4 %4,r5\n"						        \
+"	swap.w %3,r4\n"							\
+"	swap.w r5,r6\n"							\
+"       nop\n"                                                          \
+"	jsr @%5\n"							\
+"	shll16 r6\n"							\
+"	swap.w r4,r4\n"							\
+"       nop\n"                                                          \
+"	jsr @%5\n"							\
+"	swap.w r1,%0\n"							\
+"	or r1,%0"							\
+	: "=r" (q), "=&z" (r)						\
+	: "1" (n1), "r" (n0), "rm" (d), "r" (&__udiv_qrnnd_16)		\
+	: "r1", "r2", "r4", "r5", "r6", "pr");				\
+  } while (0)
+#else
 #define udiv_qrnnd(q, r, n1, n0, d) \
   do {									\
     extern UWtype __udiv_qrnnd_16 (UWtype, UWtype)			\
@@ -865,6 +889,7 @@
 	: "1" (n1), "r" (n0), "rm" (d), "r" (&__udiv_qrnnd_16)		\
 	: "r1", "r2", "r4", "r5", "r6", "pr");				\
   } while (0)
+#endif
 
 #define UDIV_TIME 80
 
Index: gcc/opts.c
===================================================================
--- gcc/opts.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/opts.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2,6 +2,7 @@
    Copyright (C) 2002, 2003, 2004, 2005, 2006, 2007
    Free Software Foundation, Inc.
    Contributed by Neil Booth.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -483,6 +484,7 @@
       flag_schedule_insns_after_reload = 1;
 #endif
       flag_regmove = 1;
+      flag_optimize_related_values = 1;
       flag_strict_aliasing = 1;
       flag_strict_overflow = 1;
       flag_delete_null_pointer_checks = 1;
@@ -732,6 +734,7 @@
 
     case OPT_aux_info:
     case OPT_aux_info_:
+      CYGPATH (arg);
       aux_info_file_name = arg;
       flag_gen_aux_info = 1;
       break;
@@ -754,6 +757,7 @@
       break;
 
     case OPT_dumpbase:
+      CYGPATH (arg);
       dump_base_name = arg;
       break;
 
@@ -993,6 +997,7 @@
       break;
 
     case OPT_o:
+      CYGPATH (arg);
       asm_file_name = arg;
       break;
 
Index: gcc/tree-ssa-loop-ivcanon.c
===================================================================
--- gcc/tree-ssa-loop-ivcanon.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/tree-ssa-loop-ivcanon.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -75,6 +75,7 @@
   tree cond, type, var;
   block_stmt_iterator incr_at;
   enum tree_code cmp;
+  tree t2;
 
   if (dump_file && (dump_flags & TDF_DETAILS))
     {
@@ -104,9 +105,11 @@
 	     &incr_at, false, NULL, &var);
 
   cmp = (exit->flags & EDGE_TRUE_VALUE) ? EQ_EXPR : NE_EXPR;
-  COND_EXPR_COND (cond) = build2 (cmp, boolean_type_node,
+
+  t2 = build2 (cmp, boolean_type_node,
 				  var,
 				  build_int_cst (type, 0));
+  COND_EXPR_COND (cond) = t2;
   update_stmt (cond);
 }
 
@@ -279,18 +282,12 @@
   edge exit = NULL;
   tree niter;
 
-  niter = number_of_iterations_in_loop (loop);
+  niter = number_of_latch_executions (loop);
   if (TREE_CODE (niter) == INTEGER_CST)
     {
       exit = loop->single_exit;
       if (!just_once_each_iteration_p (loop, exit->src))
 	return false;
-
-      /* The result of number_of_iterations_in_loop is by one higher than
-	 we expect (i.e. it returns number of executions of the exit
-	 condition, not of the loop latch edge).  */
-      niter = fold_build2 (MINUS_EXPR, TREE_TYPE (niter), niter,
-			   build_int_cst (TREE_TYPE (niter), 1));
     }
   else
     {
Index: gcc/lcm.c
===================================================================
--- gcc/lcm.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/lcm.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -370,17 +370,18 @@
     }
 }
 
-/* Given local properties TRANSP, ANTLOC, AVOUT, KILL return the insert and
-   delete vectors for edge based LCM.  Returns an edgelist which is used to
-   map the insert vector to what edge an expression should be inserted on.  */
+/* Given local properties TRANSP, ANTLOC, AVLOC, KILL return the insert and
+   delete vectors for edge based LCM, and return the AVIN, AVOUT bitmap.
+   Returns an edgelist which is used to map the insert vector to
+   what edge an expression should be inserted on.  */
 
 struct edge_list *
-pre_edge_lcm (int n_exprs, sbitmap *transp,
+pre_edge_lcm_avs (int n_exprs, sbitmap *transp,
 	      sbitmap *avloc, sbitmap *antloc, sbitmap *kill,
+		  sbitmap *avin, sbitmap *avout,
 	      sbitmap **insert, sbitmap **delete)
 {
   sbitmap *antin, *antout, *earliest;
-  sbitmap *avin, *avout;
   sbitmap *later, *laterin;
   struct edge_list *edge_list;
   int num_edges;
@@ -402,10 +403,7 @@
 #endif
 
   /* Compute global availability.  */
-  avin = sbitmap_vector_alloc (last_basic_block, n_exprs);
-  avout = sbitmap_vector_alloc (last_basic_block, n_exprs);
   compute_available (avloc, kill, avout, avin);
-  sbitmap_vector_free (avin);
 
   /* Compute global anticipatability.  */
   antin = sbitmap_vector_alloc (last_basic_block, n_exprs);
@@ -431,7 +429,6 @@
 
   sbitmap_vector_free (antout);
   sbitmap_vector_free (antin);
-  sbitmap_vector_free (avout);
 
   later = sbitmap_vector_alloc (num_edges, n_exprs);
 
@@ -468,6 +465,28 @@
   return edge_list;
 }
 
+/* Wrapper to allocate avin/avout and call pre_edge_lcm_avs.  */
+
+struct edge_list *
+pre_edge_lcm (int n_exprs, sbitmap *transp,
+	      sbitmap *avloc, sbitmap *antloc, sbitmap *kill, 
+	      sbitmap **insert, sbitmap **delete)
+{
+  struct edge_list *edge_list;
+  sbitmap *avin, *avout;
+
+  avin = sbitmap_vector_alloc (last_basic_block, n_exprs);
+  avout = sbitmap_vector_alloc (last_basic_block, n_exprs);
+
+  edge_list = pre_edge_lcm_avs (n_exprs, transp, avloc, antloc, kill,
+				 avin, avout, insert, delete);
+
+  sbitmap_vector_free (avout);
+  sbitmap_vector_free (avin);
+
+  return edge_list;
+}
+
 /* Compute the AVIN and AVOUT vectors from the AVLOC and KILL vectors.
    Return the number of passes we performed to iterate to a solution.  */
 
Index: gcc/regmove.c
===================================================================
--- gcc/regmove.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/regmove.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,6 +1,7 @@
 /* Move registers around to reduce number of move instructions needed.
    Copyright (C) 1987, 1988, 1989, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005, 2007 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -42,6 +43,9 @@
 #include "except.h"
 #include "toplev.h"
 #include "reload.h"
+#include "obstack.h"
+#include "ggc.h"
+#include "optabs.h"
 #include "timevar.h"
 #include "tree-pass.h"
 
@@ -53,6 +57,13 @@
 #else
 #define STACK_GROWS_DOWNWARD 0
 #endif
+/* Likewise for AUTO_INC_DEC.  */
+#ifdef AUTO_INC_DEC
+#undef AUTO_INC_DEC
+#define AUTO_INC_DEC 1
+#else
+#define AUTO_INC_DEC 0
+#endif
 
 static int perhaps_ends_bb_p (rtx);
 static int optimize_reg_copy_1 (rtx, rtx, rtx);
@@ -81,6 +92,37 @@
 static int replacement_quality (rtx);
 static int fixup_match_2 (rtx, rtx, rtx, rtx);
 
+struct related;
+struct rel_use_chain;
+struct rel_mod;
+struct rel_use;
+
+static struct rel_use *lookup_related (int, enum reg_class, HOST_WIDE_INT, int);
+static void rel_build_chain (struct rel_use *, struct rel_use *,
+			     struct related *);
+static int recognize_related_for_insn (rtx, int, int);
+static void record_reg_use (rtx *, rtx, int, int);
+static struct rel_use *create_rel_use (rtx, rtx *, int, int, int);
+static void new_reg_use (rtx, rtx *, int, int, int, int);
+static void rel_record_mem (rtx *, rtx, int, int, int, rtx, int, int);
+static void new_base (rtx, rtx, int, int);
+static void invalidate_related (rtx, rtx, int, int);
+static void find_related (rtx *, rtx, int, int);
+static void find_related_toplev (rtx, int, int);
+static int chain_starts_earlier (const void *, const void *);
+static int chain_ends_later (const void *, const void *);
+static int mod_before (const void *, const void *);
+static void remove_setting_insns (struct related *, rtx);
+static rtx perform_addition (struct rel_mod *, struct rel_use *, rtx,
+			     struct rel_use_chain *);
+static void modify_address (struct rel_mod *, struct rel_use *, HOST_WIDE_INT);
+static void optimize_related_values_1 (struct related *, int, int, rtx);
+static void optimize_related_values_0 (struct related *, int, int, rtx);
+static void optimize_related_values (int);
+static void count_sets (rtx, rtx, void *);
+static int link_chains (struct rel_use_chain *, struct rel_use_chain *,
+			enum machine_mode);
+
 /* Return nonzero if registers with CLASS1 and CLASS2 can be merged without
    causing too much register allocation problems.  */
 static int
@@ -93,232 +135,2137 @@
 	      && ! CLASS_LIKELY_SPILLED_P (class1)));
 }
 
-/* INC_INSN is an instruction that adds INCREMENT to REG.
-   Try to fold INC_INSN as a post/pre in/decrement into INSN.
-   Iff INC_INSN_SET is nonzero, inc_insn has a destination different from src.
-   Return nonzero for success.  */
-static int
-try_auto_increment (rtx insn, rtx inc_insn, rtx inc_insn_set, rtx reg,
-		    HOST_WIDE_INT increment, int pre)
-{
-  enum rtx_code inc_code;
+/* INC_INSN is an instruction that adds INCREMENT to REG.
+   Try to fold INC_INSN as a post/pre in/decrement into INSN.
+   Iff INC_INSN_SET is nonzero, inc_insn has a destination different from src.
+   Return nonzero for success.  */
+static int
+try_auto_increment (rtx insn, rtx inc_insn, rtx inc_insn_set, rtx reg,
+		    HOST_WIDE_INT increment, int pre)
+{
+  enum rtx_code inc_code;
+
+  rtx pset = single_set (insn);
+  if (pset)
+    {
+      /* Can't use the size of SET_SRC, we might have something like
+	 (sign_extend:SI (mem:QI ...  */
+      rtx use = find_use_as_address (pset, reg, 0);
+      if (use != 0 && use != (rtx) (size_t) 1)
+	{
+	  int size = GET_MODE_SIZE (GET_MODE (use));
+	  if (0
+	      || (HAVE_POST_INCREMENT
+		  && pre == 0 && (inc_code = POST_INC, increment == size))
+	      || (HAVE_PRE_INCREMENT
+		  && pre == 1 && (inc_code = PRE_INC, increment == size))
+	      || (HAVE_POST_DECREMENT
+		  && pre == 0 && (inc_code = POST_DEC, increment == -size))
+	      || (HAVE_PRE_DECREMENT
+		  && pre == 1 && (inc_code = PRE_DEC, increment == -size))
+	  )
+	    {
+	      if (inc_insn_set)
+		validate_change
+		  (inc_insn,
+		   &SET_SRC (inc_insn_set),
+		   XEXP (SET_SRC (inc_insn_set), 0), 1);
+	      validate_change (insn, &XEXP (use, 0),
+			       gen_rtx_fmt_e (inc_code, Pmode, reg), 1);
+	      if (apply_change_group ())
+		{
+		  /* If there is a REG_DEAD note on this insn, we must
+		     change this not to REG_UNUSED meaning that the register
+		     is set, but the value is dead.  Failure to do so will
+		     result in a sched1 dieing -- when it recomputes lifetime
+		     information, the number of REG_DEAD notes will have
+		     changed.  */
+		  rtx note = find_reg_note (insn, REG_DEAD, reg);
+		  if (note)
+		    PUT_MODE (note, REG_UNUSED);
+
+		  REG_NOTES (insn)
+		    = gen_rtx_EXPR_LIST (REG_INC,
+					 reg, REG_NOTES (insn));
+		  if (! inc_insn_set)
+		    delete_insn (inc_insn);
+		  return 1;
+		}
+	    }
+	}
+    }
+  return 0;
+}
+
+/* Determine if the pattern generated by add_optab has a clobber,
+   such as might be issued for a flags hard register.  To make the
+   code elsewhere simpler, we handle cc0 in this same framework.
+
+   Return the register if one was discovered.  Return NULL_RTX if
+   if no flags were found.  Return pc_rtx if we got confused.  */
+
+static rtx
+discover_flags_reg (void)
+{
+  rtx tmp;
+  tmp = gen_rtx_REG (word_mode, 10000);
+  tmp = gen_add3_insn (tmp, tmp, const2_rtx);
+
+  /* If we get something that isn't a simple set, or a
+     [(set ..) (clobber ..)], this whole function will go wrong.  */
+  if (GET_CODE (tmp) == INSN)
+    tmp = PATTERN (tmp);
+  if (GET_CODE (tmp) == SET)
+    return NULL_RTX;
+  else if (GET_CODE (tmp) == PARALLEL)
+    {
+      int found;
+
+      if (XVECLEN (tmp, 0) != 2)
+	return pc_rtx;
+      tmp = XVECEXP (tmp, 0, 1);
+      if (GET_CODE (tmp) != CLOBBER)
+	return pc_rtx;
+      tmp = XEXP (tmp, 0);
+
+      /* Don't do anything foolish if the md wanted to clobber a
+	 scratch or something.  We only care about hard regs.
+	 Moreover we don't like the notion of subregs of hard regs.  */
+      if (GET_CODE (tmp) == SUBREG
+	  && REG_P (SUBREG_REG (tmp))
+	  && REGNO (SUBREG_REG (tmp)) < FIRST_PSEUDO_REGISTER)
+	return pc_rtx;
+      found = (REG_P (tmp) && REGNO (tmp) < FIRST_PSEUDO_REGISTER);
+
+      return (found ? tmp : NULL_RTX);
+    }
+
+  return pc_rtx;
+}
+
+/* It is a tedious task identifying when the flags register is live and
+   when it is safe to optimize.  Since we process the instruction stream
+   multiple times, locate and record these live zones by marking the
+   mode of the instructions --
+
+   QImode is used on the instruction at which the flags becomes live.
+
+   HImode is used within the range (exclusive) that the flags are
+   live.  Thus the user of the flags is not marked.
+
+   All other instructions are cleared to VOIDmode.  */
+
+/* Used to communicate with flags_set_1.  */
+static rtx flags_set_1_rtx;
+static int flags_set_1_set;
+
+static void
+mark_flags_life_zones (rtx flags)
+{
+  int flags_regno;
+  int flags_nregs;
+  basic_block block;
+
+#ifdef HAVE_cc0
+  /* If we found a flags register on a cc0 host, bail.  */
+  if (flags == NULL_RTX)
+    flags = cc0_rtx;
+  else if (flags != cc0_rtx)
+    flags = pc_rtx;
+#endif
+
+  /* Simple cases first: if no flags, clear all modes.  If confusing,
+     mark the entire function as being in a flags shadow.  */
+  if (flags == NULL_RTX || flags == pc_rtx)
+    {
+      enum machine_mode mode = (flags ? HImode : VOIDmode);
+      rtx insn;
+      for (insn = get_insns (); insn; insn = NEXT_INSN (insn))
+	PUT_MODE (insn, mode);
+      return;
+    }
+
+#ifdef HAVE_cc0
+  flags_regno = -1;
+  flags_nregs = 1;
+#else
+  flags_regno = REGNO (flags);
+  flags_nregs = hard_regno_nregs[flags_regno][GET_MODE (flags)];
+#endif
+  flags_set_1_rtx = flags;
+
+  /* Process each basic block.  */
+  FOR_EACH_BB_REVERSE (block)
+    {
+      rtx insn, end;
+      int live;
+
+      insn = BB_HEAD (block);
+      end = BB_END (block);
+
+      /* Look out for the (unlikely) case of flags being live across
+	 basic block boundaries.  */
+      live = 0;
+#ifndef HAVE_cc0
+      {
+	int i;
+	for (i = 0; i < flags_nregs; ++i)
+	  live |= REGNO_REG_SET_P (block->il.rtl->global_live_at_start,
+				   flags_regno + i);
+      }
+#endif
+
+      while (1)
+	{
+	  /* Process liveness in reverse order of importance --
+	     alive, death, birth.  This lets more important info
+	     overwrite the mode of lesser info.  */
+
+	  if (INSN_P (insn))
+	    {
+#ifdef HAVE_cc0
+	      /* In the cc0 case, death is not marked in reg notes,
+		 but is instead the mere use of cc0 when it is alive.  */
+	      if (live && reg_mentioned_p (cc0_rtx, PATTERN (insn)))
+		live = 0;
+#else
+	      /* In the hard reg case, we watch death notes.  */
+	      if (live && find_regno_note (insn, REG_DEAD, flags_regno))
+		live = 0;
+#endif
+	      PUT_MODE (insn, (live ? HImode : VOIDmode));
+
+	      /* In either case, birth is denoted simply by its presence
+		 as the destination of a set.  */
+	      flags_set_1_set = 0;
+	      note_stores (PATTERN (insn), flags_set_1, NULL);
+	      if (flags_set_1_set)
+		{
+		  live = 1;
+		  PUT_MODE (insn, QImode);
+		}
+	    }
+	  else
+	    PUT_MODE (insn, (live ? HImode : VOIDmode));
+
+	  if (insn == end)
+	    break;
+	  insn = NEXT_INSN (insn);
+	}
+    }
+}
+
+/* A subroutine of mark_flags_life_zones, called through note_stores.  */
+
+static void
+flags_set_1 (rtx x, rtx pat, void *data ATTRIBUTE_UNUSED)
+{
+  if (GET_CODE (pat) == SET
+      && reg_overlap_mentioned_p (x, flags_set_1_rtx))
+    flags_set_1_set = 1;
+}
+
+static GTY (()) rtx test_addr;
+
+/* Some machines have two-address-adds and instructions that can
+   use only register-indirect addressing and auto_increment, but no
+   offsets.  If multiple fields of a struct are accessed more than
+   once, cse will load each of the member addresses in separate registers.
+   This not only costs a lot of registers, but also of instructions,
+   since each add to initialize an address register must be really expanded
+   into a register-register move followed by an add.
+   regmove_optimize uses some heuristics to detect this case; if these
+   indicate that this is likely, optimize_related_values is run once for
+   the entire function.
+
+   We build chains of uses of related values that can be satisfied with the
+   same base register by taking advantage of auto-increment address modes
+   instead of explicit add instructions.
+
+   We try to link chains with disjoint lifetimes together to reduce the
+   number of temporary registers and register-register copies.
+
+   This optimization pass operates on basic blocks one at a time; it could be
+   extended to work on extended basic blocks or entire functions.  */
+
+/* For each set of values related to a common base register, we use a
+   hash table which maps constant offsets to instructions.
+
+   The instructions mapped to are those that use a register which may,
+   (possibly with a change in addressing mode) differ from the initial
+   value of the base register by exactly that offset after the
+   execution of the instruction.
+   Here we define the size of the hash table, and the hash function to use.  */
+#define REL_USE_HASH_SIZE 43
+#define REL_USE_HASH(I) ((I) % (unsigned HOST_WIDE_INT) REL_USE_HASH_SIZE)
+
+/* For each register in a set of registers that are related, we keep a
+   struct related.
+
+   BASE points to the struct related of the base register (i.e. the one
+   that was the source of the first three-address add for this set of
+   related values).
+
+   INSN is the instruction that initialized the register, or, for the
+   base, the instruction that initialized the first non-base register.
+
+   BASE is the register number of the base register.
+
+   For the base register only, the member BASEINFO points to some extra data.
+
+   'luid' here means linear uid.  We count them starting at the function
+   start; they are used to avoid overlapping lifetimes.
+
+   UPDATES is a list of instructions that set the register to a new
+   value that is still related to the same base.
+
+   When a register in a set of related values is set to something that
+   is not related to the base, INVALIDATE_LUID is set to the luid of
+   the instruction that does this set.  This is used to avoid re-using
+   this register in an overlapping liftime for a related value.
+
+   DEATH is first used to store the insn (if any) where the register dies.
+   When the optimization is actually performed, the REG_DEAD note from
+   the insn denoted by DEATH is removed.
+   Thereafter, the removed death note is stored in DEATH, marking not
+   only that the register dies, but also making the note available for reuse.
+
+   We also use a struct related to keep track of registers that have been
+   used for anything that we don't recognize as related values.
+   The only really interesting datum for these is u.last_luid, which is
+   the luid of the last reference we have seen.  These struct relateds
+   are marked by a zero INSN field; most other members are not used and
+   remain uninitialized.  */
+
+struct related
+{
+  rtx insn;
+  rtx reg;
+  struct related *base;
+  HOST_WIDE_INT offset;
+  struct related *prev;
+  struct update *updates;
+  struct related_baseinfo *baseinfo;
+  int invalidate_luid;
+  rtx death;
+  int reg_orig_calls_crossed;
+  int reg_set_call_tally;
+};
+
+/* HASHTAB maps offsets to register uses with a matching MATCH_OFFSET.
+   PREV_BASE points to the struct related for the previous base register
+   that we currently keep track of.
+   INSN_LUID is the luid of the instruction that started this set of
+   related values.  */
+struct related_baseinfo
+{
+  struct rel_use *hashtab[REL_USE_HASH_SIZE];
+  struct rel_use_chain *chains;
+  struct related *prev_base;
+  int insn_luid;
+};
+
+/* INSN is an instruction that sets a register that previously contained
+   a related value to a new value that is related to the same base register.
+   When the optimization is performed, we have to delete INSN.
+   DEATH_INSN points to the insn (if any) where the register died that we
+   set in INSN.  When we perform the optimization, the REG_DEAD note has
+   to be removed from DEATH_INSN.
+   PREV points to the struct update that pertains to the previous
+   instruction pertaining to the same register that set it from one
+   related value to another one.  */
+struct update
+{
+  rtx insn;
+  rtx death_insn;
+  struct update *prev;
+};
+
+struct rel_use_chain
+{
+  /* Points to first use in this chain.  */
+  struct rel_use *uses;
+  struct rel_use_chain *prev;
+  struct rel_use_chain *linked;
+  /* The following fields are only set after the chain has been completed:  */
+  /* Last use in this chain.  */
+  struct rel_use *end;
+  int start_luid;
+  int end_luid;
+  int calls_crossed;
+  /* The register allocated for this chain.  */
+  rtx reg;
+  /* The death note for this register.  */
+  rtx death_note;
+  /* Offset after execution of last insn.  */
+  HOST_WIDE_INT match_offset;
+  int invalidate_luid;
+};
+
+/* ADDRP points to the place where the actual use of the related value is.
+   This is commonly a memory address, and has to be set to a register
+   or some auto_inc addressing of this register.
+   But ADDRP is also used for all other uses of related values to
+   the place where the register is inserted; we can tell that an
+   unardorned register is to be inserted because no offset adjustment
+   is required, hence this is handled by the same logic as register-indirect
+   addressing.  The only exception to this is when SET_IN_PARALLEL is set,
+   see below.
+
+   OFFSET is the offset that is actually used in this instance, i.e.
+   the value of the base register when the set of related values was
+   created plus OFFSET yields the value that is used.
+   This might be different from the value of the used register before
+   executing INSN if we elected to use pre-{in,de}crement addressing.
+   If we have the option to use post-{in,de}crement addressing, all
+   choices are linked cyclically together with the SIBLING field.
+   Otherwise, it's a one-link-cycle, i.e. SIBLING points at the
+   struct rel_use it is a member of.
+
+   MATCH_OFFSET is the offset that is available after the execution
+   of INSN.  It is the same as OFFSET for straight register-indirect
+   addressing and for pre-{in,de}crement addressing, while it differs
+   for the post-{in,de}crement addressing modes.
+
+   If SET_IN_PARALLEL is set, MATCH_OFFSET differs from OFFSET, yet
+   this is no post-{in,de}crement addressing.  Rather, it is a set
+   inside a PARALLEL that adds some constant to a register that holds
+   one value of a set of related values that we keep track of.
+
+   NEXT_CHAIN is the link in a chain of rel_use structures.  If nonzero,
+   we will ignore this rel_use in a hash table lookup, since it has
+   already been appended to.  This field can point to its containing
+   rel_use; this means that we found a reason not to append to this
+   chain anymore (e.g. if a use comes with a clobber).
+
+   ADDRP then points only to the set destination of this set; another
+   struct rel_use is used for the source of the set.
+
+   NO_LINK_PRED is nonzero for the last use in a chain if it cannot be
+   the predecessor for a another chain to be linked to.  This can happen
+   for uses that come with a clobber, and for uses by a register that
+   is live at the end of the processed range of insns (usually a basic
+   block).  */
+
+struct rel_use
+{
+  rtx insn, *addrp;
+  int luid;
+  int call_tally;
+  enum reg_class class;
+  HOST_WIDE_INT offset;
+  HOST_WIDE_INT match_offset;
+  struct rel_use *next_chain;
+  struct rel_use **prev_chain_ref;
+  struct rel_use *next_hash;
+  struct rel_use *sibling;
+  unsigned int set_in_parallel:1;
+  unsigned int no_link_pred:1;
+};
+
+/* Describe a modification we have to do to the rtl when doing the
+   related value optimization.
+   There are two types of modifications: emitting a new add or move
+   insn, or updating an address within an existing insn.  We distinguish
+   between these two cases by testing whether the INSN field is nonzero.  */
+struct rel_mod
+{
+  /* Nonzero if we have to emit a new addition before this insn.
+     Otherwise, this describes an address update.  */
+  rtx insn;
+  /* The chain which this modification belongs to.  */
+  struct rel_use_chain *chain;
+  /* The position within the insn stream.  Used for sorting the set of
+     modifications in ascending order.  */
+  int luid;
+  /* Used to make the sort stable.  */
+  int count;
+  /* If this structure describes an addition, this is nonzero if the
+     source register is the base reg.  */
+  unsigned int from_base:1;
+};
+
+struct related **regno_related, *rel_base_list, *unrelatedly_used;
+
+#define rel_alloc(N) obstack_alloc(&related_obstack, (N))
+#define rel_new(X) ((X) = rel_alloc (sizeof *(X)))
+
+static struct obstack related_obstack;
+
+/* For each integer machine mode, the minimum and maximum constant that
+   can be added with a single constant.
+   This is supposed to define an interval around zero; if there are
+   singular points disconnected from this interval, we want to leave
+   them out.  */
+   
+static HOST_WIDE_INT add_limits[NUM_MACHINE_MODES][2];
+static char have_3addr_const_add[NUM_MACHINE_MODES];
+
+/* Try to find a related value with offset OFFSET from the base
+   register belonging to REGNO, using a register with preferred class
+   that is compatible with CLASS.  LUID is the insn in which we want
+   to use the matched register; this is used to avoid returning a
+   match that is an autoincrement within the same insn.  */
+
+static struct rel_use *
+lookup_related (int regno, enum reg_class class, HOST_WIDE_INT offset,
+		int luid)
+{
+  struct related *base = regno_related[regno]->base;
+  int hash = REL_USE_HASH (offset);
+  struct rel_use *match = base->baseinfo->hashtab[hash];
+  
+  for (; match; match = match->next_hash)
+    {
+      if (offset != match->match_offset)
+	continue;
+
+      /* If MATCH is an autoincrement in the same insn, ensure that it
+	 will not be used; otherwise we can end up with invalid rtl
+	 that uses the register outside the autoincrement.  */
+      if (match->luid == luid && match->offset != match->match_offset)
+	continue;
+
+      /* We are looking for a use which we can append to, so ignore
+	 anything that has already been appended to, and anything that
+	 must terminate a chain for other reasons.  */
+      if (match->next_chain)
+	continue;
+
+      if (regclass_compatible_p (class, match->class))
+	break;
+    }
+  
+  return match;
+}
+
+/* Add NEW_USE at the end of the chain that currently ends with MATCH;
+   If MATCH is not set, create a new chain.
+   BASE is the base register number the chain belongs to.  */
+
+static void
+rel_build_chain (struct rel_use *new_use, struct rel_use *match,
+		 struct related *base)
+{
+  int hash;
+
+  if (match)
+    {
+      struct rel_use *sibling = match;
+
+      do
+	{
+	  sibling->next_chain = new_use;
+	  if (sibling->prev_chain_ref)
+	    *sibling->prev_chain_ref = match;
+	  sibling = sibling->sibling;
+	}
+      while (sibling != match);
+
+      new_use->prev_chain_ref = &match->next_chain;
+    }
+  else
+    {
+      struct rel_use_chain *new_chain;
+
+      rel_new (new_chain);
+      new_chain->uses = new_use;
+      new_use->prev_chain_ref = &new_chain->uses;
+      new_chain->linked = 0;
+      new_chain->prev = base->baseinfo->chains;
+      base->baseinfo->chains = new_chain;
+    }
+  new_use->next_chain = 0;
+
+  hash = REL_USE_HASH (new_use->offset);
+  new_use->next_hash = base->baseinfo->hashtab[hash];
+  base->baseinfo->hashtab[hash] = new_use;
+}
+
+static struct rel_use *
+create_rel_use (rtx insn, rtx *xp, int regno, int luid, int call_tally)
+{
+  struct rel_use *new_use;
+  HOST_WIDE_INT offset = regno_related[regno]->offset;
+  enum reg_class class = reg_preferred_class (regno);
+
+  rel_new (new_use);
+  new_use->insn = insn;
+  new_use->addrp = xp;
+  new_use->luid = luid;
+  new_use->call_tally = call_tally;
+  new_use->class = class;
+  new_use->set_in_parallel = 0;
+  new_use->offset = offset;
+  new_use->match_offset = offset;
+  new_use->sibling = new_use;
+  new_use->no_link_pred = 0;
+
+  return new_use;
+}
+
+/* Record a new use of reg REGNO, which is found at address XP in INSN.
+   LUID and CALL_TALLY correspond to INSN.
+
+   There is a special case for uses of REGNO that must be preserved and
+   can't be optimized.  This case can happen either if we reach the end
+   of a block and a register which we track is still live, or if we find
+   a use of that register that can't be replaced inside an insn.  In
+   either case, TERMINATE should be set to a nonzero value.  */
+
+static void
+new_reg_use (rtx insn, rtx *xp, int regno, int luid, int call_tally,
+	     int terminate)
+{
+  struct rel_use *new_use, *match;
+  HOST_WIDE_INT offset = regno_related[regno]->offset;
+  enum reg_class class = reg_preferred_class (regno);
+  struct related *base = regno_related[regno]->base;
+
+  new_use = create_rel_use (insn, xp, regno, luid, call_tally);
+  match = lookup_related (regno, class, offset, luid);
+
+  rel_build_chain (new_use, match, base);
+  if (terminate)
+    new_use->next_chain = new_use;
+}
+
+/* Record the use of register ADDR in a memory reference.
+   ADDRP is the memory location where the address is stored.
+   MEM_MODE is the mode of the enclosing MEM.
+   SIZE is the size of the memory reference.
+   PRE_OFFS is the offset that has to be added to the value in ADDR
+   due to PRE_{IN,DE}CREMENT addressing in the original address; likewise,
+   POST_OFFSET denotes POST_{IN,DE}CREMENT addressing.  INSN is the
+   instruction that uses this address, LUID its luid, and CALL_TALLY
+   the current number of calls encountered since the start of the
+   function.  */
+      
+static void
+rel_record_mem (rtx *addrp, rtx addr, int size, int pre_offs, int post_offs,
+		rtx insn, int luid, int call_tally)
+{
+  rtx orig_addr = *addrp;
+  int regno;
+  struct related *base;
+  HOST_WIDE_INT offset;
+  struct rel_use *new_use, *match;
+  int hash;
+
+  gcc_assert (REG_P (addr));
+  
+  regno = REGNO (addr);
+
+  if (! regno_related[regno] || regno_related[regno]->invalidate_luid)
+    {
+      invalidate_related (addr, insn, luid, call_tally);
+      return;
+    }
+
+  offset = regno_related[regno]->offset += pre_offs;
+  base = regno_related[regno]->base;
+
+  if (base == 0)
+    return;
+
+  if (! test_addr)
+    test_addr = gen_rtx_PLUS (Pmode, addr, const0_rtx);
+
+  XEXP (test_addr, 0) = addr;
+  *addrp = test_addr;
+
+  new_use = create_rel_use (insn, addrp, regno, luid, call_tally);
+
+  match = lookup_related (regno, new_use->class, offset, luid);
+
+  /* Skip all the autoinc stuff if we found a match within the same insn.  */
+  if (match && match->luid == luid)
+    goto found_match;
+
+  if (! match)
+    {
+      /* We can choose PRE_{IN,DE}CREMENT on the spot with the information
+	 we have gathered about the preceding instructions, while we have
+	 to record POST_{IN,DE}CREMENT possibilities so that we can check
+	 later if we have a use for their output value.  */
+      /* We use recog here directly because we are only testing here if
+	 the changes could be made, but don't really want to make a
+	 change right now.  The caching from recog_memoized would only
+	 get in the way.  */
+
+      if (HAVE_PRE_INCREMENT)
+	{
+	  rtx p = PATTERN (insn);
+	  if (GET_CODE (p) == PARALLEL)
+	    p = XVECEXP (p, 0, 0);
+	  if (GET_CODE (p) != SET
+	      || !reg_overlap_mentioned_p (addr, XEXP (p, 1)))
+	    {
+	      match = lookup_related (regno, new_use->class, offset - size, luid);
+	      PUT_CODE (test_addr, PRE_INC);
+	      if (match && match->luid != luid
+		  && recog (PATTERN (insn), insn, NULL) >= 0)
+		goto found_match;
+	    }
+	}
+
+      if (HAVE_PRE_DECREMENT)
+	{
+	  rtx p = PATTERN (insn);
+	  if (GET_CODE (p) == PARALLEL)
+	    p = XVECEXP (p, 0, 0);
+	  if (GET_CODE (p) != SET
+	      || !reg_overlap_mentioned_p (addr, XEXP (p, 1)))
+	    {
+	      match = lookup_related (regno, new_use->class, offset + size, luid);
+	      PUT_CODE (test_addr, PRE_DEC);
+	      if (match && match->luid != luid
+		  && recog (PATTERN (insn), insn, NULL) >= 0)
+		goto found_match;
+	    }
+	}
+
+      match = 0;
+    }
+
+  PUT_CODE (test_addr, POST_INC);
+      
+  if (HAVE_POST_INCREMENT && recog (PATTERN (insn), insn, NULL) >= 0)
+    {
+      struct rel_use *inc_use;
+
+      rel_new (inc_use);
+      *inc_use = *new_use;
+      inc_use->sibling = new_use;
+      new_use->sibling = inc_use;
+      inc_use->prev_chain_ref = NULL;
+      inc_use->next_chain = NULL;
+      hash = REL_USE_HASH (inc_use->match_offset = offset + size);
+      inc_use->next_hash = base->baseinfo->hashtab[hash];
+      base->baseinfo->hashtab[hash] = inc_use;
+    }
+
+  PUT_CODE (test_addr, POST_DEC);
+
+  if (HAVE_POST_DECREMENT && recog (PATTERN (insn), insn, NULL) >= 0)
+    {
+      struct rel_use *dec_use;
+
+      rel_new (dec_use);
+      *dec_use = *new_use;
+      dec_use->sibling = new_use->sibling;
+      new_use->sibling = dec_use;
+      dec_use->prev_chain_ref = NULL;
+      dec_use->next_chain = NULL;
+      hash = REL_USE_HASH (dec_use->match_offset = offset + size);
+      dec_use->next_hash = base->baseinfo->hashtab[hash];
+      base->baseinfo->hashtab[hash] = dec_use;
+    }
+
+ found_match:
+      
+  rel_build_chain (new_use, match, base);
+  *addrp = orig_addr;
+
+  regno_related[regno]->offset += post_offs;
+}
+
+/* Note that REG is set to something that we do not regognize as a
+   related value, at an insn with linear uid LUID.  */
+
+static void
+invalidate_related (rtx reg, rtx insn, int luid, int call_tally)
+{
+  int regno = REGNO (reg);
+  struct related *rel = regno_related[regno];
+  if (rel && rel->base)
+    {
+      rel->invalidate_luid = luid;
+      rel->reg_orig_calls_crossed = call_tally - rel->reg_set_call_tally;
+    }
+  if (! rel || rel->base)
+    {
+      rel_new (rel);
+      regno_related[regno] = rel;
+      rel->prev = unrelatedly_used;
+      unrelatedly_used = rel;
+      rel->reg = reg;
+      rel->base = NULL;
+    }
+  rel->invalidate_luid = luid;
+  rel->insn = insn;
+}
+
+/* Record REG as a new base for related values.  INSN is the insn in which
+   we found it, LUID is its luid, and CALL_TALLY the number of calls seen
+   up to this point.  */
+
+static void
+new_base (rtx reg, rtx insn, int luid, int call_tally)
+{
+  int regno = REGNO (reg);
+  struct related *new_related;
+
+  rel_new (new_related);
+  new_related->reg = reg;
+  new_related->insn = insn;
+  new_related->updates = 0;
+  new_related->reg_set_call_tally = call_tally;
+  new_related->base = new_related;
+  new_related->offset = 0;
+  new_related->prev = 0;
+  new_related->invalidate_luid = 0;
+  new_related->death = NULL_RTX;
+  rel_new (new_related->baseinfo);
+  memset ((char *) new_related->baseinfo, 0, sizeof *new_related->baseinfo);
+  new_related->baseinfo->prev_base = rel_base_list;
+  rel_base_list = new_related;
+  new_related->baseinfo->insn_luid = luid;
+  regno_related[regno] = new_related;
+}
+
+/* Check out if INSN sets a new related value.  Return nonzero if we could
+   handle this insn.  */
+
+static int
+recognize_related_for_insn (rtx insn, int luid, int call_tally)
+{
+  rtx set = single_set (insn);
+  rtx src, dst;
+  rtx src_reg, src_const;
+  int src_regno, dst_regno;
+  struct related *new_related;
+
+  /* We don't care about register class differences here, since
+     we might still find multiple related values share the same
+     class even if it is disjunct from the class of the original
+     register.  */
+
+  if (set == 0)
+    return 0;
+
+  dst = SET_DEST (set);
+  src = SET_SRC (set);
+
+  /* First check that we have actually something like
+     (set (reg pseudo_dst) (plus (reg pseudo_src) (const_int))) .  */
+  if (GET_CODE (src) == PLUS)
+    {
+      src_reg = XEXP (src, 0);
+      src_const = XEXP (src, 1);
+    }
+  else if (REG_P (src) && GET_MODE_CLASS (GET_MODE (src)) == MODE_INT)
+    {
+      src_reg = src;
+      src_const = const0_rtx;
+    }
+  else
+    return 0;
+
+  if (!REG_P (src_reg) || GET_CODE (src_const) != CONST_INT || !REG_P (dst))
+    return 0;
+
+  dst_regno = REGNO (dst);
+  src_regno = REGNO (src_reg);
+
+  if (src_regno < FIRST_PSEUDO_REGISTER
+      || dst_regno < FIRST_PSEUDO_REGISTER)
+    return 0;
+
+  /* Check if this is merely an update of a register with a
+     value belonging to a group of related values we already
+     track.  */
+  if (regno_related[dst_regno] && ! regno_related[dst_regno]->invalidate_luid)
+    {
+      struct update *new_update;
+
+      /* If the base register changes, don't handle this as a
+	 related value.  We can currently only attribute the
+	 register to one base, and keep record of one lifetime
+	 during which we might re-use the register.  */
+      if (! regno_related[src_regno]
+	  || regno_related[src_regno]->invalidate_luid
+	  || (regno_related[dst_regno]->base
+	      != regno_related[src_regno]->base))
+	return 0;
+
+      regno_related[dst_regno]->offset
+	= regno_related[src_regno]->offset + INTVAL (src_const);
+      rel_new (new_update);
+      new_update->insn = insn;
+      new_update->death_insn = regno_related[dst_regno]->death;
+      regno_related[dst_regno]->death = NULL_RTX;
+      new_update->prev = regno_related[dst_regno]->updates;
+      regno_related[dst_regno]->updates = new_update;
+
+      return 1;
+    }
+
+  if (! regno_related[src_regno] || regno_related[src_regno]->invalidate_luid)
+    {
+      if (src_regno == dst_regno)
+	return 0;
+
+      new_base (src_reg, insn, luid, call_tally);
+    }
+  /* If the destination register has been used since we started
+     tracking this group of related values, there would be tricky
+     lifetime problems that we don't want to tackle right now.  */
+  else if (regno_related[dst_regno]
+	   && (regno_related[dst_regno]->invalidate_luid
+	       >= regno_related[src_regno]->base->baseinfo->insn_luid))
+    return 0;
+
+  rel_new (new_related);
+  new_related->reg = dst;
+  new_related->insn = insn;
+  new_related->updates = 0;
+  new_related->reg_set_call_tally = call_tally;
+  new_related->base = regno_related[src_regno]->base;
+  new_related->offset = regno_related[src_regno]->offset + INTVAL (src_const);
+  new_related->invalidate_luid = 0;
+  new_related->death = NULL_RTX;
+  new_related->prev = regno_related[src_regno]->prev;
+  regno_related[src_regno]->prev = new_related;
+  regno_related[dst_regno] = new_related;
+
+  return 1;
+}
+
+/* Record a use of a register at *XP, which is not inside a MEM which we
+   consider changing except for plain register substitution.  */
+static void
+record_reg_use (rtx *xp, rtx insn, int luid, int call_tally)
+{
+  rtx x = *xp;
+  int regno = REGNO (x);
+	
+  if (! regno_related[regno])
+    {
+      rel_new (regno_related[regno]);
+      regno_related[regno]->prev = unrelatedly_used;
+      unrelatedly_used = regno_related[regno];
+      regno_related[regno]->reg = x;
+      regno_related[regno]->base = NULL;
+      regno_related[regno]->invalidate_luid = luid;
+      regno_related[regno]->insn = insn;
+    }
+  else if (regno_related[regno]->invalidate_luid)
+    {
+      regno_related[regno]->invalidate_luid = luid;
+      regno_related[regno]->insn = insn;
+    }
+  else
+    new_reg_use (insn, xp, regno, luid, call_tally, 0);
+}
+
+/* Check the RTL fragment pointed to by XP for related values - that is,
+   if any new are created, or if they are assigned new values.  Also
+   note any other sets so that we can track lifetime conflicts.
+   INSN is the instruction XP points into, LUID its luid, and CALL_TALLY
+   the number of preceding calls in the function.  */
+
+static void
+find_related (rtx *xp, rtx insn, int luid, int call_tally)
+{
+  rtx x = *xp;
+  enum rtx_code code = GET_CODE (x);
+  const char *fmt;
+  int i;
+
+  if (code == REG)
+    record_reg_use (xp, insn, luid, call_tally);
+  else if (code == MEM)
+    {
+      enum machine_mode mem_mode = GET_MODE (x);
+      int size = GET_MODE_SIZE (mem_mode);
+      rtx *addrp= &XEXP (x, 0), addr = *addrp;
+
+      switch (GET_CODE (addr))
+	{
+	case REG:
+	  rel_record_mem (addrp, addr, size, 0, 0,
+			  insn, luid, call_tally);
+	  return;
+	case PRE_INC:
+	  rel_record_mem (addrp, XEXP (addr, 0), size, size, 0,
+			  insn, luid, call_tally);
+	  return;
+	case POST_INC:
+	  rel_record_mem (addrp, XEXP (addr, 0), size, 0, size,
+			  insn, luid, call_tally);
+	  return;
+	case PRE_DEC:
+	  rel_record_mem (addrp, XEXP (addr, 0), size, -size, 0,
+			  insn, luid, call_tally);
+	  return;
+	case POST_DEC:
+	  rel_record_mem (addrp, XEXP (addr, 0), size, 0, -size,
+			  insn, luid, call_tally);
+	  return;
+	default:
+	  break;
+	}
+    }
+  
+  fmt = GET_RTX_FORMAT (code);
+
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    {
+      if (fmt[i] == 'e')
+	find_related (&XEXP (x, i), insn, luid, call_tally);
+      
+      if (fmt[i] == 'E')
+	{
+	  register int j;
+	  
+	  for (j = 0; j < XVECLEN (x, i); j++)
+	    find_related (&XVECEXP (x, i, j), insn, luid, call_tally);
+	}
+    }
+}
+
+/* Process one insn for optimize_related_values.  INSN is the insn, LUID
+   and CALL_TALLY its corresponding luid and number of calls seen so
+   far.  */
+static void
+find_related_toplev (rtx insn, int luid, int call_tally)
+{
+  int i;
+
+  /* First try to process the insn as a whole.  */
+  if (recognize_related_for_insn (insn, luid, call_tally))
+    return;
+
+  if (GET_CODE (PATTERN (insn)) == USE
+      || GET_CODE (PATTERN (insn)) == CLOBBER)
+    {
+      rtx *xp = &XEXP (PATTERN (insn), 0);
+      int regno;
+      
+      if (!REG_P (*xp))
+	{
+	  find_related (xp, insn, luid, call_tally);
+	  return;
+	}
+
+      regno = REGNO (*xp);
+      if (GET_CODE (PATTERN (insn)) == USE
+	  && regno_related[regno]
+	  && ! regno_related[regno]->invalidate_luid)
+	new_reg_use (insn, xp, regno, luid, call_tally, 1);
+      invalidate_related (*xp, insn, luid, call_tally);
+      return;
+    }
+
+  if (CALL_P (insn) && CALL_INSN_FUNCTION_USAGE (insn))
+    {
+      rtx usage;
+
+      for (usage = CALL_INSN_FUNCTION_USAGE (insn);
+	   usage;
+	   usage = XEXP (usage, 1))
+	find_related (&XEXP (usage, 0), insn, luid, call_tally);
+    }
+
+  extract_insn (insn);
+  /* Process all inputs.  */
+  for (i = 0; i < recog_data.n_operands; i++)
+    {
+      rtx *loc = recog_data.operand_loc[i];
+      rtx op = *loc;
+
+      if (op == NULL)
+	continue;
+
+      while (GET_CODE (op) == SUBREG
+	     || GET_CODE (op) == ZERO_EXTRACT
+	     || GET_CODE (op) == SIGN_EXTRACT
+	     || GET_CODE (op) == STRICT_LOW_PART)
+	loc = &XEXP (op, 0), op = *loc;
+
+      if (recog_data.operand_type[i] == OP_IN || !REG_P (op))
+	find_related (loc, insn, luid, call_tally);
+    }
+
+
+  /* If we have an OP_IN type operand with match_dups, process those
+     duplicates also.  */
+  for (i = 0; i < recog_data.n_dups; i++)
+    {
+      int opno = recog_data.dup_num[i];
+      rtx *loc = recog_data.dup_loc[i];
+      rtx op = *loc;
+
+      while (GET_CODE (op) == SUBREG
+	     || GET_CODE (op) == ZERO_EXTRACT
+	     || GET_CODE (op) == SIGN_EXTRACT
+	     || GET_CODE (op) == STRICT_LOW_PART)
+	loc = &XEXP (op, 0), op = *loc;
+
+      if (recog_data.operand_type[opno] == OP_IN || !REG_P (op))
+	find_related (loc, insn, luid, call_tally);
+    }
+  
+  /* Process outputs.  */
+  for (i = 0; i < recog_data.n_operands; i++)
+    {
+      enum op_type type = recog_data.operand_type[i];
+      rtx *loc = recog_data.operand_loc[i];
+      rtx op = *loc;
+
+      if (op == NULL)
+	continue;
+
+      while (GET_CODE (op) == SUBREG
+	     || GET_CODE (op) == ZERO_EXTRACT
+	     || GET_CODE (op) == SIGN_EXTRACT
+	     || GET_CODE (op) == STRICT_LOW_PART)
+	loc = &XEXP (op, 0), op = *loc;
+
+      /* Detect if we're storing into only one word of a multiword
+	 subreg.  */
+      if (loc != recog_data.operand_loc[i] && type == OP_OUT)
+	type = OP_INOUT;
+
+      if (REG_P (op))
+	{
+	  int regno = REGNO (op);
+
+	  if (type == OP_INOUT)
+	    {							
+	      /* This is a use we can't handle.  Add a dummy use of this
+		 register as well as invalidating it.  */
+	      if (regno_related[regno]
+		  && ! regno_related[regno]->invalidate_luid)
+		new_reg_use (insn, loc, regno, luid, call_tally, 1);
+	    }
+
+	  if (type != OP_IN)
+	    /* A set of a register invalidates it (unless the set was
+	       handled by recognize_related_for_insn).  */
+	    invalidate_related (op, insn, luid, call_tally);
+	}
+    }
+}
+
+/* Comparison functions for qsort.  */
+static int
+chain_starts_earlier (const void *chain1, const void *chain2)
+{
+  int d = ((*(struct rel_use_chain **)chain2)->start_luid
+	   - (*(struct rel_use_chain **)chain1)->start_luid);
+  if (! d)
+    d = ((*(struct rel_use_chain **)chain2)->uses->offset
+         - (*(struct rel_use_chain **)chain1)->uses->offset);
+  if (! d)
+    d = ((*(struct rel_use_chain **)chain2)->uses->set_in_parallel
+         - (*(struct rel_use_chain **)chain1)->uses->set_in_parallel);
+  
+  /* If set_in_parallel is not set on both chain's first use, they must
+     differ in start_luid or offset, since otherwise they would use the
+     same chain.
+     Thus the remaining problem is with set_in_parallel uses; for these, we
+     know that *addrp is a register.  Since the same register may not be set
+     multiple times in the same insn, the registers must be different.  */
+     
+  if (! d)
+    d = (REGNO (*(*(struct rel_use_chain **)chain2)->uses->addrp)
+         - REGNO (*(*(struct rel_use_chain **)chain1)->uses->addrp));
+  return d;
+}
+
+static int
+chain_ends_later (const void *chain1, const void *chain2)
+{
+  int d = ((*(struct rel_use_chain **)chain1)->end->no_link_pred
+	   - (*(struct rel_use_chain **)chain2)->end->no_link_pred);
+  if (! d)
+    d = ((*(struct rel_use_chain **)chain1)->end_luid
+	 - (*(struct rel_use_chain **)chain2)->end_luid);
+  if (! d)
+    d = ((*(struct rel_use_chain **)chain2)->uses->offset
+         - (*(struct rel_use_chain **)chain1)->uses->offset);
+  if (! d)
+    d = ((*(struct rel_use_chain **)chain2)->uses->set_in_parallel
+         - (*(struct rel_use_chain **)chain1)->uses->set_in_parallel);
+  
+  /* If set_in_parallel is not set on both chain's first use, they must
+     differ in start_luid or offset, since otherwise they would use the
+     same chain.
+     Thus the remaining problem is with set_in_parallel uses; for these, we
+     know that *addrp is a register.  Since the same register may not be set
+     multiple times in the same insn, the registers must be different.  */
+     
+  if (! d)
+    {
+      rtx reg1 = (*(*(struct rel_use_chain **)chain1)->uses->addrp);
+      rtx reg2 = (*(*(struct rel_use_chain **)chain2)->uses->addrp);
+
+      switch (GET_CODE (reg1))
+	{
+	case REG:
+	  break;
+
+	case PRE_INC:
+	case POST_INC:
+	case PRE_DEC:
+	case POST_DEC:
+	  reg1 = XEXP (reg1, 0);
+	  break;
+
+	default:
+	  gcc_unreachable ();
+	}
+
+      switch (GET_CODE (reg2))
+	{
+	case REG:
+	  break;
+
+	case PRE_INC:
+	case POST_INC:
+	case PRE_DEC:
+	case POST_DEC:
+	  reg2 = XEXP (reg2, 0);
+	  break;
+
+	default:
+	  gcc_unreachable ();
+	}
+
+	d = (REGNO (reg2) - REGNO (reg1));
+    }
+  return d;
+}
+
+/* Called through qsort, used to sort rel_mod structures in ascending
+   order by luid.  */
+static int
+mod_before (const void *ptr1, const void *ptr2)
+{
+  const struct rel_mod *insn1 = ptr1;
+  const struct rel_mod *insn2 = ptr2;
+  if (insn1->luid != insn2->luid)
+    return insn1->luid - insn2->luid;
+  /* New add insns get inserted before the luid, modifications are
+     performed within this luid.  */
+  if (insn1->insn == 0 && insn2->insn != 0)
+    return 1;
+  if (insn2->insn == 0 && insn1->insn != 0)
+    return -1;
+  return insn1->count - insn2->count;
+}
+
+/* Update REG_N_SETS given a newly generated insn.  Called through
+   note_stores.  */
+static void
+count_sets (rtx x, rtx pat ATTRIBUTE_UNUSED, void *data ATTRIBUTE_UNUSED)
+{
+  if (REG_P (x))
+    REG_N_SETS (REGNO (x))++;
+}
+
+/* First pass of performing the optimization on a set of related values:
+   remove all the setting insns, death notes and refcount increments that
+   are now obsolete.
+   INSERT_BEFORE is an insn which we not must delete except by by turning it
+   into a note, since it is needed later.  */
+static void
+remove_setting_insns (struct related *rel_base, rtx insert_before)
+{
+  struct related *rel;
+
+  for (rel = rel_base; rel; rel = rel->prev)
+    {
+      struct update *update;
+      int regno = REGNO (rel->reg);
+
+      if (rel != rel_base)
+	{
+	  /* The first setting insn might be the start of a basic block.  */
+	  if (rel->insn == rel_base->insn
+	      /* We have to preserve insert_before.  */
+	      || rel->insn == insert_before)
+	    {
+	      PUT_CODE (rel->insn, NOTE);
+	      NOTE_LINE_NUMBER (rel->insn) = NOTE_INSN_DELETED;
+	      NOTE_SOURCE_FILE (rel->insn) = 0;
+	    }
+	  else
+	    delete_insn (rel->insn);
+	  REG_N_SETS (regno)--;
+	}
+
+      REG_N_CALLS_CROSSED (regno) -= rel->reg_orig_calls_crossed;
+	  
+      for (update = rel->updates; update; update = update->prev)
+	{
+	  rtx death_insn = update->death_insn;
+	      
+	  if (death_insn)
+	    {
+	      rtx death_note
+		= find_reg_note (death_insn, REG_DEAD, rel->reg);
+	      if (! death_note)
+		death_note
+		  = find_reg_note (death_insn, REG_UNUSED, rel->reg);
+	      remove_note (death_insn, death_note);
+	      REG_N_DEATHS (regno)--;
+	    }
+	      
+	  /* We have to preserve insert_before.  */
+	  if (update->insn == insert_before)
+	    {
+	      PUT_CODE (update->insn, NOTE);
+	      NOTE_LINE_NUMBER (update->insn) = NOTE_INSN_DELETED;
+	      NOTE_SOURCE_FILE (update->insn) = 0;
+	    }
+	  else
+	    delete_insn (update->insn);
+	      
+	  REG_N_SETS (regno)--;
+	}
+	  
+      if (rel->death)
+	{
+	  rtx death_note = find_reg_note (rel->death, REG_DEAD, rel->reg);
+	  if (! death_note)
+	    death_note = find_reg_note (rel->death, REG_UNUSED, rel->reg);
+	  remove_note (rel->death, death_note);
+	  rel->death = death_note;
+	  REG_N_DEATHS (regno)--;
+	}
+    }
+}
+
+/* Create a new add (or move) instruction as described by the modification
+   MOD, which is for the rel_use USE.  BASE_REG is the base register for
+   this set of related values, REL_BASE_REG_USER is the chain that uses
+   it.  */
+static rtx
+perform_addition (struct rel_mod *mod, struct rel_use *use, rtx base_reg,
+		  struct rel_use_chain *rel_base_reg_user)
+{
+  HOST_WIDE_INT use_offset = use->offset;
+  /* We have to generate a new addition or move insn and emit it
+     before the current use in this chain.  */
+  HOST_WIDE_INT new_offset = use_offset;
+  rtx reg = mod->chain->reg;
+  rtx src_reg;
+
+  if (mod->from_base)
+    {
+      src_reg = base_reg;
+      if (rel_base_reg_user)
+	use_offset -= rel_base_reg_user->match_offset;
+    }
+  else
+    {
+      src_reg = reg;
+      use_offset -= mod->chain->match_offset;
+    }
+
+  if (use_offset != 0 || src_reg != reg)
+    {
+      rtx new;
+      if (use_offset == 0)
+	new = gen_move_insn (reg, src_reg);
+      else
+	new = gen_add3_insn (reg, src_reg, GEN_INT (use_offset));
+
+      gcc_assert (new);
+
+      if (GET_CODE (new) == SEQUENCE)
+	{
+	  int i;
+
+	  for (i = XVECLEN (new, 0) - 1; i >= 0; i--)
+	    note_stores (PATTERN (XVECEXP (new, 0, i)), count_sets,
+			 NULL);
+	}
+      else
+	note_stores (new, count_sets, NULL);
+      new = emit_insn_before (new, mod->insn);
+
+      mod->chain->match_offset = new_offset;
+      return new;
+    }
+  return 0;
+}
+
+/* Perform the modification described by MOD, which applies to the use
+   described by USE.
+   This function calls validate_change; the caller must call
+   apply_change_group after all modifications for the same insn have
+   been performed.  */
+static void
+modify_address (struct rel_mod *mod, struct rel_use *use,
+		HOST_WIDE_INT current_offset)
+{
+  HOST_WIDE_INT use_offset = use->offset;
+  rtx reg = mod->chain->reg;
+  /* We have to perform a modification on a given use.  The
+     current use will be removed from the chain afterwards.  */
+  rtx addr = *use->addrp;
+
+  if (!REG_P (addr))
+    remove_note (use->insn,
+		 find_reg_note (use->insn, REG_INC,
+				XEXP (addr, 0)));
+
+  if (use_offset == current_offset)
+    {
+      if (use->set_in_parallel)
+	{
+	  REG_N_SETS (REGNO (addr))--;
+	  addr = reg;
+	}
+      else if (use->match_offset > use_offset)
+	addr = gen_rtx_POST_INC (Pmode, reg);
+      else if (use->match_offset < use_offset)
+	addr = gen_rtx_POST_DEC (Pmode, reg);
+      else
+	addr = reg;
+    }
+  else if (use_offset > current_offset)
+    addr = gen_rtx_PRE_INC (Pmode, reg);
+  else
+    addr = gen_rtx_PRE_DEC (Pmode, reg);
+
+  /* Group changes from the same chain for the same insn
+     together, to avoid failures for match_dups.  */
+  validate_change (use->insn, use->addrp, addr, 1);
+
+  if (addr != reg)
+    REG_NOTES (use->insn)
+      = gen_rtx_EXPR_LIST (REG_INC, reg, REG_NOTES (use->insn));
+
+  /* Update the chain's state: set match_offset as appropriate,
+     and move towards the next use.  */
+  mod->chain->match_offset = use->match_offset;
+  mod->chain->uses = use->next_chain;
+  if (mod->chain->uses == 0 && mod->chain->linked)
+    {
+      struct rel_use_chain *linked = mod->chain->linked;
+      mod->chain->linked = linked->linked;
+      mod->chain->uses = linked->uses;
+    }
+}
+
+/* Try to link SUCC_CHAIN as sucessor of PRED_CHAIN.  BASE_MODE is
+   the machine mode of the base register.  Return nonzero on success.  */
+static int
+link_chains (struct rel_use_chain *pred_chain,
+	     struct rel_use_chain *succ_chain, enum machine_mode base_mode)
+{
+  if (succ_chain->start_luid > pred_chain->end_luid
+      && ! pred_chain->end->no_link_pred
+      && (! pred_chain->invalidate_luid
+	  || pred_chain->invalidate_luid > succ_chain->end_luid)
+      && regclass_compatible_p (succ_chain->uses->class,
+				pred_chain->uses->class)
+      /* add_limits is not valid for MODE_PARTIAL_INT .  */
+      && GET_MODE_CLASS (base_mode) == MODE_INT
+      && !have_3addr_const_add[(int) base_mode]
+      && (succ_chain->uses->offset - pred_chain->match_offset
+	  >= add_limits[(int) base_mode][0])
+      && (succ_chain->uses->offset - pred_chain->match_offset
+	  <= add_limits[(int) base_mode][1]))
+    {
+      /* We can link these chains together.  */
+      pred_chain->linked = succ_chain;
+      succ_chain->start_luid = 0;
+      pred_chain->end_luid = succ_chain->end_luid;
+      return 1;
+    }
+  return 0;
+}
+
+/* Perform the optimization for a single set of related values.
+   INSERT_BEFORE is an instruction before which we may emit instructions
+   to initialize registers that remain live beyond the end of the group
+   of instructions which have been examined.  */
+
+static void
+optimize_related_values_1 (struct related *rel_base, int luid, int call_tally,
+			   rtx insert_before)
+{
+  struct related_baseinfo *baseinfo = rel_base->baseinfo;
+  struct related *rel;
+  struct rel_use_chain *chain, **chain_starttab, **chain_endtab;
+  struct rel_use_chain **pred_chainp, *pred_chain;
+  int num_regs, num_av_regs, num_chains, num_linked, max_end_luid, i;
+  int max_start_luid;
+  struct rel_use_chain *rel_base_reg_user;
+  enum machine_mode mode;
+  HOST_WIDE_INT rel_base_reg_user_offset = 0;
+
+  /* For any registers that are still live, we have to arrange
+     to have them set to their proper values.
+     Also count with how many registers (not counting base) we are
+     dealing with here.  */
+  for (num_regs = -1, rel = rel_base; rel; rel = rel->prev, num_regs++)
+    {
+      int regno = REGNO (rel->reg);
+
+      if (! rel->death && ! rel->invalidate_luid)
+	{
+	  new_reg_use (insert_before, &rel->reg, regno, luid, call_tally, 1);
+	  rel->reg_orig_calls_crossed = call_tally - rel->reg_set_call_tally;
+	}
+    }
+
+  /* Now for every chain of values related to the base, set start
+     and end luid, match_offset, and reg.  Also count the number of these
+     chains, and determine the largest end luid.  */
+  num_chains = 0;
+  
+  for (max_end_luid = 0, chain = baseinfo->chains; chain; chain = chain->prev)
+    {
+      struct rel_use *use, *next;
+
+      num_chains++;
+      next = chain->uses;
+      chain->start_luid = next->luid;
+      do
+	{
+	  use = next;
+	  next = use->next_chain;
+	}
+      while (next && next != use);
+
+      use->no_link_pred = next != NULL;
+      use->next_chain = 0;
+
+      chain->end = use;
+      chain->end_luid = use->luid;
+      chain->match_offset = use->match_offset;
+      chain->calls_crossed = use->call_tally - chain->uses->call_tally;
+      
+      chain->reg = ! use->no_link_pred ? NULL_RTX : *use->addrp;
+
+      if (chain->reg)
+	{
+	  /* Copy if an invalidate_luid is attached to this register.
+	     ??? This loop makes the algorithm quadratic on the number
+	     of registers.  If this is a problem, the cost can be brought
+	     down to O(N*log(N)) by using sorting.  */
+	  for (rel = rel_base; rel; rel = rel->prev)
+	    if (rel->reg == chain->reg)
+	      {
+		chain->invalidate_luid = rel->invalidate_luid;
+		break;
+	      }
+	}
+      else
+	chain->invalidate_luid = 0;
+
+      if (use->luid > max_end_luid)
+	max_end_luid = use->luid;
+
+      if (dump_file)
+	fprintf (dump_file, "Chain start: %d end: %d\n",
+		 chain->start_luid, chain->end_luid);
+    }
+
+  if (dump_file)
+    fprintf (dump_file,
+	     "Insn %d reg %d: found %d chains.\n",
+	     INSN_UID (rel_base->insn), REGNO (rel_base->reg), num_chains);
+
+  if (! num_chains)
+    return;
+
+  /* For every chain, we try to find another chain the lifetime of which
+     ends before the lifetime of said chain starts.
+     So we first sort according to luid of first and last instruction that
+     is in the chain, respectively;  this is O(n * log n) on average.  */
+  chain_starttab = rel_alloc (num_chains * sizeof *chain_starttab);
+  chain_endtab = rel_alloc (num_chains * sizeof *chain_starttab);
+  
+  for (chain = baseinfo->chains, i = 0; chain; chain = chain->prev, i++)
+    {
+      chain_starttab[i] = chain;
+      chain_endtab[i] = chain;
+    }
+  
+  qsort (chain_starttab, num_chains, sizeof *chain_starttab,
+	 chain_starts_earlier);
+  qsort (chain_endtab, num_chains, sizeof *chain_endtab, chain_ends_later);
+
+  /* Now we go through every chain, starting with the one that starts
+     second (we can skip the first because we know there would be no match),
+     and check it against the chain that ends first.  */
+  /* ??? We assume here that reg_class_compatible_p will seldom return false.
+     If that is not true, we should do a more thorough search for suitable
+     chain combinations.  */
+  pred_chainp = chain_endtab;
+  pred_chain = *pred_chainp;
+  max_start_luid = chain_starttab[num_chains - 1]->start_luid;
+  
+  mode = GET_MODE (rel_base->reg);
+  for (num_linked = 0, i = num_chains - 2; i >= 0; i--)
+    {
+      struct rel_use_chain *succ_chain = chain_starttab[i];
+
+      if ((pred_chain->calls_crossed
+	   ? succ_chain->calls_crossed
+	   : succ_chain->end->call_tally == pred_chain->uses->call_tally)
+	  && link_chains (pred_chain, succ_chain, mode))
+	{
+	  num_linked++;
+	  pred_chain = *++pred_chainp;
+	}
+      else
+	max_start_luid = succ_chain->start_luid;
+    }
+
+  if (dump_file && num_linked)
+    fprintf (dump_file, "Linked to %d sets of chains.\n",
+	     num_chains - num_linked);
+
+  /* Now count the number of registers that are available for reuse.  */
+  /* ??? In rare cases, we might reuse more if we took different
+     end luids of the chains into account.  Or we could just allocate
+     some new regs.  But that would probably not be worth the effort.  */
+  /* ??? We should pay attention to preferred register classes here too,
+     if the to-be-allocated register have a life outside the range that
+     we handle.  */
+  for (num_av_regs = 0, rel = rel_base->prev; rel; rel = rel->prev)
+    {
+      if (! rel->invalidate_luid
+	  || rel->invalidate_luid > max_end_luid)
+	num_av_regs++;
+    }
+
+  /* Propagate mandatory register assignments to the first chain in
+     all sets of linked chains, and set rel_base_reg_user.  */
+  for (rel_base_reg_user = 0, i = 0; i < num_chains; i++)
+    {
+      struct rel_use_chain *chain = chain_starttab[i];
+      if (chain->linked)
+	chain->reg = chain->linked->reg;
+      if (chain->reg == rel_base->reg)
+	rel_base_reg_user = chain;
+    }
+    
+  /* If rel_base->reg is not a mandatory allocated register, allocate
+     it to that chain that starts first and has no allocated register,
+     and that allows the addition of the start value in a single
+     instruction.  */
+  if (! rel_base_reg_user)
+    {
+      for (i = num_chains - 1; i >= 0; --i)
+	{
+	  struct rel_use_chain *chain = chain_starttab[i];
+	  if (! chain->reg
+	      && chain->start_luid
+	      && (!have_3addr_const_add[(int) mode] || !chain->uses->offset)
+	      && chain->uses->offset >= add_limits[(int) mode][0]
+	      && chain->uses->offset <= add_limits[(int) mode][1]
+	      /* Also can't use this chain if its register is clobbered
+		 and other chains need to start later.  */
+	      && (! (chain->end->no_link_pred && chain->end->insn)
+		  || chain->end_luid >= max_start_luid)
+	      /* Also can't use it if it lasts longer than the
+		 base reg is available.  */
+	      && (! rel_base->invalidate_luid
+		  || rel_base->invalidate_luid > chain->end_luid))
+	    {
+	      chain->reg = rel_base->reg;
+	      chain->invalidate_luid = rel_base->invalidate_luid;
+	      rel_base_reg_user = chain;
+	      if (num_linked < num_chains - 1)
+		{
+		  int old_linked = num_linked;
+
+		  for (i = num_chains - 2; i >= 0; i--)
+		    {
+		      struct rel_use_chain *succ_chain = chain_starttab[i];
+
+		      while (chain->linked)
+			chain = chain->linked;
+		      if (succ_chain->start_luid
+			  && ! succ_chain->reg
+			  && link_chains (chain, succ_chain, mode))
+			{
+			  num_linked++;
+			  chain = succ_chain;
+			}
+		    }
+		  if (dump_file && num_linked > old_linked)
+		      fprintf (dump_file,
+			       "Linked to %d sets of chains.\n",
+			       num_chains - num_linked);
+		}
+	      break;
+	    }
+	}
+    }
+  else
+    rel_base_reg_user_offset = rel_base_reg_user->uses->offset;
+
+  /* If there are any chains that need to be initialized after the base
+     register has been invalidated, the optimization cannot be done.  */
+  for (i = 0; i < num_chains; i++)
+    {
+      struct rel_use_chain *chain = chain_starttab[i];
+
+      if (rel_base->invalidate_luid
+	  && chain->start_luid > rel_base->invalidate_luid)
+	return;
+    }
+
+  /* Now check if it is worth doing this optimization after all.
+     Using separate registers per value, like in the code generated by cse,
+     costs two instructions per register (one move and one add).
+     Using the chains we have set up, we need two instructions for every
+     linked set of chains, plus one instruction for every link;
+     however, if the base register is allocated to a chain
+     (i.e. rel_base_reg_user != 0), we don't need a move insn to start
+     that chain.
+     If we have a three-address add, however, the cost per value / chain
+     is just one insn, and linking chains is pointless.
+     We do the optimization if we save instructions, or if we
+     stay with the same number of instructions, but save registers.
+     We also require that we have enough registers available for reuse.
+     Moreover, we have to check that we can add the offset for
+     rel_base_reg_user, in case it is a mandatory allocated register.  */
+  if ((have_3addr_const_add[(int) mode]
+       ? (num_regs > num_chains - (rel_base_reg_user != 0))
+       : (2 * num_regs
+	  > ((2 * num_chains - num_linked - (rel_base_reg_user != 0))
+	     - (num_linked != 0))))
+      && num_av_regs + (rel_base_reg_user != 0) >= num_chains - num_linked
+      && rel_base_reg_user_offset >= add_limits[(int) mode][0]
+      && rel_base_reg_user_offset <= add_limits[(int) mode][1])
+    {
+      unsigned int base_regno = REGNO (rel_base->reg);
+      int num_mods;
+      int num_uses;
+      struct rel_mod *mods;
+      rtx last_changed_insn = 0;
+
+      /* Record facts about the last place where the base register is used.  */
+      int last_base_call_tally = rel_base->reg_set_call_tally;
+      rtx last_base_insn = 0;
+
+      if (dump_file)
+	fprintf (dump_file, "Optimization is worth while.\n");
+
+      remove_setting_insns (rel_base, insert_before);
+
+      /* Allocate regs for each chain, and count the number of uses.  */
+      rel = rel_base;
+      for (num_uses = 0, i = 0; i < num_chains; i++)
+	{
+	  struct rel_use_chain *chain0 = chain_starttab[i];
+	  unsigned int regno;
+	  int first_call_tally, last_call_tally;
+
+	  if (! chain0->start_luid)
+	    continue;
+
+	  /* If this chain has not got a register yet, assign one.  */
+	  if (! chain0->reg)
+	    {
+	      do
+		rel = rel->prev;
+	      while (! rel->death
+		     || (rel->invalidate_luid
+			 && rel->invalidate_luid <= max_end_luid));
+
+	      chain0->reg = rel->reg;
+	      chain0->death_note = rel->death;
+	    }
+	  else
+	    chain0->death_note = 0;
+
+	  /* For all registers except the base register, we can already
+	     determine the number of calls crossed at this point by
+	     examining the call_tally of the first and the last use.
+	     We can't do this for the base register yet, since we don't
+	     know its exact lifetime yet.  */
+	  regno = REGNO (chain0->reg);
+	  first_call_tally = last_call_tally = chain0->uses->call_tally;
+
+	  while (chain0)
+		{
+	      struct rel_use *use;
+	      for (use = chain0->uses; use; use = use->next_chain)
+		    {
+		  num_uses++;
+		  last_call_tally = use->call_tally;
+		    }
+	      chain0 = chain0->linked;
+		}
+
+	  if (regno != base_regno)
+	    REG_N_CALLS_CROSSED (regno) += last_call_tally - first_call_tally;
+	    }
+
+      /* Record all the modifications we need to perform together with
+	 their position, then sort the array by position.  */
+      mods = rel_alloc ((num_chains + num_uses) * sizeof *mods);
+      for (i = num_mods = 0; i < num_chains; i++)
+	{
+	  struct rel_use_chain *chain0 = chain_starttab[i];
+
+	  if (! chain0->start_luid)
+	    continue;
+
+	  for (chain = chain0; chain; chain = chain->linked)
+	    {
+	      struct rel_use *use = chain->uses;
+
+	      /* Initializing insn: an add (or move if offset == 0).  */
+	      mods[num_mods].from_base = use == chain0->uses;
+	      mods[num_mods].chain = chain0;
+	      mods[num_mods].insn = use->insn;
+	      mods[num_mods].luid = use->luid;
+	      mods[num_mods].count = num_mods;
+	      num_mods++;
+
+	      /* All the other uses: no additional insn, but offset
+		 updates.  */
+	      for (; use; use = use->next_chain)
+		{
+		  mods[num_mods].chain = chain0;
+		  mods[num_mods].insn = 0;
+		  mods[num_mods].luid = use->luid;
+		  mods[num_mods].count = num_mods;
+		  num_mods++;
+		}
+	    }
+	}
+
+      gcc_assert (num_mods == num_chains + num_uses);
+      qsort (mods, num_mods, sizeof *mods, mod_before);
 
-  rtx pset = single_set (insn);
-  if (pset)
+      /* Now we have a list of all changes we have to make, sorted in
+	 ascending order so we can go through the basic block from
+	 start to end and keep track of the current state at all times.  */
+      if (rel_base_reg_user)
+	rel_base_reg_user->match_offset = 0;
+      for (i = 0; i < num_mods; i++)
     {
-      /* Can't use the size of SET_SRC, we might have something like
-	 (sign_extend:SI (mem:QI ...  */
-      rtx use = find_use_as_address (pset, reg, 0);
-      if (use != 0 && use != (rtx) (size_t) 1)
+	  struct rel_mod *this = mods + i;
+	  struct rel_use *use = this->chain->uses;
+	  HOST_WIDE_INT current_offset = this->chain->match_offset;
+	  rtx reg = this->chain->reg;
+
+	  /* Calling apply_change_group is deferred to this point from
+	     the call to validate_change in modify_address; the reason is
+	     that we want to group together multiple changes to the same insn,
+	     to avoid failures for match_dups.  */
+	  if (last_changed_insn
+	      && (this->insn != 0 || use->insn != last_changed_insn))
 	{
-	  int size = GET_MODE_SIZE (GET_MODE (use));
-	  if (0
-	      || (HAVE_POST_INCREMENT
-		  && pre == 0 && (inc_code = POST_INC, increment == size))
-	      || (HAVE_PRE_INCREMENT
-		  && pre == 1 && (inc_code = PRE_INC, increment == size))
-	      || (HAVE_POST_DECREMENT
-		  && pre == 0 && (inc_code = POST_DEC, increment == -size))
-	      || (HAVE_PRE_DECREMENT
-		  && pre == 1 && (inc_code = PRE_DEC, increment == -size))
-	  )
+	      last_changed_insn = 0;
+	      /* Don't use gcc_assert on the result of apply_change_group
+		 because that would prevent setting a breakpoint on the
+		 failure.  */
+	      if (! apply_change_group ())
+		gcc_assert (0);
+	    }
+
+	  if (this->insn != 0)
 	    {
-	      if (inc_insn_set)
-		validate_change
-		  (inc_insn,
-		   &SET_SRC (inc_insn_set),
-		   XEXP (SET_SRC (inc_insn_set), 0), 1);
-	      validate_change (insn, &XEXP (use, 0),
-			       gen_rtx_fmt_e (inc_code, Pmode, reg), 1);
-	      if (apply_change_group ())
+	      rtx new = perform_addition (this, use, rel_base->reg,
+					  rel_base_reg_user);
+	      if (this->from_base && new)
+		{
+		  /* If perform_addition emitted more than one insn, find
+		     the last one that actually used the base register.  */
+		  while (! reg_overlap_mentioned_p (rel_base->reg,
+						    PATTERN (new)))
+		    new = PREV_INSN (new);
+		  last_base_call_tally = use->call_tally;
+		  last_base_insn = new;
+		}
+	    }
+	  else
 		{
-		  /* If there is a REG_DEAD note on this insn, we must
-		     change this not to REG_UNUSED meaning that the register
-		     is set, but the value is dead.  Failure to do so will
-		     result in a sched1 dieing -- when it recomputes lifetime
-		     information, the number of REG_DEAD notes will have
-		     changed.  */
-		  rtx note = find_reg_note (insn, REG_DEAD, reg);
-		  if (note)
-		    PUT_MODE (note, REG_UNUSED);
+	      if (! use->no_link_pred)
+		modify_address (this, use, current_offset);
 
-		  REG_NOTES (insn)
-		    = gen_rtx_EXPR_LIST (REG_INC,
-					 reg, REG_NOTES (insn));
-		  if (! inc_insn_set)
-		    delete_insn (inc_insn);
-		  return 1;
+	      /* See if the register dies in this insn.  We cannot reliably
+		 detect this for the base register, which is handled later
+		 after all modifications are processed.  We can rely on the
+		 DEATH_NOTE field being 0 for the base register's chain.  */
+	      if (this->chain->death_note && this->chain->uses == 0)
+		{
+		  rtx note = this->chain->death_note;
+		  XEXP (note, 0) = reg;
+
+		  /* Note that passing only PATTERN (LAST_USE->insn) to
+		     reg_set_p here is not enough, since we might have
+		     created an REG_INC for REG above.  */
+
+		  PUT_MODE (note, (reg_set_p (reg, use->insn)
+				   ? REG_UNUSED : REG_DEAD));
+		  XEXP (note, 1) = REG_NOTES (use->insn);
+		  REG_NOTES (use->insn) = note;
+		  REG_N_DEATHS (REGNO (reg))++;
 		}
+
+	      if (REGNO (reg) == base_regno)
+		{
+		  last_base_call_tally = use->call_tally;
+		  last_base_insn = use->insn;
 	    }
+	      last_changed_insn = use->insn;
 	}
     }
-  return 0;
-}
 
-/* Determine if the pattern generated by add_optab has a clobber,
-   such as might be issued for a flags hard register.  To make the
-   code elsewhere simpler, we handle cc0 in this same framework.
+      if (last_changed_insn)
+	if (! apply_change_group ())
+	  gcc_assert (0);
 
-   Return the register if one was discovered.  Return NULL_RTX if
-   if no flags were found.  Return pc_rtx if we got confused.  */
+      /* We now have performed all modifications, and we therefore know the
+	 last insn that uses the base register.  This means we can now update
+	 its life information.  */
+      if (rel_base->death)
+	{
+	  rtx note = rel_base->death;
+	  XEXP (note, 0) = rel_base->reg;
 
-static rtx
-discover_flags_reg (void)
-{
-  rtx tmp;
-  tmp = gen_rtx_REG (word_mode, 10000);
-  tmp = gen_add3_insn (tmp, tmp, const2_rtx);
+	  /* Note that passing only PATTERN (LAST_USE->insn) to
+	     reg_set_p here is not enough, since we might have
+	     created an REG_INC for REG above.  */
 
-  /* If we get something that isn't a simple set, or a
-     [(set ..) (clobber ..)], this whole function will go wrong.  */
-  if (GET_CODE (tmp) == SET)
-    return NULL_RTX;
-  else if (GET_CODE (tmp) == PARALLEL)
+	  PUT_MODE (note, (reg_set_p (rel_base->reg, last_base_insn)
+			   ? REG_UNUSED : REG_DEAD));
+	  XEXP (note, 1) = REG_NOTES (last_base_insn);
+	  REG_NOTES (last_base_insn) = note;
+	  REG_N_DEATHS (base_regno)++;
+	}
+      else if (rel_base->invalidate_luid
+	       && ! reg_set_p (rel_base->reg, last_base_insn))
     {
-      int found;
-
-      if (XVECLEN (tmp, 0) != 2)
-	return pc_rtx;
-      tmp = XVECEXP (tmp, 0, 1);
-      if (GET_CODE (tmp) != CLOBBER)
-	return pc_rtx;
-      tmp = XEXP (tmp, 0);
+	  REG_NOTES (last_base_insn)
+	    = alloc_EXPR_LIST (REG_DEAD, rel_base->reg,
+			       REG_NOTES (last_base_insn));
+	  REG_N_DEATHS (base_regno)++;
+	}
 
-      /* Don't do anything foolish if the md wanted to clobber a
-	 scratch or something.  We only care about hard regs.
-	 Moreover we don't like the notion of subregs of hard regs.  */
-      if (GET_CODE (tmp) == SUBREG
-	  && REG_P (SUBREG_REG (tmp))
-	  && REGNO (SUBREG_REG (tmp)) < FIRST_PSEUDO_REGISTER)
-	return pc_rtx;
-      found = (REG_P (tmp) && REGNO (tmp) < FIRST_PSEUDO_REGISTER);
+      REG_N_CALLS_CROSSED (base_regno)
+	+= last_base_call_tally - rel_base->reg_set_call_tally;
+    }
+}
 
-      return (found ? tmp : NULL_RTX);
+/* Finalize the optimization for any related values know so far, and reset
+   the entries in regno_related that we have disturbed.  */
+static void
+optimize_related_values_0 (struct related *rel_base_list,
+			   int luid, int call_tally, rtx insert_before)
+{
+  while (rel_base_list)
+    {
+      struct related *rel;
+      optimize_related_values_1 (rel_base_list, luid, call_tally,
+				 insert_before);
+      /* Clear the entries that we used in regno_related.  We do it
+	 item by item here, because doing it with memset for each
+	 basic block would give O(n*n) time complexity.  */
+      for (rel = rel_base_list; rel; rel = rel->prev)
+	regno_related[REGNO (rel->reg)] = 0;
+      rel_base_list = rel_base_list->baseinfo->prev_base;
     }
 
-  return pc_rtx;
+  for ( ; unrelatedly_used; unrelatedly_used = unrelatedly_used->prev)
+    regno_related[REGNO (unrelatedly_used->reg)] = 0;
 }
 
-/* It is a tedious task identifying when the flags register is live and
-   when it is safe to optimize.  Since we process the instruction stream
-   multiple times, locate and record these live zones by marking the
-   mode of the instructions --
+/* For each integer mode, find minimum and maximum value for a single-
+   instruction reg-constant add.
+   The arm has SImode add patterns that will accept large values - with a
+   matching splitter - but when you use gen_addsi3, you already get
+   multiple instructions.  So getting one insn and testing if it can be
+   changed is not good enough; we need to try to generate each add from
+   scratch.  */
+static void
+init_add_limits (void)
+{
+  static int is_initialized;
 
-   QImode is used on the instruction at which the flags becomes live.
+  enum machine_mode mode;
 
-   HImode is used within the range (exclusive) that the flags are
-   live.  Thus the user of the flags is not marked.
+  if (is_initialized)
+    return;
 
-   All other instructions are cleared to VOIDmode.  */
+  for (mode = GET_CLASS_NARROWEST_MODE (MODE_INT); mode != VOIDmode;
+       mode = GET_MODE_WIDER_MODE (mode))
+    {
+      rtx reg = gen_rtx_REG (mode, LAST_VIRTUAL_REGISTER+1);
+      rtx reg2 = gen_rtx_REG (mode, LAST_VIRTUAL_REGISTER+2);
+      int icode = (int) add_optab->handlers[(int) mode].insn_code;
+      HOST_WIDE_INT tmp;
+      rtx add = NULL, set = NULL;
+      int p, p_max;
+      rtx tmp_add;
+      struct match match;
 
-/* Used to communicate with flags_set_1.  */
-static rtx flags_set_1_rtx;
-static int flags_set_1_set;
+      have_3addr_const_add[(int) mode] = 0;
+      add_limits[(int) mode][0] = 0;
+      add_limits[(int) mode][1] = 0;
 
-static void
-mark_flags_life_zones (rtx flags)
-{
-  int flags_regno;
-  int flags_nregs;
-  basic_block block;
+      if (icode == CODE_FOR_nothing
+	  || ! (*insn_data[icode].operand[0].predicate) (reg, mode)
+	  || ! (*insn_data[icode].operand[1].predicate) (reg, mode)
+	  || ! (*insn_data[icode].operand[2].predicate) (const1_rtx, mode))
+	continue;
 
-#ifdef HAVE_cc0
-  /* If we found a flags register on a cc0 host, bail.  */
-  if (flags == NULL_RTX)
-    flags = cc0_rtx;
-  else if (flags != cc0_rtx)
-    flags = pc_rtx;
-#endif
+      tmp_add = GEN_FCN (icode) (reg, reg2, const1_rtx);
+      if (tmp_add != NULL_RTX
+	  && !NEXT_INSN (tmp_add)
+	  && !find_matches (tmp_add, &match))
+	have_3addr_const_add[(int) mode] = 1;
 
-  /* Simple cases first: if no flags, clear all modes.  If confusing,
-     mark the entire function as being in a flags shadow.  */
-  if (flags == NULL_RTX || flags == pc_rtx)
-    {
-      enum machine_mode mode = (flags ? HImode : VOIDmode);
-      rtx insn;
-      for (insn = get_insns (); insn; insn = NEXT_INSN (insn))
-	PUT_MODE (insn, mode);
-      return;
-    }
+      p_max = GET_MODE_BITSIZE (mode) - 1;
 
-#ifdef HAVE_cc0
-  flags_regno = -1;
-  flags_nregs = 1;
-#else
-  flags_regno = REGNO (flags);
-  flags_nregs = hard_regno_nregs[flags_regno][GET_MODE (flags)];
-#endif
-  flags_set_1_rtx = flags;
+      if (p_max > HOST_BITS_PER_WIDE_INT - 2)
+	p_max = HOST_BITS_PER_WIDE_INT - 2;
 
-  /* Process each basic block.  */
-  FOR_EACH_BB_REVERSE (block)
+      for (p = 1; p < p_max; p++)
     {
-      rtx insn, end;
-      int live;
+	  rtx add_const = GEN_INT (((HOST_WIDE_INT) 1 << p) - 1);
 
-      insn = BB_HEAD (block);
-      end = BB_END (block);
+	  if (! (*insn_data[icode].operand[2].predicate) (add_const, mode))
+	    break;
 
-      /* Look out for the (unlikely) case of flags being live across
-	 basic block boundaries.  */
-      live = 0;
-#ifndef HAVE_cc0
+	  tmp_add = GEN_FCN (icode) (reg, reg, add_const);
+      
+	  if (tmp_add == NULL_RTX || NEXT_INSN (tmp_add))
+	    break;
+      
+	  set = single_set (tmp_add);
+      
+	  if (! set
+	      || GET_CODE (SET_SRC (set)) != PLUS
+	      || XEXP (SET_SRC (set), 1) != add_const)
+	    break;
+	  add = tmp_add;
+	}
+      
+      add_limits[(int) mode][1] = tmp = ((HOST_WIDE_INT) 1 << (p - 1)) - 1;
+      
+      /* We need a range of known good values for the constant of the add.
+	 Thus, before checking for the power of two, check for one less first,
+	 in case the power of two is an exceptional value.  */
+      if (add
+	  && validate_change (add, &XEXP (SET_SRC (set), 1), GEN_INT (-tmp), 0))
       {
-	int i;
-	for (i = 0; i < flags_nregs; ++i)
-	  live |= REGNO_REG_SET_P (block->il.rtl->global_live_at_start,
-				   flags_regno + i);
+	  if (validate_change (add, &XEXP (SET_SRC (set), 1),
+			       GEN_INT (-tmp - 1), 0))
+	    add_limits[(int) mode][0] = -tmp - 1;
+	  else
+	    add_limits[(int) mode][0] = -tmp;
       }
-#endif
+    }
+  is_initialized = 1;
+}
 
-      while (1)
+/* Scan the entire function for instances where multiple registers are
+   set to values that differ only by a constant.
+   Then try to reduce the number of instructions and/or registers needed
+   by exploiting auto_increment and true two-address additions.
+   NREGS and REGMOVE_DUMP_FILE are the same as in regmove_optimize.  */
+    
+static void
+optimize_related_values (int nregs)
+{
+  basic_block bb;
+  rtx insn;
+  int luid = 0;
+  int call_tally = 0;
+
+  if (dump_file)
+    fprintf (dump_file, "Starting optimize_related_values.\n");
+
+  init_add_limits ();
+  gcc_obstack_init (&related_obstack);
+  regno_related = rel_alloc (nregs * sizeof *regno_related);
+  memset ((char *) regno_related, 0, nregs * sizeof *regno_related);
+  rel_base_list = 0;
+
+  FOR_EACH_BB (bb)
+    FOR_BB_INSNS (bb, insn)
 	{
-	  /* Process liveness in reverse order of importance --
-	     alive, death, birth.  This lets more important info
-	     overwrite the mode of lesser info.  */
+	rtx set = NULL_RTX;
+
+	luid++;
+	
+	/* Don't do anything if this instruction is in the shadow of a
+	   live flags register.  */
+	if (GET_MODE (insn) == HImode)
+	  continue;
 
 	  if (INSN_P (insn))
 	    {
-#ifdef HAVE_cc0
-	      /* In the cc0 case, death is not marked in reg notes,
-		 but is instead the mere use of cc0 when it is alive.  */
-	      if (live && reg_mentioned_p (cc0_rtx, PATTERN (insn)))
-		live = 0;
-#else
-	      /* In the hard reg case, we watch death notes.  */
-	      if (live && find_regno_note (insn, REG_DEAD, flags_regno))
-		live = 0;
-#endif
-	      PUT_MODE (insn, (live ? HImode : VOIDmode));
+	    rtx note;
 
-	      /* In either case, birth is denoted simply by its presence
-		 as the destination of a set.  */
-	      flags_set_1_set = 0;
-	      note_stores (PATTERN (insn), flags_set_1, NULL);
-	      if (flags_set_1_set)
+	    set = single_set (insn);
+
+	    find_related_toplev (insn, luid, call_tally);
+
+	    for (note = REG_NOTES (insn); note; note = XEXP (note, 1))
 		{
-		  live = 1;
-		  PUT_MODE (insn, QImode);
+		if (REG_NOTE_KIND (note) == REG_DEAD
+		    || (REG_NOTE_KIND (note) == REG_UNUSED
+			&& REG_P (XEXP (note, 0))))
+		  {
+		    rtx reg = XEXP (note, 0);
+		    int regno = REGNO (reg);
+		    
+		    if (REG_NOTE_KIND (note) == REG_DEAD
+			&& reg_set_p (reg, PATTERN (insn)))
+		      {
+			remove_note (insn, note);
+			REG_N_DEATHS (regno)--;
 		}
+		    else if (regno_related[regno]
+			     && ! regno_related[regno]->invalidate_luid)
+		      {
+			regno_related[regno]->death = insn;
+			regno_related[regno]->reg_orig_calls_crossed
+			  = call_tally - regno_related[regno]->reg_set_call_tally;
 	    }
-	  else
-	    PUT_MODE (insn, (live ? HImode : VOIDmode));
-
-	  if (insn == end)
-	    break;
-	  insn = NEXT_INSN (insn);
 	}
     }
-}
 
-/* A subroutine of mark_flags_life_zones, called through note_stores.  */
+	    /* Inputs to a call insn do not cross the call, therefore CALL_TALLY
+	       must be bumped *after* they have been processed.  */
+	    if (CALL_P (insn))
+	      call_tally++;
+	  }
 
-static void
-flags_set_1 (rtx x, rtx pat, void *data ATTRIBUTE_UNUSED)
-{
-  if (GET_CODE (pat) == SET
-      && reg_overlap_mentioned_p (x, flags_set_1_rtx))
-    flags_set_1_set = 1;
+	/* We end current processing at the end of a basic block, or when
+	   a flags register becomes live, or when we see a return value
+	   copy.
+
+	   Otherwise, we might end up with one or more extra instructions
+	   inserted in front of the user, to set up or adjust a register. 
+	   There are cases where flag register uses could be handled smarter,
+	   but most of the time the user will be a branch anyways, so the
+	   extra effort to handle the occasional conditional instruction is
+	   probably not justified by the little possible extra gain.  */
+
+	if (insn == BB_END (bb)
+	    || GET_MODE (insn) == QImode
+	    || (set
+		&& REG_P (SET_DEST (set))
+		&& REG_FUNCTION_VALUE_P (SET_DEST (set))))
+	  {
+	    optimize_related_values_0 (rel_base_list, luid, call_tally, insn);
+	    rel_base_list = 0;
+	  }
+      }
+  obstack_free (&related_obstack, 0);
+  
+  if (dump_file)
+    fprintf (dump_file, "Finished optimize_related_values.\n");
 }
 
 static int *regno_src_regno;
@@ -982,7 +2929,8 @@
 			 "Fixed operand of insn %d.\n",
 			  INSN_UID (insn));
 
-#ifdef AUTO_INC_DEC
+	      if (AUTO_INC_DEC)
+		{
 	      for (p = PREV_INSN (insn); p; p = PREV_INSN (p))
 		{
 		  if (LABEL_P (p)
@@ -1010,7 +2958,7 @@
 		      break;
 		    }
 		}
-#endif
+		}
 	      return 1;
 	    }
 	}
@@ -1052,7 +3000,7 @@
 static void
 regmove_optimize (rtx f, int nregs)
 {
-  int old_max_uid = get_max_uid ();
+  int old_max_uid;
   rtx insn;
   struct match match;
   int pass;
@@ -1069,7 +3017,15 @@
      can suppress some optimizations in those zones.  */
   mark_flags_life_zones (discover_flags_reg ());
 
-  regno_src_regno = XNEWVEC (int, nregs);
+  /* See the comment in front of REL_USE_HASH_SIZE what
+     this is about.  */
+  if (AUTO_INC_DEC && flag_regmove && flag_optimize_related_values)
+    optimize_related_values (nregs);
+  /* That could have created new insns.  */
+  old_max_uid = get_max_uid ();
+
+  regno_src_regno = xmalloc (sizeof *regno_src_regno * nregs);
+
   for (i = nregs; --i >= 0; ) regno_src_regno[i] = -1;
 
   regmove_bb_head = XNEWVEC (int, old_max_uid + 1);
@@ -2013,6 +3969,8 @@
       /* Move the death note for SRC from INSN to P.  */
       if (! overlap)
 	remove_note (insn, src_note);
+      if (find_reg_note (p, REG_INC, XEXP (src_note, 0)))
+	PUT_MODE (src_note, REG_UNUSED);
       XEXP (src_note, 1) = REG_NOTES (p);
       REG_NOTES (p) = src_note;
 
@@ -2556,3 +4514,4 @@
   0                                     /* letter */
 };
 
+#include "gt-regmove.h"
Index: gcc/configure.ac
===================================================================
--- gcc/configure.ac	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/configure.ac	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1409,7 +1409,7 @@
     target_thread_file='single'
     ;;
   aix | dce | gnat | irix | posix | posix95 | rtems | \
-  single | solaris | vxworks | win32 )
+  single | solaris | vxworks | win32 | generic)
     target_thread_file=${enable_threads}
     ;;
   *)
@@ -1430,6 +1430,14 @@
   rm -f gthr-default.h
   echo "#include \"gthr-${thread_file}.h\"" > gthr-default.h
   gthread_flags=-DHAVE_GTHR_DEFAULT
+  if test $thread_file != posix; then
+    if test -f $srcdir/gthr-${thread_file}.c; then
+      extra_libgcc_srcs=$srcdir/gthr-${thread_file}.c
+    fi
+    if test -f $srcdir/gthr-objc-${thread_file}.c; then
+      extra_libgcc_srcs="${extra_libgcc_srcs} $srcdir/gthr-objc-${thread_file}.c"
+    fi
+  fi
 fi
 AC_SUBST(gthread_flags)
 
@@ -1510,6 +1518,7 @@
 	esac
 	saved_CFLAGS="${CFLAGS}"
 	CC="${CC_FOR_BUILD}" CFLAGS="${CFLAGS_FOR_BUILD}" \
+	CONFIG_SITE="" \
 	${realsrcdir}/configure \
 		--enable-languages=${enable_languages-all} \
 		--target=$target_alias --host=$build_alias --build=$build_alias
@@ -1768,6 +1777,16 @@
 	CROSS="-DCROSS_COMPILE"
 	ALL=all.cross
 	SYSTEM_HEADER_DIR=$build_system_header_dir
+
+	# For builds with an in-tree newlib, then the headers are not
+	# copied to build_system_header_dir, so things like limits.h
+	# won't work unless we point at the real headers.
+	if test "$with_newlib" = yes \
+		&& (test -z "$with_headers" || test "$with_headers" = yes) \
+		&& test -d $srcdir/../newlib/libc/include; then
+	  SYSTEM_HEADER_DIR="\$(abs_srcdir)/../newlib/libc/include"
+	fi
+
 	case "$host","$target" in
 	# Darwin crosses can use the host system's libraries and headers,
 	# because of the fat library support.  Of course, it must be the
@@ -2178,7 +2197,7 @@
   as_ver=`$gcc_cv_as --version 2>/dev/null | sed 1q`
   if echo "$as_ver" | grep GNU > /dev/null; then
 changequote(,)dnl
-    as_ver=`echo $as_ver | sed -e 's/GNU assembler \([0-9.][0-9.]*\).*/\1/'`
+    as_ver=`echo $as_ver | sed -e 's/GNU assembler [^0-9.]*\([0-9.][0-9.]*\).*/\1/'`
     as_major=`echo $as_ver | sed 's/\..*//'`
     as_minor=`echo $as_ver | sed 's/[^.]*\.\([0-9]*\).*/\1/'`
 changequote([,])dnl
@@ -3624,6 +3643,7 @@
 AC_SUBST(tmake_file)
 AC_SUBST(extra_gcc_objs)
 AC_SUBST(extra_headers_list)
+AC_SUBST(extra_libgcc_srcs)
 AC_SUBST(extra_objs)
 AC_SUBST(extra_parts)
 AC_SUBST(extra_passes)
Index: gcc/function.c
===================================================================
--- gcc/function.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/function.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2723,7 +2723,7 @@
 
       /* TREE_USED gets set erroneously during expand_assignment.  */
       save_tree_used = TREE_USED (parm);
-      expand_assignment (parm, make_tree (data->nominal_type, tempreg));
+      expand_assignment (parm, make_tree (data->nominal_type, tempreg), false);
       TREE_USED (parm) = save_tree_used;
       all->conversion_insns = get_insns ();
       end_sequence ();
Index: gcc/cppdefault.c
===================================================================
--- gcc/cppdefault.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/cppdefault.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -96,3 +96,31 @@
 const char cpp_GCC_INCLUDE_DIR[] = "";
 const size_t cpp_GCC_INCLUDE_DIR_len = 0;
 #endif
+
+/* The configured standard exec prefix */
+const char cpp_STANDARD_EXEC_PREFIX[] = STANDARD_EXEC_PREFIX;
+const size_t cpp_STANDARD_EXEC_PREFIX_len = sizeof STANDARD_EXEC_PREFIX - 1;
+
+/* This value is set by cpp_relocated at runtime */
+const char *gcc_exec_prefix;
+
+/* Return true if the toolchain is relocated.  */
+bool
+cpp_relocated (void)
+{
+  static int relocated = -1;
+
+  /* A relocated toolchain ignores standard include directories.  */
+  if (relocated == -1)
+    {
+      /* Check if the toolchain was relocated?  */
+      GET_ENVIRONMENT (gcc_exec_prefix, "GCC_EXEC_PREFIX");
+      if (gcc_exec_prefix)
+       relocated = 1;
+      else
+       relocated = 0;
+    }
+
+  return relocated;
+}
+
Index: gcc/cppdefault.h
===================================================================
--- gcc/cppdefault.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/cppdefault.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -52,4 +52,11 @@
 extern const char cpp_GCC_INCLUDE_DIR[];
 extern const size_t cpp_GCC_INCLUDE_DIR_len;
 
+extern const char cpp_STANDARD_EXEC_PREFIX[];
+extern const size_t cpp_STANDARD_EXEC_PREFIX_len;
+extern const char *gcc_exec_prefix;
+
+/* Return true if the toolchain is relocated.  */
+bool cpp_relocated (void);
+
 #endif /* ! GCC_CPPDEFAULT_H */
Index: gcc/stor-layout.c
===================================================================
--- gcc/stor-layout.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/stor-layout.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1234,14 +1234,6 @@
   rli->offset_align = BITS_PER_UNIT;
   normalize_rli (rli);
 
-  /* Determine the desired alignment.  */
-#ifdef ROUND_TYPE_ALIGN
-  TYPE_ALIGN (rli->t) = ROUND_TYPE_ALIGN (rli->t, TYPE_ALIGN (rli->t),
-					  rli->record_align);
-#else
-  TYPE_ALIGN (rli->t) = MAX (TYPE_ALIGN (rli->t), rli->record_align);
-#endif
-
   /* Compute the size so far.  Be sure to allow for extra bits in the
      size in bytes.  We have guaranteed above that it will be no more
      than a single byte.  */
@@ -1251,6 +1243,17 @@
     unpadded_size_unit
       = size_binop (PLUS_EXPR, unpadded_size_unit, size_one_node);
 
+      
+  /* Determine the desired alignment.  */
+#ifdef ROUND_TYPE_ALIGN
+  TYPE_SIZE (rli->t) = unpadded_size;
+  TYPE_ALIGN (rli->t) = ROUND_TYPE_ALIGN (rli->t, TYPE_ALIGN (rli->t),
+					  rli->record_align);
+
+#else
+  TYPE_ALIGN (rli->t) = MAX (TYPE_ALIGN (rli->t), rli->record_align);
+#endif
+
   /* Round the size up to be a multiple of the required alignment.  */
   TYPE_SIZE (rli->t) = round_up (unpadded_size, TYPE_ALIGN (rli->t));
   TYPE_SIZE_UNIT (rli->t)
Index: gcc/tree-data-ref.c
===================================================================
--- gcc/tree-data-ref.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/tree-data-ref.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2405,7 +2405,7 @@
 static tree
 get_number_of_iters_for_loop (int loopnum)
 {
-  tree numiter = number_of_iterations_in_loop (current_loops->parray[loopnum]);
+  tree numiter = number_of_exit_cond_executions (current_loops->parray[loopnum]);
 
   if (TREE_CODE (numiter) != INTEGER_CST)
     numiter = current_loops->parray[loopnum]->estimated_nb_iterations;
Index: gcc/tree-vect-analyze.c
===================================================================
--- gcc/tree-vect-analyze.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/tree-vect-analyze.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1877,7 +1877,7 @@
   if (vect_print_dump_info (REPORT_DETAILS))
     fprintf (vect_dump, "=== get_loop_niters ===");
 
-  niters = number_of_iterations_in_loop (loop);
+  niters = number_of_exit_cond_executions (loop);
 
   if (niters != NULL_TREE
       && niters != chrec_dont_know)
Index: gcc/c-typeck.c
===================================================================
--- gcc/c-typeck.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/c-typeck.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1453,6 +1453,7 @@
       && !TREE_THIS_VOLATILE (decl)
       && TREE_READONLY (decl)
       && DECL_INITIAL (decl) != 0
+      && !DECL_WEAK (decl)
       && TREE_CODE (DECL_INITIAL (decl)) != ERROR_MARK
       /* This is invalid if initial value is not constant.
 	 If it has either a function call, a memory reference,
Index: gcc/calls.c
===================================================================
--- gcc/calls.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/calls.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1047,7 +1047,7 @@
 	      else
 		copy = assign_temp (type, 0, 1, 0);
 
-	      store_expr (args[i].tree_value, copy, 0);
+	      store_expr (args[i].tree_value, copy, false, false);
 
 	      if (callee_copies)
 		*ecf_flags &= ~(ECF_CONST | ECF_LIBCALL_BLOCK);
Index: gcc/ggc-common.c
===================================================================
--- gcc/ggc-common.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/ggc-common.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -47,19 +47,6 @@
 # define MAP_FAILED ((void *)-1)
 #endif
 
-#ifdef ENABLE_VALGRIND_CHECKING
-# ifdef HAVE_VALGRIND_MEMCHECK_H
-#  include <valgrind/memcheck.h>
-# elif defined HAVE_MEMCHECK_H
-#  include <memcheck.h>
-# else
-#  include <valgrind.h>
-# endif
-#else
-/* Avoid #ifdef:s when we can help it.  */
-#define VALGRIND_DISCARD(x)
-#endif
-
 /* When set, ggc_collect will do collection.  */
 bool ggc_force_collect;
 
@@ -163,9 +150,9 @@
 	 old_size as reachable, but that would lose tracking of writes
 	 after the end of the object (by small offsets).  Discard the
 	 handle to avoid handle leak.  */
-      VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS ((char *) x + size,
+      VALGRIND_DISCARD (VALGRIND_MAKE_MEM_NOACCESS ((char *) x + size,
 						old_size - size));
-      VALGRIND_DISCARD (VALGRIND_MAKE_READABLE (x, size));
+      VALGRIND_DISCARD (VALGRIND_MAKE_MEM_DEFINED (x, size));
       return x;
     }
 
@@ -175,7 +162,7 @@
      individually allocated object, we'd access parts of the old object
      that were marked invalid with the memcpy below.  We lose a bit of the
      initialization-tracking since some of it may be uninitialized.  */
-  VALGRIND_DISCARD (VALGRIND_MAKE_READABLE (x, old_size));
+  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_DEFINED (x, old_size));
 
   memcpy (r, x, old_size);
 
Index: gcc/except.c
===================================================================
--- gcc/except.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/except.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -3,6 +3,7 @@
    1999, 2000, 2001, 2002, 2003, 2004, 2005, 2007
    Free Software Foundation, Inc.
    Contributed by Mike Stump <mrs@cygnus.com>.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -84,7 +85,6 @@
 #define EH_RETURN_DATA_REGNO(N) INVALID_REGNUM
 #endif
 
-
 /* Protect cleanup actions with must-not-throw regions, with a call
    to the given failure handler.  */
 tree (*lang_protect_cleanup_actions) (void);
@@ -311,10 +311,10 @@
 
 static void push_uleb128 (varray_type *, unsigned int);
 static void push_sleb128 (varray_type *, int);
-#ifndef HAVE_AS_LEB128
+
 static int dw2_size_of_call_site_table (void);
 static int sjlj_size_of_call_site_table (void);
-#endif
+
 static void dw2_output_call_site_table (void);
 static void sjlj_output_call_site_table (void);
 
@@ -3416,7 +3416,6 @@
 }
 
 
-#ifndef HAVE_AS_LEB128
 static int
 dw2_size_of_call_site_table (void)
 {
@@ -3449,7 +3448,6 @@
 
   return size;
 }
-#endif
 
 static void
 dw2_output_call_site_table (void)
@@ -3475,7 +3473,8 @@
 	 generic arithmetic.  */
       /* ??? Perhaps use attr_length to choose data1 or data2 instead of
 	 data4 if the function is small enough.  */
-#ifdef HAVE_AS_LEB128
+      if (TARGET_USES_LEB128)
+	{
       dw2_asm_output_delta_uleb128 (reg_start_lab,
 				    current_function_func_begin_label,
 				    "region %d start", i);
@@ -3487,7 +3486,9 @@
 				      "landing pad");
       else
 	dw2_asm_output_data_uleb128 (0, "landing pad");
-#else
+	}
+      else 
+	{
       dw2_asm_output_delta (4, reg_start_lab,
 			    current_function_func_begin_label,
 			    "region %d start", i);
@@ -3498,7 +3499,7 @@
 			      "landing pad");
       else
 	dw2_asm_output_data (4, 0, "landing pad");
-#endif
+	}
       dw2_asm_output_data_uleb128 (cs->action, "action");
     }
 
@@ -3609,13 +3610,10 @@
 output_function_exception_table (void)
 {
   int tt_format, cs_format, lp_format, i, n;
-#ifdef HAVE_AS_LEB128
   char ttype_label[32];
   char cs_after_size_label[32];
   char cs_end_label[32];
-#else
   int call_site_len;
-#endif
   int have_tt_data;
   int tt_format_size = 0;
 
@@ -3649,10 +3647,11 @@
   else
     {
       tt_format = ASM_PREFERRED_EH_DATA_FORMAT (/*code=*/0, /*global=*/1);
-#ifdef HAVE_AS_LEB128
+      if (TARGET_USES_LEB128)
+	{
       ASM_GENERATE_INTERNAL_LABEL (ttype_label, "LLSDATT",
 				   current_function_funcdef_no);
-#endif
+	}
       tt_format_size = size_of_encoded_value (tt_format);
 
       assemble_align (tt_format_size * BITS_PER_UNIT);
@@ -3678,24 +3677,29 @@
   dw2_asm_output_data (1, tt_format, "@TType format (%s)",
 		       eh_data_format_name (tt_format));
 
-#ifndef HAVE_AS_LEB128
+
+  if (! TARGET_USES_LEB128)
+    {
   if (USING_SJLJ_EXCEPTIONS)
     call_site_len = sjlj_size_of_call_site_table ();
   else
     call_site_len = dw2_size_of_call_site_table ();
-#endif
+    }
 
   /* A pc-relative 4-byte displacement to the @TType data.  */
   if (have_tt_data)
     {
-#ifdef HAVE_AS_LEB128
+      if (TARGET_USES_LEB128)
+	{
       char ttype_after_disp_label[32];
       ASM_GENERATE_INTERNAL_LABEL (ttype_after_disp_label, "LLSDATTD",
 				   current_function_funcdef_no);
       dw2_asm_output_delta_uleb128 (ttype_label, ttype_after_disp_label,
 				    "@TType base offset");
       ASM_OUTPUT_LABEL (asm_out_file, ttype_after_disp_label);
-#else
+	}
+      else
+	{
       /* Ug.  Alignment queers things.  */
       unsigned int before_disp, after_disp, last_disp, disp;
 
@@ -3723,19 +3727,20 @@
       while (disp != last_disp);
 
       dw2_asm_output_data_uleb128 (disp, "@TType base offset");
-#endif
+	}
     }
 
   /* Indicate the format of the call-site offsets.  */
-#ifdef HAVE_AS_LEB128
+  if (TARGET_USES_LEB128)
   cs_format = DW_EH_PE_uleb128;
-#else
+  else
   cs_format = DW_EH_PE_udata4;
-#endif
+
   dw2_asm_output_data (1, cs_format, "call-site format (%s)",
 		       eh_data_format_name (cs_format));
 
-#ifdef HAVE_AS_LEB128
+  if (TARGET_USES_LEB128)
+    {
   ASM_GENERATE_INTERNAL_LABEL (cs_after_size_label, "LLSDACSB",
 			       current_function_funcdef_no);
   ASM_GENERATE_INTERNAL_LABEL (cs_end_label, "LLSDACSE",
@@ -3748,13 +3753,15 @@
   else
     dw2_output_call_site_table ();
   ASM_OUTPUT_LABEL (asm_out_file, cs_end_label);
-#else
+    }
+  else
+    {
   dw2_asm_output_data_uleb128 (call_site_len,"Call-site table length");
   if (USING_SJLJ_EXCEPTIONS)
     sjlj_output_call_site_table ();
   else
     dw2_output_call_site_table ();
-#endif
+    }
 
   /* ??? Decode and interpret the data for flag_debug_asm.  */
   n = VARRAY_ACTIVE_SIZE (cfun->eh->action_record_data);
@@ -3772,10 +3779,8 @@
       output_ttype (type, tt_format, tt_format_size);
     }
 
-#ifdef HAVE_AS_LEB128
-  if (have_tt_data)
+  if (TARGET_USES_LEB128 && have_tt_data)
       ASM_OUTPUT_LABEL (asm_out_file, ttype_label);
-#endif
 
   /* ??? Decode and interpret the data for flag_debug_asm.  */
   n = VARRAY_ACTIVE_SIZE (cfun->eh->ehspec_data);
Index: gcc/emit-rtl.c
===================================================================
--- gcc/emit-rtl.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/emit-rtl.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2,6 +2,7 @@
    Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007
    Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -3917,8 +3918,17 @@
   PREV_INSN (first) = after;
   NEXT_INSN (last) = after_after;
   if (after_after)
+    {
     PREV_INSN (after_after) = last;
 
+      if (NONJUMP_INSN_P (after_after)
+	  && GET_CODE (PATTERN (after_after)) == SEQUENCE)
+	{
+	  rtx sequence = PATTERN (after_after);
+	  PREV_INSN (XVECEXP (sequence, 0, 0)) = last;
+	}
+    }
+
   if (after == last_insn)
     last_insn = last;
   return last;
Index: gcc/c-opts.c
===================================================================
--- gcc/c-opts.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/c-opts.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2,6 +2,7 @@
    Copyright (C) 2002, 2003, 2004, 2005, 2006, 2007, 2008
    Free Software Foundation, Inc.
    Contributed by Neil Booth.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -285,6 +286,7 @@
       break;
 
     case OPT__output_pch_:
+      CYGPATH (arg);
       pch_file = arg;
       break;
 
@@ -344,11 +346,13 @@
     case OPT_MD:
     case OPT_MMD:
       cpp_opts->deps.style = (code == OPT_MD ? DEPS_SYSTEM: DEPS_USER);
+      CYGPATH (arg);
       deps_file = arg;
       break;
 
     case OPT_MF:
       deps_seen = true;
+      CYGPATH (arg);
       deps_file = arg;
       break;
 
@@ -822,6 +826,7 @@
 
     case OPT_imacros:
     case OPT_include:
+      CYGPATH (arg);
       defer_opt (code, arg);
       break;
 
@@ -830,6 +835,7 @@
       break;
 
     case OPT_iprefix:
+      CYGPATH (arg);
       iprefix = arg;
       break;
 
@@ -838,6 +844,7 @@
       break;
 
     case OPT_isysroot:
+      CYGPATH (arg);
       sysroot = arg;
       break;
 
@@ -876,7 +883,10 @@
 
     case OPT_o:
       if (!out_fname)
+	{
+	  CYGPATH (arg);
 	out_fname = arg;
+	}
       else
 	error ("output filename specified twice");
       break;
@@ -1273,7 +1283,10 @@
 
       /* Command line -MF overrides environment variables and default.  */
       if (!deps_file)
+	{
+	  CYGPATH (spec);
 	deps_file = spec;
+	}
 
       deps_append = 1;
       deps_seen = true;
Index: gcc/cfgcleanup.c
===================================================================
--- gcc/cfgcleanup.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/cfgcleanup.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,6 +1,7 @@
 /* Control flow optimization code for GNU compiler.
    Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005, 2007 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -57,11 +58,6 @@
 
 /* Set to true when we are running first pass of try_optimize_cfg loop.  */
 static bool first_pass;
-static bool try_crossjump_to_edge (int, edge, edge);
-static bool try_crossjump_bb (int, basic_block);
-static bool outgoing_edges_match (int, basic_block, basic_block);
-static int flow_find_cross_jump (int, basic_block, basic_block, rtx *, rtx *);
-static bool old_insns_match_p (int, rtx, rtx);
 
 static void merge_blocks_move_predecessor_nojumps (basic_block, basic_block);
 static void merge_blocks_move_successor_nojumps (basic_block, basic_block);
@@ -73,7 +69,6 @@
 static void notice_new_block (basic_block);
 static void update_forwarder_flag (basic_block);
 static int mentions_nonequal_regs (rtx *, void *);
-static void merge_memattrs (rtx, rtx);
 
 /* Set flags for newly created block.  */
 
@@ -847,97 +842,6 @@
 }
 
 
-/* Removes the memory attributes of MEM expression
-   if they are not equal.  */
-
-void
-merge_memattrs (rtx x, rtx y)
-{
-  int i;
-  int j;
-  enum rtx_code code;
-  const char *fmt;
-
-  if (x == y)
-    return;
-  if (x == 0 || y == 0)
-    return;
-
-  code = GET_CODE (x);
-
-  if (code != GET_CODE (y))
-    return;
-
-  if (GET_MODE (x) != GET_MODE (y))
-    return;
-
-  if (code == MEM && MEM_ATTRS (x) != MEM_ATTRS (y))
-    {
-      if (! MEM_ATTRS (x))
-	MEM_ATTRS (y) = 0;
-      else if (! MEM_ATTRS (y))
-	MEM_ATTRS (x) = 0;
-      else
-	{
-	  rtx mem_size;
-
-	  if (MEM_ALIAS_SET (x) != MEM_ALIAS_SET (y))
-	    {
-	      set_mem_alias_set (x, 0);
-	      set_mem_alias_set (y, 0);
-	    }
-
-	  if (! mem_expr_equal_p (MEM_EXPR (x), MEM_EXPR (y)))
-	    {
-	      set_mem_expr (x, 0);
-	      set_mem_expr (y, 0);
-	      set_mem_offset (x, 0);
-	      set_mem_offset (y, 0);
-	    }
-	  else if (MEM_OFFSET (x) != MEM_OFFSET (y))
-	    {
-	      set_mem_offset (x, 0);
-	      set_mem_offset (y, 0);
-	    }
-
-	  if (!MEM_SIZE (x))
-	    mem_size = NULL_RTX;
-	  else if (!MEM_SIZE (y))
-	    mem_size = NULL_RTX;
-	  else
-	    mem_size = GEN_INT (MAX (INTVAL (MEM_SIZE (x)),
-				     INTVAL (MEM_SIZE (y))));
-	  set_mem_size (x, mem_size);
-	  set_mem_size (y, mem_size);
-
-	  set_mem_align (x, MIN (MEM_ALIGN (x), MEM_ALIGN (y)));
-	  set_mem_align (y, MEM_ALIGN (x));
-	}
-    }
-
-  fmt = GET_RTX_FORMAT (code);
-  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
-    {
-      switch (fmt[i])
-	{
-	case 'E':
-	  /* Two vectors must have the same length.  */
-	  if (XVECLEN (x, i) != XVECLEN (y, i))
-	    return;
-
-	  for (j = 0; j < XVECLEN (x, i); j++)
-	    merge_memattrs (XVECEXP (x, i, j), XVECEXP (y, i, j));
-
-	  break;
-
-	case 'e':
-	  merge_memattrs (XEXP (x, i), XEXP (y, i));
-	}
-    }
-  return;
-}
-
-
 /* Return true if I1 and I2 are equivalent and thus can be crossjumped.  */
 
 static bool
@@ -1046,119 +950,6 @@
   return false;
 }
 
-/* Look through the insns at the end of BB1 and BB2 and find the longest
-   sequence that are equivalent.  Store the first insns for that sequence
-   in *F1 and *F2 and return the sequence length.
-
-   To simplify callers of this function, if the blocks match exactly,
-   store the head of the blocks in *F1 and *F2.  */
-
-static int
-flow_find_cross_jump (int mode ATTRIBUTE_UNUSED, basic_block bb1,
-		      basic_block bb2, rtx *f1, rtx *f2)
-{
-  rtx i1, i2, last1, last2, afterlast1, afterlast2;
-  int ninsns = 0;
-
-  /* Skip simple jumps at the end of the blocks.  Complex jumps still
-     need to be compared for equivalence, which we'll do below.  */
-
-  i1 = BB_END (bb1);
-  last1 = afterlast1 = last2 = afterlast2 = NULL_RTX;
-  if (onlyjump_p (i1)
-      || (returnjump_p (i1) && !side_effects_p (PATTERN (i1))))
-    {
-      last1 = i1;
-      i1 = PREV_INSN (i1);
-    }
-
-  i2 = BB_END (bb2);
-  if (onlyjump_p (i2)
-      || (returnjump_p (i2) && !side_effects_p (PATTERN (i2))))
-    {
-      last2 = i2;
-      /* Count everything except for unconditional jump as insn.  */
-      if (!simplejump_p (i2) && !returnjump_p (i2) && last1)
-	ninsns++;
-      i2 = PREV_INSN (i2);
-    }
-
-  while (true)
-    {
-      /* Ignore notes.  */
-      while (!INSN_P (i1) && i1 != BB_HEAD (bb1))
-	i1 = PREV_INSN (i1);
-
-      while (!INSN_P (i2) && i2 != BB_HEAD (bb2))
-	i2 = PREV_INSN (i2);
-
-      if (i1 == BB_HEAD (bb1) || i2 == BB_HEAD (bb2))
-	break;
-
-      if (!old_insns_match_p (mode, i1, i2))
-	break;
-
-      merge_memattrs (i1, i2);
-
-      /* Don't begin a cross-jump with a NOTE insn.  */
-      if (INSN_P (i1))
-	{
-	  /* If the merged insns have different REG_EQUAL notes, then
-	     remove them.  */
-	  rtx equiv1 = find_reg_equal_equiv_note (i1);
-	  rtx equiv2 = find_reg_equal_equiv_note (i2);
-
-	  if (equiv1 && !equiv2)
-	    remove_note (i1, equiv1);
-	  else if (!equiv1 && equiv2)
-	    remove_note (i2, equiv2);
-	  else if (equiv1 && equiv2
-		   && !rtx_equal_p (XEXP (equiv1, 0), XEXP (equiv2, 0)))
-	    {
-	      remove_note (i1, equiv1);
-	      remove_note (i2, equiv2);
-	    }
-
-	  afterlast1 = last1, afterlast2 = last2;
-	  last1 = i1, last2 = i2;
-	  ninsns++;
-	}
-
-      i1 = PREV_INSN (i1);
-      i2 = PREV_INSN (i2);
-    }
-
-#ifdef HAVE_cc0
-  /* Don't allow the insn after a compare to be shared by
-     cross-jumping unless the compare is also shared.  */
-  if (ninsns && reg_mentioned_p (cc0_rtx, last1) && ! sets_cc0_p (last1))
-    last1 = afterlast1, last2 = afterlast2, ninsns--;
-#endif
-
-  /* Include preceding notes and labels in the cross-jump.  One,
-     this may bring us to the head of the blocks as requested above.
-     Two, it keeps line number notes as matched as may be.  */
-  if (ninsns)
-    {
-      while (last1 != BB_HEAD (bb1) && !INSN_P (PREV_INSN (last1)))
-	last1 = PREV_INSN (last1);
-
-      if (last1 != BB_HEAD (bb1) && LABEL_P (PREV_INSN (last1)))
-	last1 = PREV_INSN (last1);
-
-      while (last2 != BB_HEAD (bb2) && !INSN_P (PREV_INSN (last2)))
-	last2 = PREV_INSN (last2);
-
-      if (last2 != BB_HEAD (bb2) && LABEL_P (PREV_INSN (last2)))
-	last2 = PREV_INSN (last2);
-
-      *f1 = last1;
-      *f2 = last2;
-    }
-
-  return ninsns;
-}
-
 /* Return true iff the condbranches at the end of BB1 and BB2 match.  */
 bool
 condjump_equiv_p (struct equiv_info *info, bool call_init)
@@ -1217,8 +1008,8 @@
   if (code2 == UNKNOWN)
     return false;
 
-  if (call_init && !struct_equiv_init (STRUCT_EQUIV_START | info->mode, info))
-    gcc_unreachable ();
+  if (call_init)
+    struct_equiv_init (STRUCT_EQUIV_START | info->mode, info, false);
   /* Make the sources of the pc sets unreadable so that when we call
      insns_match_p it won't process them.
      The death_notes_match_p from insns_match_p won't see the local registers
@@ -1287,15 +1078,20 @@
   return match;
 }
 
-/* Return true iff outgoing edges of BB1 and BB2 match, together with
-   the branch instruction.  This means that if we commonize the control
-   flow before end of the basic block, the semantic remains unchanged.
+/* Return true iff outgoing edges of INFO->y_block and INFO->x_block match,
+   together with the branch instruction.  This means that if we commonize the
+   control flow before end of the basic block, the semantic remains unchanged.
+   If we need to compare jumps, we set STRUCT_EQUIV_MATCH_JUMPS in *MODE,
+   and pass *MODE to struct_equiv_init or assign it to INFO->mode, as
+   appropriate.
 
    We may assume that there exists one edge with a common destination.  */
 
 static bool
-outgoing_edges_match (int mode, basic_block bb1, basic_block bb2)
+outgoing_edges_match (int *mode, struct equiv_info *info)
 {
+  basic_block bb1 = info->y_block;
+  basic_block bb2 = info->x_block;
   int nehedges1 = 0, nehedges2 = 0;
   edge fallthru1 = 0, fallthru2 = 0;
   edge e1, e2;
@@ -1311,114 +1107,19 @@
 		& (EDGE_COMPLEX | EDGE_FAKE)) == 0
 	    && (!JUMP_P (BB_END (bb2)) || simplejump_p (BB_END (bb2))));
 
+  *mode |= STRUCT_EQUIV_MATCH_JUMPS;
   /* Match conditional jumps - this may get tricky when fallthru and branch
      edges are crossed.  */
   if (EDGE_COUNT (bb1->succs) == 2
       && any_condjump_p (BB_END (bb1))
       && onlyjump_p (BB_END (bb1)))
     {
-      edge b1, f1, b2, f2;
-      bool reverse, match;
-      rtx set1, set2, cond1, cond2;
-      enum rtx_code code1, code2;
-
       if (EDGE_COUNT (bb2->succs) != 2
 	  || !any_condjump_p (BB_END (bb2))
 	  || !onlyjump_p (BB_END (bb2)))
 	return false;
-
-      b1 = BRANCH_EDGE (bb1);
-      b2 = BRANCH_EDGE (bb2);
-      f1 = FALLTHRU_EDGE (bb1);
-      f2 = FALLTHRU_EDGE (bb2);
-
-      /* Get around possible forwarders on fallthru edges.  Other cases
-	 should be optimized out already.  */
-      if (FORWARDER_BLOCK_P (f1->dest))
-	f1 = single_succ_edge (f1->dest);
-
-      if (FORWARDER_BLOCK_P (f2->dest))
-	f2 = single_succ_edge (f2->dest);
-
-      /* To simplify use of this function, return false if there are
-	 unneeded forwarder blocks.  These will get eliminated later
-	 during cleanup_cfg.  */
-      if (FORWARDER_BLOCK_P (f1->dest)
-	  || FORWARDER_BLOCK_P (f2->dest)
-	  || FORWARDER_BLOCK_P (b1->dest)
-	  || FORWARDER_BLOCK_P (b2->dest))
-	return false;
-
-      if (f1->dest == f2->dest && b1->dest == b2->dest)
-	reverse = false;
-      else if (f1->dest == b2->dest && b1->dest == f2->dest)
-	reverse = true;
-      else
-	return false;
-
-      set1 = pc_set (BB_END (bb1));
-      set2 = pc_set (BB_END (bb2));
-      if ((XEXP (SET_SRC (set1), 1) == pc_rtx)
-	  != (XEXP (SET_SRC (set2), 1) == pc_rtx))
-	reverse = !reverse;
-
-      cond1 = XEXP (SET_SRC (set1), 0);
-      cond2 = XEXP (SET_SRC (set2), 0);
-      code1 = GET_CODE (cond1);
-      if (reverse)
-	code2 = reversed_comparison_code (cond2, BB_END (bb2));
-      else
-	code2 = GET_CODE (cond2);
-
-      if (code2 == UNKNOWN)
-	return false;
-
-      /* Verify codes and operands match.  */
-      match = ((code1 == code2
-		&& rtx_renumbered_equal_p (XEXP (cond1, 0), XEXP (cond2, 0))
-		&& rtx_renumbered_equal_p (XEXP (cond1, 1), XEXP (cond2, 1)))
-	       || (code1 == swap_condition (code2)
-		   && rtx_renumbered_equal_p (XEXP (cond1, 1),
-					      XEXP (cond2, 0))
-		   && rtx_renumbered_equal_p (XEXP (cond1, 0),
-					      XEXP (cond2, 1))));
-
-      /* If we return true, we will join the blocks.  Which means that
-	 we will only have one branch prediction bit to work with.  Thus
-	 we require the existing branches to have probabilities that are
-	 roughly similar.  */
-      if (match
-	  && !optimize_size
-	  && maybe_hot_bb_p (bb1)
-	  && maybe_hot_bb_p (bb2))
-	{
-	  int prob2;
-
-	  if (b1->dest == b2->dest)
-	    prob2 = b2->probability;
-	  else
-	    /* Do not use f2 probability as f2 may be forwarded.  */
-	    prob2 = REG_BR_PROB_BASE - b2->probability;
-
-	  /* Fail if the difference in probabilities is greater than 50%.
-	     This rules out two well-predicted branches with opposite
-	     outcomes.  */
-	  if (abs (b1->probability - prob2) > REG_BR_PROB_BASE / 2)
-	    {
-	      if (dump_file)
-		fprintf (dump_file,
-			 "Outcomes of branch in bb %i and %i differ too much (%i %i)\n",
-			 bb1->index, bb2->index, b1->probability, prob2);
-
-	      return false;
-	    }
-	}
-
-      if (dump_file && match)
-	fprintf (dump_file, "Conditionals in bb %i and %i match.\n",
-		 bb1->index, bb2->index);
-
-      return match;
+      info->mode = *mode;
+      return condjump_equiv_p (info, true);
     }
 
   /* Generic case - we are seeing a computed jump, table jump or trapping
@@ -1466,31 +1167,22 @@
 		      identical = false;
 		}
 
-	      if (identical)
+	      if (identical
+		  && struct_equiv_init (STRUCT_EQUIV_START | *mode, info, true))
 		{
-		  replace_label_data rr;
 		  bool match;
 
-		  /* Temporarily replace references to LABEL1 with LABEL2
+		  /* Indicate that LABEL1 is to be replaced with LABEL2
 		     in BB1->END so that we could compare the instructions.  */
-		  rr.r1 = label1;
-		  rr.r2 = label2;
-		  rr.update_label_nuses = false;
-		  for_each_rtx (&BB_END (bb1), replace_label, &rr);
+		  info->y_label = label1;
+		  info->x_label = label2;
 
-		  match = old_insns_match_p (mode, BB_END (bb1), BB_END (bb2));
+		  match = insns_match_p (BB_END (bb1), BB_END (bb2), info);
 		  if (dump_file && match)
 		    fprintf (dump_file,
 			     "Tablejumps in bb %i and %i match.\n",
 			     bb1->index, bb2->index);
 
-		  /* Set the original label in BB1->END because when deleting
-		     a block whose end is a tablejump, the tablejump referenced
-		     from the instruction is deleted too.  */
-		  rr.r1 = label2;
-		  rr.r2 = label1;
-		  for_each_rtx (&BB_END (bb1), replace_label, &rr);
-
 		  return match;
 		}
 	    }
@@ -1498,16 +1190,20 @@
 	}
     }
 
+  /* Ensure that the edge counts do match.  */
+  if (EDGE_COUNT (bb1->succs) != EDGE_COUNT (bb2->succs))
+    return false;
+
   /* First ensure that the instructions match.  There may be many outgoing
      edges so this test is generally cheaper.  */
-  if (!old_insns_match_p (mode, BB_END (bb1), BB_END (bb2)))
+  /* FIXME: the regset compare might be costly.  We should try to get a cheap
+     and reasonably effective test first.  */
+  if (!struct_equiv_init (STRUCT_EQUIV_START | *mode, info, true)
+      || !insns_match_p (BB_END (bb1), BB_END (bb2), info))
     return false;
 
-  /* Search the outgoing edges, ensure that the counts do match, find possible
-     fallthru and exception handling edges since these needs more
-     validation.  */
-  if (EDGE_COUNT (bb1->succs) != EDGE_COUNT (bb2->succs))
-    return false;
+  /* Search the outgoing edges, find possible fallthru and exception
+     handling edges since these needs more validation.  */
 
   FOR_EACH_EDGE (e1, ei, bb1->succs)
     {
@@ -1598,14 +1294,13 @@
 static bool
 try_crossjump_to_edge (int mode, edge e1, edge e2)
 {
-  int nmatch;
+  int nmatch, i;
   basic_block src1 = e1->src, src2 = e2->src;
   basic_block redirect_to, redirect_from, to_remove;
-  rtx newpos1, newpos2;
   edge s;
   edge_iterator ei;
-
-  newpos1 = newpos2 = NULL_RTX;
+  struct equiv_info info;
+  rtx x_active, y_active;
 
   /* If we have partitioned hot/cold basic blocks, it is a bad idea
      to try this optimization.
@@ -1652,18 +1347,36 @@
     return false;
 
   /* Look for the common insn sequence, part the first ...  */
-  if (!outgoing_edges_match (mode, src1, src2))
+  info.x_block = src2;
+  info.y_block = src1;
+  if (!outgoing_edges_match (&mode, &info))
     return false;
 
   /* ... and part the second.  */
-  nmatch = flow_find_cross_jump (mode, src1, src2, &newpos1, &newpos2);
+  info.input_cost = optimize_size ? COSTS_N_INSNS (1) : -1;
+  nmatch = struct_equiv_block_eq (STRUCT_EQUIV_START | mode, &info);
 
   /* Don't proceed with the crossjump unless we found a sufficient number
      of matching instructions or the 'from' block was totally matched
      (such that its predecessors will hopefully be redirected and the
      block removed).  */
-  if ((nmatch < PARAM_VALUE (PARAM_MIN_CROSSJUMP_INSNS))
-      && (newpos1 != BB_HEAD (src1)))
+  if (!nmatch)
+    return false;
+  if ((nmatch -info.cur.input_count < PARAM_VALUE (PARAM_MIN_CROSSJUMP_INSNS))
+      && (info.cur.y_start != BB_HEAD (src1)))
+    return false;
+  while (info.need_rerun)
+    {
+      nmatch = struct_equiv_block_eq (STRUCT_EQUIV_RERUN | mode, &info);
+      if (!nmatch)
+	return false;
+      if ((nmatch -info.cur.input_count < PARAM_VALUE (PARAM_MIN_CROSSJUMP_INSNS))
+	   && (info.cur.y_start != BB_HEAD (src1)))
+	return false;
+    }
+  nmatch = struct_equiv_block_eq (STRUCT_EQUIV_FINAL | mode, &info);
+  if ((nmatch -info.cur.input_count < PARAM_VALUE (PARAM_MIN_CROSSJUMP_INSNS))
+      && (info.cur.y_start != BB_HEAD (src1)))
     return false;
 
   /* Avoid deleting preserve label when redirecting ABNORMAL edges.  */
@@ -1671,6 +1384,35 @@
       && (e1->flags & EDGE_ABNORMAL))
     return false;
 
+  /* Skip possible basic block header.  */
+  x_active = info.cur.x_start;
+  if (LABEL_P (x_active))
+    x_active = NEXT_INSN (x_active);
+  if (NOTE_P (x_active))
+    x_active = NEXT_INSN (x_active);
+
+  y_active = info.cur.y_start;
+  if (LABEL_P (y_active))
+    y_active = NEXT_INSN (y_active);
+  if (NOTE_P (y_active))
+    y_active = NEXT_INSN (y_active);
+
+  /* In order for this code to become active, either we have to be called
+     before reload, or struct_equiv_block_eq needs to add register scavenging
+     code to allocate input_reg after reload.  */
+  if (info.input_reg)
+    {
+      emit_insn_before (gen_move_insn (info.input_reg, info.x_input),
+			x_active);
+      emit_insn_before (gen_move_insn (info.input_reg, info.y_input),
+			y_active);
+    }
+
+  for (i = 0; i < info.cur.local_count; i++)
+    if (info.local_rvalue[i])
+      emit_insn_before (gen_move_insn (info.x_local[i], info.y_local[i]),
+			y_active);
+
   /* Here we know that the insns in the end of SRC1 which are common with SRC2
      will be deleted.
      If we have tablejumps in the end of SRC1 and SRC2
@@ -1705,30 +1447,34 @@
   /* Avoid splitting if possible.  We must always split when SRC2 has
      EH predecessor edges, or we may end up with basic blocks with both
      normal and EH predecessor edges.  */
-  if (newpos2 == BB_HEAD (src2)
+  if (info.cur.x_start == BB_HEAD (src2)
       && !(EDGE_PRED (src2, 0)->flags & EDGE_EH))
     redirect_to = src2;
   else
     {
-      if (newpos2 == BB_HEAD (src2))
+      if (info.cur.x_start == BB_HEAD (src2))
 	{
 	  /* Skip possible basic block header.  */
-	  if (LABEL_P (newpos2))
-	    newpos2 = NEXT_INSN (newpos2);
-	  if (NOTE_P (newpos2))
-	    newpos2 = NEXT_INSN (newpos2);
+	  if (LABEL_P (info.cur.x_start))
+	    info.cur.x_start = NEXT_INSN (info.cur.x_start);
+	  if (NOTE_P (info.cur.x_start))
+	    info.cur.x_start = NEXT_INSN (info.cur.x_start);
 	}
 
       if (dump_file)
 	fprintf (dump_file, "Splitting bb %i before %i insns\n",
 		 src2->index, nmatch);
-      redirect_to = split_block (src2, PREV_INSN (newpos2))->dest;
+      redirect_to = split_block (src2, PREV_INSN (info.cur.x_start))->dest;
     }
 
   if (dump_file)
-    fprintf (dump_file,
-	     "Cross jumping from bb %i to bb %i; %i common insns\n",
+    {
+      fprintf (dump_file, "Cross jumping from bb %i to bb %i; %i common insns",
 	     src1->index, src2->index, nmatch);
+      if (info.cur.local_count)
+	fprintf (dump_file, ", %i local registers", info.cur.local_count);
+       fprintf (dump_file, "\n");
+    }
 
   redirect_to->count += src1->count;
   redirect_to->frequency += src1->frequency;
@@ -1792,17 +1538,12 @@
 
   /* Edit SRC1 to go to REDIRECT_TO at NEWPOS1.  */
 
-  /* Skip possible basic block header.  */
-  if (LABEL_P (newpos1))
-    newpos1 = NEXT_INSN (newpos1);
-
-  if (NOTE_P (newpos1))
-    newpos1 = NEXT_INSN (newpos1);
-
-  redirect_from = split_block (src1, PREV_INSN (newpos1))->src;
+  redirect_from = split_block (src1, PREV_INSN (y_active))->src;
   to_remove = single_succ (redirect_from);
 
   redirect_edge_and_branch_force (single_succ_edge (redirect_from), redirect_to);
+  COPY_REG_SET (redirect_from->il.rtl->global_live_at_end,
+		redirect_to->il.rtl->global_live_at_start);
   delete_basic_block (to_remove);
 
   update_forwarder_flag (redirect_from);
@@ -1817,7 +1558,7 @@
    any changes made.  */
 
 static bool
-try_crossjump_bb (int mode, basic_block bb)
+try_crossjump_bb (int mode, basic_block bb, bool first_pass)
 {
   edge e, e2, fallthru;
   bool changed;
@@ -1955,21 +1696,20 @@
 static bool
 try_optimize_cfg (int mode)
 {
+  bool first_crossjump_pass = true;
   bool changed_overall = false;
   bool changed;
+  bool changed_since_crossjump = true;
   int iterations = 0;
   basic_block bb, b, next;
 
-  if (mode & CLEANUP_CROSSJUMP)
-    add_noreturn_fake_exit_edges ();
-
   if (mode & (CLEANUP_UPDATE_LIFE | CLEANUP_CROSSJUMP | CLEANUP_THREADING))
     clear_bb_flags ();
 
   FOR_EACH_BB (bb)
     update_forwarder_flag (bb);
 
-  if (! targetm.cannot_modify_jumps_p ())
+  if (!targetm.cannot_modify_jumps_p ())
     {
       first_pass = true;
       /* Attempt to merge blocks as made possible by edge removal.  If
@@ -2124,11 +1864,6 @@
 	      if (try_forward_edges (mode, b))
 		changed_here = true;
 
-	      /* Look for shared code between blocks.  */
-	      if ((mode & CLEANUP_CROSSJUMP)
-		  && try_crossjump_bb (mode, b))
-		changed_here = true;
-
 	      /* Don't get confused by the index shift caused by
 		 deleting blocks.  */
 	      if (!changed_here)
@@ -2137,23 +1872,43 @@
 		changed = true;
 	    }
 
-	  if ((mode & CLEANUP_CROSSJUMP)
-	      && try_crossjump_bb (mode, EXIT_BLOCK_PTR))
-	    changed = true;
-
 #ifdef ENABLE_CHECKING
 	  if (changed)
 	    verify_flow_info ();
 #endif
 
+	  changed_since_crossjump |= changed;
+
+	  if ((mode & CLEANUP_CROSSJUMP)
+	      && changed_since_crossjump
+	      && !changed)
+	    {
+	      update_life_info_in_dirty_blocks (UPDATE_LIFE_GLOBAL_RM_NOTES,
+						(PROP_DEATH_NOTES
+						 | ((mode & CLEANUP_POST_REGSTACK)
+						    ? PROP_POST_REGSTACK : 0)));
+	      add_noreturn_fake_exit_edges ();
+
+	      for (b = ENTRY_BLOCK_PTR->next_bb; b != EXIT_BLOCK_PTR;)
+		{
+		  if (try_crossjump_bb (mode, b, first_crossjump_pass))
+		    changed = 1;
+		  else
+		    b = b->next_bb;
+		}
+	      changed |= try_crossjump_bb (mode, EXIT_BLOCK_PTR,
+					   first_crossjump_pass);
+	      remove_fake_exit_edges ();
+	      changed_since_crossjump = false;
+	      first_crossjump_pass = false;
+	    }
+
 	  changed_overall |= changed;
 	  first_pass = false;
 	}
       while (changed);
-    }
 
-  if (mode & CLEANUP_CROSSJUMP)
-    remove_fake_exit_edges ();
+    }
 
   FOR_ALL_BB (b)
     b->flags &= ~(BB_FORWARDER_BLOCK | BB_NONTHREADABLE_BLOCK);
Index: gcc/simplify-rtx.c
===================================================================
--- gcc/simplify-rtx.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/simplify-rtx.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1694,10 +1694,10 @@
     case MINUS:
       /* We can't assume x-x is 0 even with non-IEEE floating point,
 	 but since it is zero except in very strange circumstances, we
-	 will treat it as zero with -funsafe-math-optimizations.  */
+	 will treat it as zero with -ffinite-math-only.  */
       if (rtx_equal_p (trueop0, trueop1)
 	  && ! side_effects_p (op0)
-	  && (! FLOAT_MODE_P (mode) || flag_unsafe_math_optimizations))
+	  && (!FLOAT_MODE_P (mode) || !HONOR_NANS (mode)))
 	return CONST0_RTX (mode);
 
       /* Change subtraction from zero into negation.  (0 - x) is the
Index: gcc/gthr-generic.c
===================================================================
--- gcc/gthr-generic.c	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/gthr-generic.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,170 @@
+/* Generic threads supplementary implementation. */
+/* Compile this one with gcc.  */
+/* Copyright (C) 1997, 1999, 2000, 2002, 2006 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 2, or (at your option)
+any later version.
+
+GCC is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA
+02110-1301, USA.  */
+
+/* As a special exception, if you link this library with other files,
+   some of which are compiled with GCC, to produce an executable,
+   this library does not by itself cause the resulting executable
+   to be covered by the GNU General Public License.
+   This exception does not however invalidate any other reasons why
+   the executable file might be covered by the GNU General Public License.  */
+
+#define __GTHR_WEAK __attribute__ ((weak))
+
+#include "tconfig.h"
+#include "gthr.h"
+
+#ifndef __gthr_generic_h
+#error "Generic thread support package not supported"
+#endif
+
+/* These are stub functions.  When threading is available, a suitable set of definitions should be linked in.  */
+
+/* Return 1 if thread system is active, 0 if not.  */
+int
+__generic_gxx_active_p (void)
+{
+  return 0;
+}
+
+/* The following functions should return zero on success or the error
+   number.  If the operation is not supported, -1 is returned.
+
+   __generic_gxx_once
+   __generic_gxx_key_create
+   __generic_gxx_key_delete
+   __generic_gxx_setspecific
+   __generic_gxx_mutex_lock
+   __generic_gxx_mutex_trylock
+   __generic_gxx_mutex_unlock
+   __generic_gxx_recursive_mutex_lock
+   __generic_gxx_recursive_mutex_trylock
+   __generic_gxx_recursive_mutex_unlock  */
+
+/* FUNC is a function that should be called without parameters.
+   *ONCE has been initialized to __GTHREAD_ONCE_INIT and is otherwise only
+   used in calls to __generic_gxx_once with FUNC as the second parameter.
+   If __generic_gxx_once succeeds, FUNC will have been called exactly once
+   since the initialization of ONCE through any number of calls of
+   __generic_gxx_once with this pair of ONCE and FUNC values.  */
+int
+__generic_gxx_once (__gthread_once_t *once ATTRIBUTE_UNUSED,
+		    void (*func)(void) ATTRIBUTE_UNUSED)
+{
+  return -1;
+}
+
+/* Assign a key to *KEY that can be used in calls to
+   __generic_gxx_setspecific / __generic_gxx_getspecific.
+   If DTOR is nonzero, and at thread exit the value associated with the key
+   is nonzero, DTOR will be called at thread exit with the value associated
+   with the key as its only argument.  */
+int
+__generic_gxx_key_create (__gthread_key_t *key ATTRIBUTE_UNUSED,
+			  void (*dtor)(void *) ATTRIBUTE_UNUSED)
+{
+  return -1;
+}
+
+/* KEY is a key previously allocated by __generic_gxx_key_create.
+   Remove it from the set of keys known for this thread.  */
+int
+__generic_gxx_key_delete (__gthread_key_t key ATTRIBUTE_UNUSED)
+{
+  return -1;
+}
+
+/* Return thread-specific data associated with KEY.  */
+void *
+__generic_gxx_getspecific (__gthread_key_t key ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* Set thread-specific data associated with KEY to PTR.  */
+int
+__generic_gxx_setspecific (__gthread_key_t key ATTRIBUTE_UNUSED,
+		      const void *ptr ATTRIBUTE_UNUSED)
+{
+  return -1;
+}
+
+/* Initialize *MUTEX.  */
+void
+__generic_gxx_mutex_init_function (__gthread_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+}
+
+/* Acquire a lock on *MUTEX.  The behaviour is undefined if a lock on *MUTEX
+   has already been acquired by the same thread.  */
+int
+__generic_gxx_mutex_lock (__gthread_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* Try to acquire a lock on *MUTEX.  If a lock on *MUTEX already exists,
+   return an error code.  */
+int
+__generic_gxx_mutex_trylock (__gthread_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* A lock on *MUTEX has previously been acquired with __generic_gxx_mutex_lock
+   or __generic_gxx_mutex_trylock.  Release the lock.  */
+int
+__generic_gxx_mutex_unlock (__gthread_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* Initialize *MUTEX.  */
+void
+__generic_gxx_recursive_mutex_init_function (__gthread_recursive_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+}
+
+/* Acquire a lock on *MUTEX.  If a lock on *MUTEX has already been acquired by
+   the same thread, succeed.  */
+int
+__generic_gxx_recursive_mutex_lock (__gthread_recursive_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* Try to acquire a lock on *MUTEX.  If a lock on *MUTEX has already been
+   acquired by the same thread, succeed.  If any other lock on *MUTEX
+   already exists, return an error code.  */
+int
+__generic_gxx_recursive_mutex_trylock (__gthread_recursive_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* A lock on *MUTEX has previously been acquired with
+   __generic_gxx_recursive_mutex_lock or
+   __generic_gxx_recursive_mutex_trylock.  Release the lock.  */
+int
+__generic_gxx_recursive_mutex_unlock (__gthread_recursive_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
Index: gcc/tree-ssa-live.c
===================================================================
--- gcc/tree-ssa-live.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/tree-ssa-live.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1260,7 +1260,7 @@
 static
 int compare_pairs (const void *p1, const void *p2)
 {
-#if 0
+#if 1
   partition_pair_p * pp1 = (partition_pair_p *) p1;
   partition_pair_p * pp2 = (partition_pair_p *) p2;
   int result;
Index: gcc/gthr-generic.h
===================================================================
--- gcc/gthr-generic.h	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/gthr-generic.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,372 @@
+/* Generic threads compatibility routines for libgcc2 and libobjc. */
+/* Compile this one with gcc.  */
+/* Copyright (C) 1997, 1999, 2000, 2002, 2006 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 2, or (at your option)
+any later version.
+
+GCC is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA
+02110-1301, USA.  */
+
+/* As a special exception, if you link this library with other files,
+   some of which are compiled with GCC, to produce an executable,
+   this library does not by itself cause the resulting executable
+   to be covered by the GNU General Public License.
+   This exception does not however invalidate any other reasons why
+   the executable file might be covered by the GNU General Public License.  */
+
+#ifndef __gthr_generic_h
+#define __gthr_generic_h
+
+#define __GTHREADS 1
+
+#define __GTHREAD_ONCE_INIT 0
+#define __GTHREAD_MUTEX_INIT_FUNCTION __gthread_mutex_init_function
+#define __GTHREAD_RECURSIVE_MUTEX_INIT_FUNCTION __gthread_recursive_mutex_init_function
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/* Avoid depedency on specific headers.
+   The general idea is that you dynamically allocate the required data
+   structures, and a void * is used to point to this dynamically allocated
+   data.  If your implementation can put all the required information in
+   the void * itself, that's fine, too, of course.
+   libstdc++ inherits from the mutex types, whcih is why they need to be
+   wrapped up as structs.  */
+typedef void *__gthread_key_t;
+typedef void *__gthread_once_t;
+typedef struct __gthread_mutex_s { void *__p; } __gthread_mutex_t;
+typedef struct __gthread_recursive_mutex_s { void *__p; } __gthread_recursive_mutex_t;
+
+/* We should always link with at least one definition, so we want strong
+   references.  The stub definitions are weak so that they can be overriden.  */
+#ifndef __GTHR_WEAK
+#define __GTHR_WEAK
+#endif
+
+extern int __generic_gxx_active_p (void) __GTHR_WEAK;
+
+extern int __generic_gxx_once (__gthread_once_t *, void (*)(void)) __GTHR_WEAK;
+
+extern int __generic_gxx_key_create (__gthread_key_t *,
+				     void (*)(void *)) __GTHR_WEAK;
+
+extern int __generic_gxx_key_delete (__gthread_key_t key) __GTHR_WEAK;
+
+extern void *__generic_gxx_getspecific (__gthread_key_t key) __GTHR_WEAK;
+
+extern int __generic_gxx_setspecific (__gthread_key_t, const void *) __GTHR_WEAK;
+
+extern void __generic_gxx_mutex_init_function (__gthread_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_mutex_lock (__gthread_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_mutex_trylock (__gthread_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_mutex_unlock (__gthread_mutex_t *) __GTHR_WEAK;
+
+extern void __generic_gxx_recursive_mutex_init_function (__gthread_recursive_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_recursive_mutex_lock (__gthread_recursive_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_recursive_mutex_trylock (__gthread_recursive_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_recursive_mutex_unlock (__gthread_recursive_mutex_t *) __GTHR_WEAK;
+
+#ifdef __cplusplus
+}
+#endif
+
+#ifdef _LIBOBJC
+
+extern int __generic_gxx_objc_init_thread_system (void) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_close_thread_system (void) __GTHR_WEAK;
+
+extern objc_thread_t __generic_gxx_objc_thread_detach (void (*)(void *), void *) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_thread_set_priority (int priority) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_thread_get_priority (void) __GTHR_WEAK;
+
+extern void __generic_gxx_objc_thread_yield (void) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_thread_exit (void) __GTHR_WEAK;
+
+extern objc_thread_t __generic_gxx_objc_thread_id (void) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_thread_set_data (void *value) __GTHR_WEAK;
+
+extern void *__generic_gxx_objc_thread_get_data (void) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_allocate (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_deallocate (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_lock (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_trylock (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_unlock (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_allocate (objc_condition_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_deallocate (objc_condition_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_wait (objc_condition_t, objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_broadcast (objc_condition_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_signal (objc_condition_t) __GTHR_WEAK;
+
+/* Backend initialization functions */
+
+/* Initialize the threads subsystem.  */
+static inline int
+__gthread_objc_init_thread_system (void)
+{
+  return __generic_gxx_objc_init_thread_system ();
+}
+
+/* Close the threads subsystem.  */
+static inline int
+__gthread_objc_close_thread_system (void)
+{
+  return __generic_gxx_objc_close_thread_system ();
+}
+
+/* Backend thread functions */
+
+/* Create a new thread of execution.  */
+static inline objc_thread_t
+__gthread_objc_thread_detach (void (* func)(void *), void * arg)
+{
+  return __generic_gxx_objc_thread_detach (func, arg);
+}
+
+/* Set the current thread's priority.  */
+static inline int
+__gthread_objc_thread_set_priority (int priority)
+{
+  return __generic_gxx_objc_thread_set_priority (priority);
+}
+
+/* Return the current thread's priority.  */
+static inline int
+__gthread_objc_thread_get_priority (void)
+{
+  return __generic_gxx_objc_thread_get_priority ();
+}
+
+/* Yield our process time to another thread.  */
+static inline void
+__gthread_objc_thread_yield (void)
+{
+  __generic_gxx_objc_thread_yield ();
+}
+
+/* Terminate the current thread.  */
+static inline int
+__gthread_objc_thread_exit (void)
+{
+  return __generic_gxx_objc_thread_exit ();
+}
+
+/* Returns an integer value which uniquely describes a thread.  */
+static inline objc_thread_t
+__gthread_objc_thread_id (void)
+{
+  return __generic_gxx_objc_thread_id ();
+}
+
+/* Sets the thread's local storage pointer.  */
+static inline int
+__gthread_objc_thread_set_data (void *value)
+{
+  return __generic_gxx_objc_thread_set_data (value);
+}
+
+/* Returns the thread's local storage pointer.  */
+static inline void *
+__gthread_objc_thread_get_data (void)
+{
+  return __generic_gxx_objc_thread_get_data ();
+}
+
+/* Backend mutex functions */
+
+/* Allocate a mutex.  */
+static inline int
+__gthread_objc_mutex_allocate (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_allocate (mutex);
+}
+
+/* Deallocate a mutex.  */
+static inline int
+__gthread_objc_mutex_deallocate (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_deallocate (mutex);
+}
+
+/* Grab a lock on a mutex.  */
+static inline int
+__gthread_objc_mutex_lock (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_lock (mutex);
+}
+
+/* Try to grab a lock on a mutex.  */
+static inline int
+__gthread_objc_mutex_trylock (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_trylock (mutex);
+}
+
+/* Unlock the mutex */
+static inline int
+__gthread_objc_mutex_unlock (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_unlock (mutex);
+}
+
+/* Backend condition mutex functions */
+
+/* Allocate a condition.  */
+static inline int
+__gthread_objc_condition_allocate (objc_condition_t condition)
+{
+  return __generic_gxx_objc_condition_allocate (condition);
+}
+
+/* Deallocate a condition.  */
+static inline int
+__gthread_objc_condition_deallocate (objc_condition_t condition)
+{
+  return __generic_gxx_objc_condition_deallocate (condition);
+}
+
+/* Wait on the condition */
+static inline int
+__gthread_objc_condition_wait (objc_condition_t condition, objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_condition_wait (condition, mutex);
+}
+
+/* Wake up all threads waiting on this condition.  */
+static inline int
+__gthread_objc_condition_broadcast (objc_condition_t condition)
+{
+  return __generic_gxx_objc_condition_broadcast ( condition);
+}
+
+/* Wake up one thread waiting on this condition.  */
+static inline int
+__gthread_objc_condition_signal (objc_condition_t condition)
+{
+  return __generic_gxx_objc_condition_signal (condition);
+}
+
+#else /* !_LIBOBJC */
+
+static inline int
+__gthread_active_p (void)
+{
+  return __generic_gxx_active_p ();
+}
+
+static inline int
+__gthread_once (__gthread_once_t *once, void (*func)(void))
+{
+  return __generic_gxx_once (once, func);
+}
+
+static inline int
+__gthread_key_create (__gthread_key_t *key, void (*dtor)(void *))
+{
+  return __generic_gxx_key_create (key, dtor);
+}
+
+static inline int
+__gthread_key_delete (__gthread_key_t key)
+{
+  return __generic_gxx_key_delete (key);
+}
+
+static inline void *
+__gthread_getspecific (__gthread_key_t key)
+{
+  return __generic_gxx_getspecific (key);
+}
+
+static inline int
+__gthread_setspecific (__gthread_key_t key, const void *ptr)
+{
+  return __generic_gxx_setspecific (key, ptr);
+}
+
+static inline void
+__gthread_mutex_init_function (__gthread_mutex_t *mutex)
+{
+  __generic_gxx_mutex_init_function (mutex);
+}
+
+static inline int
+__gthread_mutex_lock (__gthread_mutex_t * mutex)
+{
+  return __generic_gxx_mutex_lock (mutex);
+}
+
+static inline int
+__gthread_mutex_trylock (__gthread_mutex_t * mutex)
+{
+  return __generic_gxx_mutex_trylock (mutex);
+}
+
+static inline int
+__gthread_mutex_unlock (__gthread_mutex_t * mutex)
+{
+  return __generic_gxx_mutex_unlock (mutex);
+}
+
+static inline void
+__gthread_recursive_mutex_init_function (__gthread_recursive_mutex_t *mutex)
+{
+  __generic_gxx_recursive_mutex_init_function (mutex);
+}
+
+static inline int
+__gthread_recursive_mutex_lock (__gthread_recursive_mutex_t * mutex)
+{
+  return __generic_gxx_recursive_mutex_lock (mutex);
+}
+
+static inline int
+__gthread_recursive_mutex_trylock (__gthread_recursive_mutex_t * mutex)
+{
+  return __generic_gxx_recursive_mutex_trylock (mutex);
+}
+
+static inline int
+__gthread_recursive_mutex_unlock (__gthread_recursive_mutex_t * mutex)
+{
+  return __generic_gxx_recursive_mutex_unlock (mutex);
+}
+
+#endif /* _LIBOBJC */
+
+#endif /* __gthr_generic_h */
Index: gcc/version.c
===================================================================
--- gcc/version.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/version.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -8,7 +8,7 @@
    in parentheses.  You may also wish to include a number indicating
    the revision of your modified compiler.  */
 
-#define VERSUFFIX ""
+#define VERSUFFIX " (STMicroelectronics Special" DATESTAMP ") [build "__DATE__"]"
 
 /* This is the location of the online document giving instructions for
    reporting bugs.  If you distribute a modified version of GCC,
@@ -17,9 +17,9 @@
    forward us bugs reported to you, if you determine that they are
    not bugs in your modifications.)  */
 
-const char bug_report_url[] = "<URL:http://gcc.gnu.org/bugs.html>";
+const char bug_report_url[] = "<file://doc/docbug.htm> on the installation CD";
 
 /* The complete version string, assembled from several pieces.
    BASEVER, DATESTAMP, and DEVPHASE are defined by the Makefile.  */
 
-const char version_string[] = BASEVER DATESTAMP DEVPHASE VERSUFFIX;
+const char version_string[] = BASEVER DEVPHASE VERSUFFIX;
Index: gcc/ChangeLog.STM
===================================================================
--- gcc/ChangeLog.STM	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/ChangeLog.STM	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,1037 @@
+2009-04-12  Christian Bruel  <christian.bruel@st.com>
+
+	* default.h (TARGET_USES_LEB128): New define.
+	* config/sh/sh.h (TARGET_USES_LEB128): Redefine.
+	* dwarf2asm.c (TARGET_USES_LEB128): Use instead of HAVE_AS_LEB128.
+	* except.c: Idem.
+	
+2009-05-05  Antony King  <antony.king@st.com>
+	    Christian Bruel  <christian.bruel@st.com>
+
+        INSbl30131
+	* lib1funcs.asm: Change local label naming convention.
+	* lib1funcs-Os-4-200.asm: Idem.
+
+2009-04-09  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/t-sh (ic_invalidate_array_4a.o): Fix st40-300 isa build.
+       (ic_invalidate_4a): Idem.
+        * config/sh/embed-elf.h (LIBGCC_SPEC): Fix ic_invalidate*.
+	* config/sh/sh.h (TARGET_CPU_DEFAULT, SUBTARGET_ASM_ISA_SPEC): Idem.
+	(ASM_ISA_SPEC_DEFAULT): Idem.
+
+2009-04-09  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (load_gbr): Fix operand constraint.
+
+2009-04-09  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl30099
+	* expr.c (may_tailcall): Return false in case of label.
+
+2009-03-31  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (broken_move): Fixed.
+
+2009-03-31  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (OVERRIDE_OPTIONS, OPTIMIZATION_OPTIONS): Set 
+	flag_emit_frame_pointer.
+
+2009-03-24  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/trap-handler.c (exit): Declare noreturn.
+	* config/sh/t-sh $(CFLAGS_FOR_TARGET): passed to trap-handler build.
+
+2009-03-13  Christian Bruel  <christian.bruel@st.com>
+
+	* c-incpath.c (add_standard_paths): Fix relocated include path.
+
+2009-03-09  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl21915
+	* config/sh/sh.h (SH_LINK_SPEC): Pass -shared on -pic.
+
+2009-03-03  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (CASE_VECTOR_MODE): Fix offset size for hwbug.
+	* config/sh/sh.c (sh_insn_length_adjustment): Fix pools for hwbug.
+
+2009-02-16  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/superh.h (SUBTARGET_ASM_RELAX_SPEC): Remove.
+	* config/sh/sh.h (SUBTARGET_ASM_RELAX_SPEC): Likewise.
+	(subtarget_asm_relax_spec). Likewise.
+
+2009-02-05  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (asm_size): Handle alignments.
+	(sh_asm_count): Likewise.
+	(sh_hw_workaround): Redesigned.
+	* config/sh/sh.h (SH_LINK_SPEC): pass --db-page-bug to the linker.
+	(INSN_LENGTH_ALIGNMENT): Fix minimum alignment.
+	* config/sh/linux-atomic.h: DB_ST40300_BUG_WORKAROUND fixes.
+	* config/sh/lib1funcs.asm: Likewise.
+
+2009-01-24  Christian Bruel  <christian.bruel@st.com>
+
+	* emit-rtl.c (emit_insn_after_1): Update SEQUENCE.
+
+2009-01-20  Christian Bruel  <christian.bruel@st.com>
+
+	* final.c (TARGET_ASM_COUNT): Use.
+	* config/sh/sh-protos.h (sh_asm_count): Declared.
+	* config/sh/sh.h (TARGET_ASM_COUNT): Declared.
+	* config/sh/sh.c (sh_asm_count): Defined.
+
+2009-01-19  Christian Bruel  <christian.bruel@st.com>
+
+	* final.c (asm_insn_count): Check for empty asm.
+
+2009-01-05  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl29600
+	* config/sh/sh.c (sh_dwarf_register_span): New function.
+	(TARGET_DWARF_REGISTER_SPAN): Defined.
+	* config/sh/sh-protos.h (sh_dwarf_register_span): Declared.
+
+2008-12-08  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_insn_length_adjustment): Optimize out delay slot.
+	* config/sh/sh.md (dup_db_insn): New unspec pattern.
+	* config/sh/sh.opt (mdead-delay): New option.
+	* final.c (realloc_insn_lengths): New function.
+	* output.h (realloc_insn_lengths): Declare.
+
+2008-12-08  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (OPVERRIDE_OPTIONS): Don't force function alignment.
+
+2008-11-28  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl29605
+	* config/sh/sh.opt (mfmovd): Document.
+
+2008-11-27  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (CAN_DEBUG_WITHOUT_FP): Defined.
+
+2008-11-14  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.opt (mspace): Removed.
+	* doc/invoke.texi (mspace): Removed.
+	* config/sh/sh.h: Use optimize_size for TARGET_SMALLCODE.
+	* config/sh/sh.c: Likewise.
+
+2008-10-30  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_reorg): Allow relaxation within simple loops.
+	
+2006-10-13  Carlos O'Donell  <carlos@codesourcery.com>
+	    Mark Mitchell  <mark@codesourcery.com>
+
+	* cppdefault.c: Define cpp_STANDARD_EXEC_PREFIX,
+	cpp_STANDARD_EXEC_PREFIX_len, and gcc_exec_prefix.
+	(cpp_relocated): New function.
+	* cppdefault.h: Declare cpp_STANARD_EXEC_PREFIx,
+	cpp_STANARD_EXEC_PREFIX_len, gcc_exec_prefix and cpp_relocated.
+	* Makefile.in (PREPROCESSOR_DEFINES): Add -DSTANDARD_EXEC_PREFIX
+	option.
+	* c-incpath.c (add_standard_paths): Call cpp_relocated. If relocated
+	replace configured prefix with gcc_exec_prefix.
+
+2006-10-13  Carlos O'Donell  <carlos@codesourcery.com>
+	    Mark Mitchell  <mark@codesourcery.com>
+
+	* gcc.c: Organize search path variables into $prefix relative,
+	and well-known native. Add comments.
+	(add_sysrooted_prefix): Add comment.
+	(process_command): If !gcc_exec_prefix add $prefix based paths.
+	If *cross_compile == '0', add native well-known paths.
+	Assert tooldir_base_prefix is always relative.
+	(main): If print_search_dirs, and if gcc_exec_prefix is set,
+	use this value for 'install:' path.
+	* Makefile.in: Add GCC_EXEC_PREFIX to generated site.exp.
+
+2008-10-24  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=4907
+	* config/sh/sh.md (casesi_worker_x): Add MEM indirect.
+	* config/sh/sh.c (sh_insn_length_adjustment): Handle casesi_worker.
+
+2008-10-12  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_forward_branch_p): Handle casesi_worker.
+
+2008-05-28  Antony King  <antony.king@st.com>
+
+	Fix INSbl27707:
+	* config/sh/superh.h (LIB_SPEC): Re-order libraries.
+
+2008-08-03  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/24528
+	* config/sh/sh.md (ashrsi2_16): make it a define_expand.
+	(ashrsi2_31): Likewise.
+
+2008-08-03  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (find_barrier): Update lengths for conditional branches.
+
+2008-07-09  Christian Bruel  <christian.bruel@st.com>
+
+	st40-300 hardware bug workaround	
+	* config/sh/linux.h: (SUBTARGET_LINK_SPEC): Options passed to the linker.
+	* config/sh/sh.h (TARGET_CPU_CPP_BUILTINS): Define DB_ST40300_BUG_WORKAROUND.
+	(OVERRIDE_OPTIONS): Set align_functions.
+	* config/sh/sh.c (sh_hw_workaround, sh_forward_branch_p): New function.
+	(sh_insn_length_adjustment): Add length parameter,
+	adjust length for workaround.
+	* config/sh/sh_protos.h (sh_hw_workaround): Likewise.
+	(sh_insn_length_adjustment): Add length parameter.
+	* final.c (final_scan_insn): call FINAL_PRESCAN_INSN.
+	* config/sh/sh.md (db-page-bug): new option.
+	* config/sh/sh.opt (mdb-page-bug): New option.
+
+2008-07-02  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (consttable_end): set length.
+	* config/sh/sh.h (MD_CAN_REDIRECT_BRANCH): Disable.
+	* final.c (shorten_branches): Add assertion.
+	* config/sh/sh.c (sh_jump_align): Use get_attr_min_length.
+	(barrier_align): Likewise.
+	(find_barrier): Take into account alignments into size.
+	(sh_reorg): use init_insn_lengths instead of INSN_ADDRESSES_FREE.
+
+2008-07-02  Christian Bruel  <christian.bruel@st.com>
+
+	* final.c (get_attr_length_1): Call get_attr_length_1 with fallback_fn
+	 instead of get_attr_length.
+
+2008-06-22  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_cfun_naked_p): New function.
+	(sh_handle_fndecl_attribute): Likewise.
+	(sh_attribute_table): Add "naked".
+	(sh_expand_prologue): Check sh_cfun_naked_p.
+	(sh_expand_prologue): Likewise.
+
+2008-06-22  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_can_redirect_branch): Check simplejump_p.
+	backwark starts from PREV_INSN.
+	(expand_cbranchdi4): Shut up warning.
+
+2008-06-22  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/24993
+	* config/sh/elf.h (MAX_OFILE_ALIGNMENT): Define.
+
+2008-06-17  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (doloop_end): Disable when optimizing for size.
+
+2008-05-07  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=3891
+	* config/sh/sh.c (find_barrier): Increase size of conditional branches.
+
+2008-05-06  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/28671
+	* tree-ssa-ccp.c (get_default_value): Check DECL_WEAK.
+	* c-typeck.c (decl_constant_value): Likewise.
+
+2008-04-17  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/28594
+	* config/sh/sh.c (expand_cbranchdi4): Use original operands for
+	msw_skip comparison.
+
+2007-10-04  Richard Guenther  <rguenther@suse.de>
+
+	PR tree-optimization/33627
+	* tree-gimple.h (canonicalize_cond_expr_cond): Declare.
+	* tree-gimple.c (canonicalize_cond_expr_cond): New function,
+	split out from ...
+	* tree-ssa-forwprop.c (combine_cond_expr_cond): ... here.
+
+2006-12-10  Zdenek Dvorak <dvorakz@suse.cz>
+
+	* doc/loop.texi: Document number_of_latch_executions and
+	number_of_cond_exit_executions.
+	* tree-scalar-evolution.c (compute_overall_effect_of_inner_loop,
+	chrec_is_positive, number_of_iterations_for_all_loops,
+	scev_const_prop): Use number_of_latch_executions.
+	(set_nb_iterations_in_loop): Do not increase the value of the
+	number of iterations.
+	(number_of_iterations_in_loop): Renamed to ...
+	(number_of_latch_executions): ... this.
+	(number_of_exit_cond_executions): New function.
+	* tree-scalar-evolution.h (number_of_iterations_in_loop): Declaration
+	removed.
+	(number_of_latch_executions, number_of_exit_cond_executions): Declare.
+	* tree-ssa-loop-ivcanon.c (canonicalize_loop_induction_variables): Use
+	number_of_latch_executions.
+	* tree-data-ref.c (get_number_of_iters_for_loop): Use
+	number_of_exit_cond_executions.
+	* tree-vect-analyze.c (vect_get_loop_niters): Ditto.
+	* cfgloop.h (struct loop): Improve description of the nb_iterations
+	field.
+
+2008-04-25  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/28502
+	* config/sh/sh.c (barrier_align): Skip notes.
+
+2008-02-20  Kaz Kojima  <kkojima@gcc.gnu.org>
+
+	PR target/35190
+	* config/sh/sh.md (jump_compact): Disable for crossing jumps.
+
+	* config/sh/sh.c (find_barrier): Don't go past
+	NOTE_INSN_SWITCH_TEXT_SECTIONS note.
+
+2008-03-11  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/28482
+	* struct-equiv.c (insns_match_p): Test registers in same regclass.
+
+2008-02-22  Christian Bruel  <christian.bruel@st.com>
+
+	* version.c (VERSUFFIX, version_string): Customize for STM.
+
+2008-01-28  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=3313
+	* tree-ssa-loop-ivopts.c (may_be_unaligned_p): Check loop offset.
+	(loop_offset_multiple_of): New function.
+	* tree.h (contains_packed_reference): Backmerge proto from trunk.
+	* expr.c (contains_packed_reference): Backmerge function from trunk.
+
+2008-02-19  Hans-Peter Nilsson  <hp@axis.com>
+
+	Support valgrind 3.3 for --enable-checking=valgrind.
+	* system.h: Consolidate ENABLE_VALGRIND_CHECKING-dependent defines
+	here.
+	[!VALGRIND_MAKE_MEM_NOACCESS]: Define as VALGRIND_MAKE_NOACCESS.
+	[!VALGRIND_MAKE_MEM_DEFINED]: Define as VALGRIND_MAKE_READABLE.
+	[!VALGRIND_MAKE_MEM_UNDEFINED]: Define as VALGRIND_MAKE_WRITABLE.
+	* ggc-common.c: Remove ENABLE_VALGRIND_CHECKING-dependent defines.
+	Replace use of VALGRIND_MAKE_READABLE, VALGRIND_MAKE_WRITABLE, and
+	VALGRIND_MAKE_NOACCESS with VALGRIND_MAKE_MEM_DEFINED,
+	VALGRIND_MAKE_MEM_UNDEFINED, and VALGRIND_MAKE_MEM_NOACCESS
+	respectively.
+	* ggc-zone.c: Similar.
+	* ggc-page.c: Similar.
+
+2007-08-31  Nick Clifton  <nickc@redhat.com>
+            Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/28192
+	* tree-ssa-coalesce.c (compare_pairs): Use the elements as
+	secondary keys in order to obtain a stable sort.
+
+2007-12-12  Christian Bruel  <christian.bruel@st.com>
+
+	* store-layout.c (finalize_record_size): Fixed TYPE_ALIGN.
+	* sh.c (expand_block_move): Optimize 64 bits copies if -mfmovd.
+	* sh.h (MOVE_BY_PIECES_P): Handle -mfmov.
+	(ROUND_TYPE_ALIGN): Likewise.
+
+2007-10-23  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/t-sh (_addsub_sf, _mul_sf, _addsub_df,  _extendsfdf2,
+	 _truncdfsf2, _fixunssfsi, _fixsfsi, _floatunssisf, _floatsisf,
+	_fixdfsi _floatunssidf _floatsidf, _muldf3, _divsf3): Renamed.
+	* config/sh/ieee-754-df.S: Likewise.
+	* config/sh/ieee-754-sf.S: Likewise.
+
+2007-10-23  Yvan Roux  <yvan.roux@st.com>
+
+	* config/sh/lib1funcs-4-300.asm (le128_neg): Fixed.
+	
+2007-10-14  Christian Bruel  <christian.bruel@st.com>
+
+	* expr.c (may_tailcall): Fixed.
+
+2007-10-04  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.c (for_each_path): Check just_multi_suffix and multi_suffix.
+
+2007-10-04  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/t-sh (TARGET_LIBGCC2_CFLAGS): Defined.
+	* config/sh/t-linux (TARGET_LIBGCC2_CFLAGS): Redefined.
+	(LIB1ASMFUNCS_CACHE): Cleaned up.
+
+2007-10-04  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (TARGET_HAVE_TLS): Removed.
+	* config/sh/linux.h (TARGET_HAVE_TLS): Defined.
+
+2007-10-03  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (cmpnedf_i1): Fix.
+
+2007-10-03  Christian Bruel  <christian.bruel@st.com>
+
+	* tree-tailcall.c (suitable_for_tail_call_opt_p): Export.
+	* expr.c (expand_assignment): Add parameter.
+	(may_tailcall): New function. Call.
+	* expr.h (expand_assignment): Fix prototype and add parameter.
+	* function.c (expand_assignment): Likewise.
+	(store_expr): Likewise.
+	* calls.c (store_expr): Likewise.
+	* stmt.c (expand_assignment): Likewise.
+
+2007-10-02  Antony King  <antony.king@st.com>
+
+	* genautomata.c (initiate_automaton_gen): Add call to CYGPATH().
+	* genchecksum.c (dosum): Likewise.
+	* gengtype-lex.l (parse_file): 	Add calls to CYGPATH() and
+	CYGPATH_FREE().
+	* gengtype.c: (close_output_files): Likewise.
+	* gensupport.c (cygpath_fopen): New function.
+	(process_include): Replace fopen() with cygpath_fopen().
+	(init_md_reader_args_cb): Likewise.
+	* gengtype-lex.c: Regenerate.
+
+2007-02-16  Richard Guenther  <rguenther@suse.de>
+	    Christian Bruel  <christian.bruel@st.com>
+
+	* fold-const.c (tree_swap_operands_p): Treat SSA_NAMEs like
+	DECLs but prefer SSA_NAMEs over DECLs.
+
+2006-10-28  Richard Guenther  <rguenther@suse.de>
+
+	PR middle-end/26899
+	* fold-const.c (maybe_canonicalize_comparison_1): Helper
+	for maybe_canonicalize_comparison.
+	(maybe_canonicalize_comparison): New function for canonicalizing
+	comparison trees.
+	(fold_comparison): Call it to canonicalize comparisons with
+	constants involved.
+
+2007-09-20  Yvan Roux  <yvan.roux@st.com>
+
+	* config/sh/t-sh: (LIB1ASMFUNCS) Add asm functions.
+	* config/sh/ieee-754-df.S: Fixed.
+	* config/sh/IEEE-754/m3/divsf3.S: Fixed.
+	* config/sh/IEEE-754/m3/divdf3.S: Fixed.
+	* config/sh/IEEE-754/m3/addsf3.S: Fixed.
+	* config/sh/IEEE-754/m3/adddf3.S: Fixed.
+	* config/sh/IEEE-754/m3/mulsf3.S: Fixed.
+	* config/sh/IEEE-754/m3muldf3.S: Fixed.
+
+2007-09-19  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/25896
+	* sh.md (movsf_y): New pattern.
+	(pop_fpul2_y): Likewise.
+
+2007-09-07  Christian Bruel  <christian.bruel@st.com>
+
+	* sh.h (SH_DBX_REGISTER_NUMBER): Added fpscr and fixed sr/gbr_regs.
+	* linux-unwind.h (SH_DWARF_FRAME_GBR): Fixed.
+
+2007-08-21  Christian Bruel  <christian.bruel@st.com>
+
+	* fold-const.c (fold_binary): Optimize A-A->0 if -ffinite-math-only.
+	* simplify_rtx.c (simplify_binary_operation_1): Likewise.
+
+2007-08-16  Antony King  <antony.king@st.com>
+
+	* configure.ac: Relaxed check for .[su]leb128 support.
+	* configure: Regenerate.
+
+2007-08-14  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* configure.ac (SYSTEM_HEADER_DIR): Adjust for in-tree Newlib.
+	* configure: Regenerate.
+
+2007-08-14  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* gcc.c (process_command): Apply CYGPATH to -B option arguments.
+
+2007-07-16  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (MOVE_MAX_PIECES): Tuned for TARGET_SH1.
+
+2007-07-12  Carl Shaw  <carl.shaw@st.com>
+
+	* config/sh/linux-unwind.h (sh_fallback_frame_state): Fix R0-R7 location.
+
+2007-06-26  Christian Bruel  <christian.bruel@st.com>
+
+	* gthr-generic.h: Rename *p to *__p.
+
+2007-06-20  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh-protos.h (sh_loads_bankedreg_p): Declare.
+	* config/sh/sh.c (sh_loads_bankedreg_p): New function.
+	(push_regs): Changed saving order or banked registers.
+	(sh_expand_epilogue): Likewise.
+	* config/sh/sh.h (BANKED_REGISTER_P): New macro.
+	(FIRST_BANKED_REG): Likewise.
+	(LAST_BANKED_REG): Likewise.
+	* config/sh/sh.md (banked) New attribute.
+	(in_delay_slot): Check banked attribute.
+
+2007-06-14  Christian Bruel  <christian.bruel@st.com>
+
+	* configure.ac: Unset CONFIG_SITE for canadian cross compilation.
+	* configure: Regenerate.
+
+2007-06-08  Christian Bruel  <christian.bruel@st.com>
+
+	PR target/29953
+	* config/sh/sh.md (doloop_end): New pattern and splitter.
+	* loop-iv.c (simple_rhs_p): Check for hardware registers.
+
+2007-05-23  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.opt (align-small-blocks=): New Optimisation.
+	* doc/invoke.texi (align-small-blocks=): Likewise.
+	* config/sh/sh.c (sh_jump_align): Check sh_align_small_blocks.
+	(barrier_align): Check sh_align_small_blocks.
+
+2007-04-30  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh-protos.h (sh_jump_align): New Function.
+	* config/sh/sh.c (sh_jump_align): Likewise.
+	(barrier_align): compute alignment based on TARGET_CACHE32.
+	* config/sh/sh.h (JUMP_ALIGN): Define.
+
+2007-03-29  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh.h (OVERRIDE_OPTIONS): Set assembler_dialect for sh1.
+2007-03-28  Christian Bruel  <christian.bruel@st.com>
+	* doc/invoke.texi: Document -m4-300.
+
+2007-02-20  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (movsi_i): Fix type attribute.
+	* config/sh/sh.md (movsi_ie): Set memory constraints attribute length.
+	(movsf_ie): Likewise.
+
+2007-03-09  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (__nesf2): Renamed.
+	(__nedf2): Likewise.
+	* config/sh/ieee-754-df.S (__nesf2): Likewise.
+	* config/sh/ieee-754-sf.S (__nedf2): Likewise.
+	* config/sh/t-sh: Likewise.
+
+2007-03-04  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/t-mlib-sh4-300: New file.
+	* config.gcc: (with-multilib-list) accept sh4-300.
+
+https://bugzilla.stlinux.com/show_bug.cgi?id=1203
+2007-02-20  Christian Bruel  <christian.bruel@st.com>
+
+	* regmove.c (rel_record_mem): check reg_overlap_mentioned_p.
+
+2007-01-31  Christian Bruel  <christian.bruel@st.com>
+
+	* basic-block.h (pre_edge_lcm_avs): Declare.
+	* config/i386/i386.h (EMIT_MODE_SET): Add FLIP parameter.
+	* doc/tm.texi (EMIT_MODE_SET): Idem.
+	* config/sh/sh.h (EMIT_MODE_SET): Idem. Call emit_fpu_flip.
+        (CONDITIONAL_REGISTER_USAGE): Set global_regs[FPSCR_REG].
+	* config/sh/sh-protos.h	(emit_fpu_flip): Add proto.
+	* config/sh/sh.c (emit_fpu_flip): New function.
+	* config/sh/sh.md (toggle_pr): Defined for TARGET_SH4_300.
+	Defined if TARGET_FPU_SINGLE.
+	fpscr_toggle don't go in delay slot (temporary fix).
+	* lcm.c (pre_edge_lcm_avs): Renamed from pre_edge_lcm.
+	Call clear_aux_for_edges. Fix comments.
+	(pre_edge_lcm): New wrapper function to call pre_edge_lcm_avs.
+	(pre_edge_rev_lcm): Idem.
+	* mode-switching.c (init_modes_infos): New function.
+	(bb_has_complex_pred): New function.
+	(free_modes_infos): Idem.
+	(init_modes_infos): Idem
+	(add_mode_set): Idem.
+	(get_mode): Idem.
+	(commit_mode_sets): Idem.
+	(merge_modes): Idem.
+	(set_flip_status): Idem
+	(test_flip_status): Idem.
+	(optimize_mode_switching): Add support to maintain flip mode information.
+	* testsuite/gcc.target/sh/sh.exp: New file.
+	* testsuite/gcc.target/sh/fpchg1.c: New test.
+	* testsuite/gcc.target/sh/fpchg2.c: Idem.
+
+2007-01-29  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/IEEE-754/m3/adddf3.S: Fix inf mantissa.
+	* config/sh/IEEE-754/m3/addsf3.S: Likewise.
+	* config/sh/IEEE-754/m3/divsf3.S: Intialize xff000000 label.
+	* config/sh/sh.c (expand_sfunc_op): Use FIRST_FP_PARM_REG for
+	parameters.
+	* config/sh/sh.h (TARGET_OSFP): Disable.
+	* config/sh/sh.md (addsf3, subsf3, mulsf3): Use expand_sfunc_binopt
+	only when TARGET_OSFP.
+	(adddf3, subdf3, muldf3): Likewise.
+	(trunkdfsf2): Likewise.
+
+2007-01-22  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/t-sh (LIB1ASMFUNCS): Remove _add_sub_sf3, _mulsf3,
+	_hypotf, _muldf3, _add_sub_df3, _divsf3, _divdf3, _fixunssfsi,
+	_fixsfsi, _fixunsdfsi, _fixdfsi, _floatunssisf, _floatsisf,
+	_floatunssidf and _floatsidf.
+	(FPBIT, DPBIT, dp-bit.c, fp-bit.c): Re-instated.
+
+2007-01-12  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* config/sh/trap-handler.c: Call exit like old one used to.
+
+2006-11-30  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* regmove.c (struct rel_use_chain): New member invalidate_luid.
+	(link_chains): Check invalidate_luid of predecessor.
+	(optimize_related_values_1): Initialize invalidate_luid for chains.
+
+http://gcc.gnu.org/ml/gcc-patches/2006-11/msg01043.html
+2006-11-29  J"orn Rennecke  <joern.rennecke@st.com>
+
+	PR rtl-optimization/28618:
+	* sched-deps.c (sched_analyze_2): When a likely spilled register
+	is used, put in into a scheduling group with the insn that
+	sets it and with all the insns in-between.
+
+2006-09-02  J"orn Rennecke  <joern.rennecke@st.com>
+
+	config/sh/t-sh: ($(T)ic_invalidate_array_4-100.o): Add -I. .
+	($(T)ic_invalidate_array_4-200.o): Likewise.
+	($(T)ic_invalidate_array_4a.o): Likewise.
+
+2006-09-02  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* sh.md (*movsicc_t_false, *movsicc_t_true): Add mode.
+
+2006-11-10  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* sh.c (expand_cbranchdi4): After reload, use appropriate scratch
+	operands.
+
+2006-11-10  J"orn Rennecke  <joern.rennecke@st.com>
+	    Aanchal Khanna   <aanchalk@noida.hcltech.com>
+	    Rakesh Kumar  <rakesh.kumar@noida.hcltech.com>
+
+	PR target/29845
+	* config/sh/sh-protos.h (sh_function_kind): New enumerator
+	SFUNC_FREQUENT.
+	(expand_sfunc_unop, expand_sfunc_binop): Declare.
+	(sh_expand_float_cbranch): Likewise.
+	* config/sh/lib1funcs.asm (ieee-754-sf.S, ieee-754-df.S): #include.
+	* config/sh/t-sh (LIB1ASMFUNCS): Add nesf2, _nedf2, _gtsf2t, _gtdf2t,
+	_gesf2f, _gedf2f, _extendsfdf2, , _truncdfsf2, _add_sub_sf3, _mulsf3,
+	_hypotf, _muldf3, _add_sub_df3, _divsf3, _divdf3, _fixunssfsi,
+	_fixsfsi, _fixunsdfsi, _fixdfsi, _floatunssisf, _floatsisf,
+	_floatunssidf and _floatsidf.
+	(FPBIT, DPBIT, dp-bit.c, fp-bit.c): Removed.
+	* config/sh/ieee-754-df.S, config/sh/ieee-754-sf.S: New files.
+	* config/sh/predicates.md (soft_fp_comparison_operand): New predicate.
+	(soft_fp_comparison_operator): Likewise.
+	* config/sh/sh.c (sh_soft_fp_cmp, expand_sfunc_op): New functions.
+	(expand_sfunc_unop, expand_sfunc_binop): Likewise.
+	(sh_expand_float_cbranch): Likewise.
+	(sh_expand_float_condop, sh_expand_float_scc): Likewise.
+	(from_compare): Add support for software floating point.
+	(function_symbol): Always look up name.  Add SFUNC_FREQUENT case.
+	* config/sh/sh.h (TARGET_SH1_SOFTFP): New macro.
+	(TARGET_SH1_SOFTFP_MODE): Likewise.
+	* config/sh/sh-modes.def (CC_FP_NE, CC_FP_GT, CC_FP_UNLT): New modes.
+	* config/sh/lib1funcs.h (SLC, SLI, SLCMP, DMULU_SAVE): New macros.
+	(DMULUL, DMULUH, DMULU_RESTORE, SHLL4, SHLR4, SHLL6, SHLR6): Likewise.
+	(SHLL12, SHLR12, SHLR19, SHLL23, SHLR24, SHLR21, SHLL21): Likewise.
+	(SHLR11, SHLR22, SHLR23, SHLR20, SHLL20, SHLD_COUNT, SHLRN): Likewise.
+	(SHLLN, DYN_SHIFT): Likewise.
+	(SUPPORT_SH3_OSFP, SUPPORT_SH3E_OSFP): Likewise.
+	(SUPPORT_SH4_NOFPU_OSFP, SUPPORT_SH4_SINGLE_ONLY_OSFP): Likewise.
+	(TARGET_OSFP): Likewise.
+	(OPTIMIZATION_OPTIONS): Always enable TARGET_CBRANCHDI4 and
+	TARGET_EXPAND_CBRANCHDI4.
+	If flag_trapping_math is set, make it 2.
+	(OVERRIDE_OPTIONS): If flag_trapping_math is 2 and non-trapping
+	software floating point is used, clear flag_trapping_math.
+	For SH1, set TARGET_EXPAND_CBRANCHDI4
+	* config/sh/ieee-754-df.S, config/sh/ieee-754-sf.S: New files.
+	* config/sh/IEEE-754/m3/divsf3.S: Likewise.
+	* config/sh/IEEE-754/m3/divdf3.S: Likewise.
+	* config/sh/IEEE-754/m3/floatunssisf.S: Likewise.
+	* config/sh/IEEE-754/m3/floatunssidf.S: Likewise.
+	* config/sh/IEEE-754/m3/fixunsdfsi.S: Likewise.
+	* config/sh/IEEE-754/m3/divdf3-rt.S: Likewise.
+	* config/sh/IEEE-754/m3/addsf3.S: Likewise.
+	* config/sh/IEEE-754/m3/adddf3.S: Likewise.
+	* config/sh/IEEE-754/m3/mulsf3.S: Likewise.
+	* config/sh/IEEE-754/m3/muldf3.S: Likewise.
+	* config/sh/IEEE-754/m3/floatsisf.S: Likewise.
+	* config/sh/IEEE-754/m3/floatsidf.S: Likewise.
+	* config/sh/IEEE-754/m3/fixdfsi.S: Likewise.
+	* config/sh/IEEE-754/divdf3.S: Likewise.
+	* config/sh/IEEE-754/floatunssisf.S: Likewise.
+	* config/sh/IEEE-754/fixunsdfsi.S: Likewise.
+	* config/sh/IEEE-754/adddf3.S: Likewise.
+	* config/sh/IEEE-754/floatsisf.S: Likewise.
+	* config/sh/IEEE-754/muldf3.S: Likewise.
+	* config/sh/IEEE-754/fixdfsi.S: Likewise.
+	* config/sh/IEEE-754/divsf3.S: Likewise.
+	* config/sh/IEEE-754/fixunssfsi.S: Likewise.
+	* config/sh/IEEE-754/floatunssidf.S: Likewise.
+	* config/sh/IEEE-754/addsf3.S: Likewise.
+	* config/sh/IEEE-754/mulsf3.S: Likewise.
+	* config/sh/IEEE-754/floatsidf.S: Likewise.
+	* config/sh/IEEE-754/fixsfsi.S: Likewise.
+	* config/sh/sh.md (SF_NAN_MASK, DF_NAN_MASK, FR4_REG): New constants.
+	(fpcmp_i1, cbranchsf4, cbranchdf4, addsf3_i3, subsf3_i3): New patterns.
+	(mulsf3_i3, cmpnesf_i1, cmpgtsf_i1, cmpunltsf_i1): Likewise.
+	(cmpeqsf_i1_finite, cmplesf_i1_finite, cmpunsf_i1): Likewise.
+	(cmpuneqsf_i1, movcc_fp_ne, movcc_fp_gtmovcc_fp_unlt): Likewise.
+	(cmpltgtsf_t, cmporderedsf_t, cmpltgtsf_t_4): Likewise.
+	(cmporderedsf_t_4, abssc2, adddf3_i3_wrap, adddf3_i3): Likewise.
+	(muldf3_i3_wrap, muldf3_i3, cmpnedf_i1, cmpgtdf_i1): Likewise.
+	(cmpunltdf_i1, cmpeqdf_i1_finite, cmpundf_i1, cmpuneqdf_i1): Likewise.
+	(cmpltgtdf_t, cmpordereddf_t_4, extendsfdf2_i1): Likewise.
+	(extendsfdf2_i2e, extendsfdf2_i2e_r0, truncdfsf2_i2e): Likewise.
+	(extendsfdf2_i1_r0, truncdfsf2_i1): Likewise.
+	(cmpun_sdf, sunle, cmpuneq_sdf, bunle, bunlt): Likewise.
+	(beq, bne, bgt, blt, ble, bge): Only support for SHmedia.
+	(bgtu, bltu, bgeu, bleu): Likewise.
+	(seq, sge, sne): Add support for software floating point.
+	(addsf3, subsf3, mulsf3): Likewise.
+	(adddf3, subdf3, muldf3, extendsfdf2, truncdfsf2): Likewise.
+	(movnegt): Match only one operand.  Changed user.
+
+http://gcc.gnu.org/ml/gcc-patches/2006-11/msg01074.html
+2006-11-15  J"orn Rennecke  <joern.rennecke@st.com>
+
+	PR middle-end/29847
+	* targhooks.c (regs.h): Include.
+	(default_match_adjust): New function, broken out of
+	reload.c:operands_match_p.
+	* targhooks.h (default_match_adjust): Declare.
+	* reload.c (operands_match_p): Use targetm.match_adjust.
+	* target.h (struct gcc_target): New member match_adjust.
+	* target-def.h (TARGET_MATCH_ADJUST): Define.
+	(TARGET_INITIALIZER): Add TARGET_MATCH_ADJUST.
+	* Makefile.in (targhooks.o): Depend on $(REGS_H).
+	* doc/tm.texi (TARGET_MATCH_ADJUST): Document.
+
+2006-09-02  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* config/sh/sh-protos.h (sh_match_adjust): Declare.
+	* config/sh/sh.c (TARGET_MATCH_ADJUST): Define as sh_match_adjust.
+	(sh_match_adjust): New function.
+
+http://gcc.gnu.org/ml/gcc-patches/2006-11/msg01069.html
+2006-11-15  J"orn Rennecke  <joern.rennecke@st.com>
+
+	PR middle-end/29846
+	* optabs.c (can_compare_p): Succeed if a cbranch handler exists,
+	irrespective of the existance of a compare instruction.
+
+2006-11-06  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* sh.opt (minline-ic_invalidate): New option.
+	(musermode): Adjust comment.
+	* sh.c (sh_initialize_trampoline): Emit library call unless
+	is set; if it is set, don't emit library call if we can use icbi
+	instead.
+	* sh.md (ic_invalidate_line, ic_invalidate_line_sh4a): Also use
+	icbi for TARGET_SH4_300.
+	* t-sh (LIB1ASMFUNCS_CACHE): Set.
+
+2006-11-03  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* config/sh/crt1.asm (_superh_trap_handler): Remove function.
+	* config/sh/trap-handler.c: New file.
+	* config/sh/t-elf (EXTRA_MULTILIB_PARTS): Add trap-handler.o.
+	* config/sh/t-superh (EXTRA_MULTILIB_PARTS): Likewise.
+	* config/sh/t-sh: Add rule for trap-handler.o.
+	* config/sh/elf.h (STARTFILE_SPEC): Add trap-handler.o.
+	* config/sh/superh.h (STARTFILE_SPEC): Likewise.
+
+2006-10-06  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* config/sh/crt1.asm (vbr_600): Add missing #if.
+
+2006-08-03  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* sh.opt (mfused-madd): New option.
+	* sh.md (mac_media, macsf3): Make conditional on TARGET_FMAC.
+
+2006-08-01  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* sh.md (cbranchdi4): Move earlyclobber modifier to affected
+	alternative.
+	* sh.c (expand_cbranchsi4): New argument probability.  Changed all
+	callers.
+	(expand_cbranchdi4): When emitting more than one branch, distribute
+	probabilities.
+	* sh-protos.h (expand_cbranchsi4): Update prorotype.
+
+2006-07-04  J"orn Rennecke  <joern.rennecke@st.com>
+
+	PR tree-optimization/28144
+	* fold-const.c (fold_convert_const_int_from_real): Fix endianness in
+	real_from_integer calls; adjust high word for lower bound.
+
+2006-07-04  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	Fix INSbl24460:
+	* config/sh/crt1.asm (vbr_start): Move to new section .test.vbr.
+	Remove pointless handler at VBR+0.
+	(vbr_200, vbr_300, vbr_500): Remove pointless handler.
+	(vbr_600): Save and restore mach and macl, fpul and fpscr and fr0 to
+	fr7. Make sure the timer handler is called with the correct FPU
+	precision setting, according to the ABI.
+
+2006-06-23  J"orn Rennecke <joern.rennecke@st.com>
+
+	PR tree-optimization/28144
+	* fold-const.c (fold_convert_const_int_from_real): When converting
+	to a type with a precision less than 32, use INT_MIN / INT_MAX
+	for saturation bounds.
+
+2006-06-14  J"orn Rennecke <joern.rennecke@st.com>
+
+	* config/sh/sh.opt (m2a-single, m2a-single-only): Fix Condition.
+	* config/sh/sh.h (SUPPORT_SH2A_NOFPU): Fix condition.
+	(SUPPORT_SH2A_SINGLE_ONLY, SUPPORT_SH2A_SINGLE_ONLY): Likewise.
+
+2006-06-14  J"orn Rennecke <joern.rennecke@st.com>
+
+	* Makefile.in (version.o): Depend on $(OBJS) except for version.o .
+
+2006-06-13  J"orn Rennecke <joern.rennecke@st.com>
+
+	* t-sh (MULTILIB_MATCHES): Add and m4-100-nofpu,m4-200-nofpu.
+
+2006-06-09  J"orn Rennecke <joern.rennecke@st.com>
+
+	* sh.md (cmpgeusi_t): Change into define_insn_and_split.  Accept
+	zero as second operand.
+
+2006-05-09  J"orn Rennecke <joern.rennecke@st.com>
+
+	* config/sh/sh.c (expand_cbranchdi4): Avoid calling
+	swap_condition (CODE_FOR_nothing).
+
+2006-05-03  Andrew Stubbs  <andrew.stubbs@st.com>
+            J"orn Rennecke <joern.rennecke@st.com>
+
+	* gcc.c (process_command): Try to set gcc_exec_prefix with
+	make_relative_prefix_ignore_links before resolving to
+	make_relative_prefix.
+	If gcc_exec_prefix has been pre-set, use
+	make_relative_prefix_ignore_links to calculate gcc_libexec_prefix.
+
+2006-04-28  J"orn Rennecke <joern.rennecke@st.com>
+
+	* config/sh/divtab-sh4-300.c, config/sh/lib1funcs-4-300.asm:
+	Fixed some bugs related to nagative values, in particular -0
+	and overflow at -0x80000000.
+	* config/sh/divcost-analysis: added sh4-300 figures.
+	* testsuite/gcc.c-torture/execute/arith-rand-ll.c:
+	Also test for bogus rest sign.
+
+2006-04-27  J"orn Rennecke <joern.rennecke@st.com>
+
+	* config/sh/t-sh (MULTILIB_MATCHES): Add -m4-300* / -m4-340 options.
+
+2006-04-26  J"orn Rennecke <joern.rennecke@st.com>
+
+	* sh.c (expand_cbranchdi4): Avoid generating >= 0U tests.
+
+2006-04-26  J"orn Rennecke <joern.rennecke@st.com>
+
+	* config/sh/t-sh (OPT_EXTRA_PARTS): Add libgcc-4-300.a.
+	($(T)div_table-4-300.o, $(T)libgcc-4-300.a): New rules.
+	* config/sh/divtab-sh4-300.c, config/sh/lib1funcs-4-300.asm: New files.
+	* config/sh/embed-elf.h (LIBGCC_SPEC): Use -lgcc-4-300 for -m4-300* /
+	-m4-340.
+
+	* config/sh/sh.opt: Amend -mdiv= option description.
+
+2006-04-24  J"orn Rennecke <joern.rennecke@st.com>
+
+	* sh.md (mulr03, mulr03+1, mulr03+2): New patterns.
+	(mulsi3) Use mulr03 for TARGET_R0R3_TO_REG_MUL.
+	* sh.h (CONDITIONAL_REGISTER_USAGE): If -mr0r3-to-reg-mul is not in
+	effect, set regno_reg_class for R1_REG .. R3_REG to GENERAL_REGS.
+	(OVERRIDE_OPTIONS): Initilize TARGET_R0R3_TO_REG_MUL if it has
+	not been set by an option.
+	(enum reg_class, REG_CLASS_NAMES, REG_CLASS_CONTENTS): Add R0R3_REGS.
+	(REG_CLASS_FROM_CONSTRAINT): Decode R03.
+	* sh.opt (mlate-r0r3-to-reg-mul, mr0r3-to-reg-mul): New options.
+
+	SH4-300 scheduling description & fixes to SH4-[12]00 description:
+	* sh.md: New instruction types: fstore, movi8, fpscr_toggle, gp_mac,
+	mac_mem, mem_mac, dfp_mul, fp_cmp.
+	(insn_class, dfp_comp, any_fp_comp): Update.
+	(push_fpul, movsf_ie, fpu_switch, toggle_sz, toggle_pr): Update type.
+	(cmpgtsf_t, "cmpeqsf_t, cmpgtsf_t_i4, cmpeqsf_t_i4): Likewise.
+	(muldf3_i): Likewise.
+	(movsi_i): Split rI08 alternative into two separate alternatives.
+	Update type.
+	(movsi_ie, movsi_i_lowpart): Likewise.
+	(movqi_i): Split ri alternative into two separate alternatives.
+	Update type.
+	* sh1.md (sh1_load_store, sh1_fp): Update.
+	* sh4.md (sh4_store, sh4_mac_gp, fp_arith, fp_double_arith): Update.
+	(mac_mem, sh4_fpscr_toggle): New insn_reservations.
+	* sh4a.md (sh4a_mov, sh4a_load, sh4a_store, sh4a_fp_arith): Update.
+	(sh4a_fp_double_arith): Likewise.
+	* sh4-300.md: New file.
+	* sh.c (sh_handle_option): Handle m4-300* options.
+	(sh_adjust_cost): Fix latency of auto-increments.
+	Handle SH4-300 differently than other SH4s.  Check for new insn types.
+	* sh.h (OVERRIDE_OPTIONS): Initilize sh_branch_cost if it has not been
+	set by an option.
+	* sh.opt (m4-300, m4-100-nofpu, m4-200-nofpu): New options.
+	(m4-300-nofpu, -m4-340, m4-300-single, m4-300-single-only): Likewise.
+	(mbranch-cost=, mload-latency=): Likewise.
+	* superh.h (STARTFILE_SPEC): Take -m4-340 into account.
+
+	* sh.md (mulsf3): Remove special expansion code.
+	(mulsf3_ie): Now a define_insn_and_split.
+	(macsf3): Allow for TARGET_SH4.
+
+	* config/sh/predicates.md (less_comparison_operator): Remove
+	redundant code.
+
+	* sh.md (cbranchsi4, cbranchdi4, cbranchdi4_i): New patterns.
+	* sh.c (prepare_cbranch_operands, expand_cbranchsi4): New functions.
+	(expand_cbranchdi4): Likewise.
+	(sh_rtx_costs): Give lower cost for certain CONST_INT values and for
+	CONST_DOUBLE if the outer code is COMPARE.
+	* sh.h (OPTIMIZATION_OPTIONS): If not optimizing for size, set
+	TARGET_CBRANCHDI4 and TARGET_EXPAND_CBRANCHDI4.
+	(OVERRIDE_OPTIONS): For TARGET_SHMEDIA, clear TARGET_CBRANCHDI4.
+	(LEGITIMATE_CONSTANT_P): Also allow DImode and VOIDmode CONST_DOUBLEs.
+	Remove redundant fp_{zero,one}_operand checks.
+	* sh.opt (mcbranchdi, mexpand-cbranchdi, mcmpeqdi): New options.
+	* sh-protos.h (prepare_cbranch_operands, expand_cbranchsi4): Declare.
+	(expand_cbranchdi4): Likewise.
+
+2006-04-20  J"orn Rennecke <joern.rennecke@st.com>
+
+	* sh.h (LEGITIMATE_CONSTANT_P): Allow VOIDmode CONST_DOUBLEs.
+
+2006-04-20  J"orn Rennecke <joern.rennecke@st.com>
+
+	* sh.h (LOCAL_ALIGNMENT): Use DATA_ALIGNMENT.
+
+2006-04-11  J"orn Rennecke <joern.rennecke@st.com>
+
+	* gthr-generic.h: Update to match
+	http://gcc.gnu.org/ml/gcc-patches/2006-04/msg00237.html .
+	* gthr-generic.c, gthr-objc-generic.c: Likewise.
+	* Makefile.in configure.ac: Likewise.
+	* configure: Regenerate.
+
+2006-03-27  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* gcc.c (read_specs): Add Cygwin path support for MinGW.
+	(find_a_file, process_file): Likewise.
+	* opts.c (common_handle_option): Likewise.
+	* prefix.c (update_path): Likewise.
+	* c-opts.c (c_common_handle_option): Likewise.
+	(check_deps_environment_vars): Likewise.
+	* c-incpath.c (add_path): Likewise.
+	* collect2.c (find_a_file, add_prefix): Likewise.
+	* Makefile.in (INSTALL_HEADERS): Disable the broken install-mkheaders.
+
+2006-03-24  J"orn Rennecke <joern.rennecke@st.com>
+
+	* t-sh (IC_EXTRA_PARTS): Replace ic_invalidate_array_* parts with
+	libic_invalidate_array_* parts.  Rename affected rules.
+	* embed-elf.h (LIBGCC_SPEC): Update.
+
+2006-03-21  J"orn Rennecke <joern.rennecke@st.com>
+
+	Apply patch to re-instate struct-equiv code for crossjump
+	matching:
+	http://gcc.gnu.org/ml/gcc-patches/2006-02/msg01488.html
+	Bernd Schmidt  <bernds@redhat.com>
+	* cfgcleanup.c
+	* cfgrtl.c
+	* cfgcleanup.c
+	* basic-block.h
+	* cfgcleanup.c
+	* struct-equiv.c
+
+2006-03-15  J"orn Rennecke <joern.rennecke@st.com>
+
+	* version.c (VERSUFFIX, bug_report_url): Customize for STM.
+
+2006-03-10  J"orn Rennecke <joern.rennecke@st.com>
+
+	http://gcc.gnu.org/ml/gcc-patches/2006-03/msg00602.html
+	* regmove.c (init_add_limits): Check find_matches to verify a
+	three-address-add instruction is genuine.
+
+2006-01-20  J"orn Rennecke <joern.rennecke@st.com>
+
+	http://gcc.gnu.org/ml/gcc-patches/2006-01/msg01387.html
+	* postreload.c (reload_combine): For every new use of REG_SUM, record
+	the use of BASE.
+
+2006-01-17  Antony King <anthony.king@st.com>
+            J"orn Rennecke <joern.rennecke@st.com>
+
+	* configure.ac: Recognize 'generic' value for threads.
+	Check for existance of a *.c and gthr-objc-*.c file for thread support.
+	Substiture in extra_libgcc_srcs and extra_libgcc_static_srcs.
+	* configure: Regenerate.
+	* Makefile.in (LIB2ADD): Add @extra_libgcc_srcs@.
+	(LIB2ADD_ST): Add @extra_libgcc_static_srcs@.
+	* gthr-generic.h: New file.
+	* gthr-generic.c: New file.
+	* gthr-objc-generic.c: New file.
+
+2005-09-15  J"orn Rennecke <joern.rennecke@st.com>
+	    Bernd Schmidt  <bernds@redhat.com>
+
+	PR rtl-optimization/20211
+	http://gcc.gnu.org/ml/gcc-patches/2005-09/msg01176.html
+	* common.opt: Add optimize-related-values entry.
+	* opts.c (decode_options): Set flag_optimize_related_values.
+	* optabs.c (gen_add3_insn): If direct addition is not possible,
+	try to move the constant into the destination register first.
+	* regmove.c (obstack.h, ggc.h, optabs.h): Include.
+	(related, rel_use_chain, rel_mod, rel_use): New structures.
+	(related_baseinfo, update): Likewise.
+	(lookup_related, rel_build_chain): New functions.
+	(recognize_related_for_insn, record_reg_use, create_rel_use): Likewise.
+	(new_reg_use, rel_record_mem, new_base, invalidate_related): Likewise.
+	(find_related, find_related_toplev, chain_starts_earlier): Likewise.
+	(chain_ends_later, mod_before, remove_setting_insns): Likewise.
+	(perform_addition, modify_address): Likewise.
+	(optimize_related_values_1, optimize_related_values_0): Likewise.
+	(optimize_related_values, count_sets, link_chains): Likewise.
+	(init_add_limits): Likewise.
+	(REL_USE_HASH_SIZE, REL_USE_HASH, rel_alloc, rel_new): New macros.
+	(regno_related, rel_base_list, unrelatedly_used): New variables.
+	(related_obstack, add_limits): Likewise.
+	(regmove_optimize): Call optimize_related_values.
+	Include gt-regmove.h.
+	(have_3addr_const_add): New variable.
+	* Makefile.in (gt-regmove.h): New rule.
+	(regmove.o): Depend on $(OPTABS_H) and gt-regmove.h.
+	(GTFILES): Add regmove.c.
+	* doc/invoke.texi: Document -foptimize-related-values.
+
+2005-09-15  J"orn Rennecke <joern.rennecke@st.com>
+
+	* regmove.c (discover_flags_reg): Use the PATTERN of an INSN.
+
+	* regmove.c (fixup_match_1): When moving a death note, check if
+	it needs changing into a REG_UNUSED note.
+
Index: gcc/loop-iv.c
===================================================================
--- gcc/loop-iv.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/loop-iv.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1357,7 +1357,7 @@
   rtx op0, op1;
 
   if (CONSTANT_P (rhs)
-      || REG_P (rhs))
+      || (REG_P (rhs) && !HARD_REGISTER_P (rhs)))
     return true;
 
   switch (GET_CODE (rhs))
@@ -1367,9 +1367,9 @@
       op0 = XEXP (rhs, 0);
       op1 = XEXP (rhs, 1);
       /* Allow reg + const sets only.  */
-      if (REG_P (op0) && CONSTANT_P (op1))
+      if (REG_P (op0) && !HARD_REGISTER_P (op0) && CONSTANT_P (op1))
 	return true;
-      if (REG_P (op1) && CONSTANT_P (op0))
+      if (REG_P (op1) && !HARD_REGISTER_P (op1) && CONSTANT_P (op0))
 	return true;
 
       return false;
Index: gcc/tree-ssa-forwprop.c
===================================================================
--- gcc/tree-ssa-forwprop.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/tree-ssa-forwprop.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -152,6 +152,133 @@
 /* Set to true if we delete EH edges during the optimization.  */
 static bool cfg_changed;
 
+/* Get the statement we can propagate from into NAME skipping
+   trivial copies.  Returns the statement which defines the
+   propagation source or NULL_TREE if there is no such one.
+   If SINGLE_USE_ONLY is set considers only sources which have
+   a single use chain up to NAME.  If SINGLE_USE_P is non-null,
+   it is set to whether the chain to NAME is a single use chain
+   or not.  SINGLE_USE_P is not written to if SINGLE_USE_ONLY is set.  */
+
+static tree
+get_prop_source_stmt (tree name, bool single_use_only, bool *single_use_p)
+{
+  bool single_use = true;
+
+  do {
+    tree def_stmt = SSA_NAME_DEF_STMT (name);
+
+    if (!has_single_use (name))
+      {
+	single_use = false;
+	if (single_use_only)
+	  return NULL_TREE;
+      }
+
+    /* If name is defined by a PHI node or is the default def, bail out.  */
+    if (TREE_CODE (def_stmt) != MODIFY_EXPR)
+      return NULL_TREE;
+
+    /* If name is not a simple copy destination, we found it.  */
+    if (TREE_CODE (TREE_OPERAND (def_stmt, 1)) != SSA_NAME)
+      {
+	if (!single_use_only && single_use_p)
+	  *single_use_p = single_use;
+
+	return def_stmt;
+      }
+
+    /* Continue searching the def of the copy source name.  */
+    name = TREE_OPERAND (def_stmt, 1);
+  } while (1);
+}
+
+/* Checks if the destination ssa name in DEF_STMT can be used as
+   propagation source.  Returns true if so, otherwise false.  */
+
+static bool
+can_propagate_from (tree def_stmt)
+{
+  tree rhs = TREE_OPERAND (def_stmt, 1);
+
+  /* If the rhs has side-effects we cannot propagate from it.  */
+  if (TREE_SIDE_EFFECTS (rhs))
+    return false;
+
+  /* If the rhs is a load we cannot propagate from it.  */
+  if (REFERENCE_CLASS_P (rhs))
+    return false;
+
+  /* We cannot propagate ssa names that occur in abnormal phi nodes.  */
+  switch (TREE_CODE_LENGTH (TREE_CODE (rhs)))
+    {
+    case 3:
+      if (TREE_OPERAND (rhs, 2) != NULL_TREE
+	  && TREE_CODE (TREE_OPERAND (rhs, 2)) == SSA_NAME
+	  && SSA_NAME_OCCURS_IN_ABNORMAL_PHI (TREE_OPERAND (rhs, 2)))
+	return false;
+    case 2:
+      if (TREE_OPERAND (rhs, 1) != NULL_TREE
+	  && TREE_CODE (TREE_OPERAND (rhs, 1)) == SSA_NAME
+	  && SSA_NAME_OCCURS_IN_ABNORMAL_PHI (TREE_OPERAND (rhs, 1)))
+	return false;
+    case 1:
+      if (TREE_OPERAND (rhs, 0) != NULL_TREE
+	  && TREE_CODE (TREE_OPERAND (rhs, 0)) == SSA_NAME
+	  && SSA_NAME_OCCURS_IN_ABNORMAL_PHI (TREE_OPERAND (rhs, 0)))
+	return false;
+      break;
+
+    default:
+      return false;
+    }
+
+  /* If the definition is a conversion of a pointer to a function type,
+     then we can not apply optimizations as some targets require function
+     pointers to be canonicalized and in this case this optimization could
+     eliminate a necessary canonicalization.  */
+  if ((TREE_CODE (rhs) == NOP_EXPR
+       || TREE_CODE (rhs) == CONVERT_EXPR)
+      && POINTER_TYPE_P (TREE_TYPE (TREE_OPERAND (rhs, 0)))
+      && TREE_CODE (TREE_TYPE (TREE_TYPE
+			        (TREE_OPERAND (rhs, 0)))) == FUNCTION_TYPE)
+    return false;
+
+  return true;
+}
+
+/* Combine OP0 CODE OP1 in the context of a COND_EXPR.  Returns
+   the folded result in a form suitable for COND_EXPR_COND or
+   NULL_TREE, if there is no suitable simplified form.  If
+   INVARIANT_ONLY is true only gimple_min_invariant results are
+   considered simplified.  */
+
+static tree
+combine_cond_expr_cond (enum tree_code code, tree type,
+			tree op0, tree op1, bool invariant_only)
+{
+  tree t;
+
+  gcc_assert (TREE_CODE_CLASS (code) == tcc_comparison);
+
+  t = fold_binary (code, type, op0, op1);
+  if (!t)
+    return NULL_TREE;
+
+  /* Require that we got a boolean type out if we put one in.  */
+  gcc_assert (TREE_CODE (TREE_TYPE (t)) == TREE_CODE (type));
+
+  /* Canonicalize the combined condition for use in a COND_EXPR.  */
+  t = canonicalize_cond_expr_cond (t);
+
+  /* Bail out if we required an invariant but didn't get one.  */
+  if (!t
+      || (invariant_only
+	  && !is_gimple_min_invariant (t)))
+    return NULL_TREE;
+
+  return t;
+}
 
 /* Given an SSA_NAME VAR, return true if and only if VAR is defined by
    a comparison.  */
@@ -193,8 +320,60 @@
 	   && TREE_CODE (TREE_OPERAND (cond, 0)) == SSA_NAME
 	   && CONSTANT_CLASS_P (TREE_OPERAND (cond, 1))
 	   && INTEGRAL_TYPE_P (TREE_TYPE (TREE_OPERAND (cond, 1)))))
+    {
+      /* We can do tree combining on SSA_NAME and comparison expressions.  */
+      if (COMPARISON_CLASS_P (cond))
+	{
+	  if (TREE_CODE (TREE_OPERAND (cond, 0)) == SSA_NAME)
+	    {
+	      tree tmp = NULL_TREE;
+	      tree name, def_stmt, rhs0 = NULL_TREE, rhs1 = NULL_TREE;
+	      bool single_use0_p = false, single_use1_p = false;
+
+	      /* For comparisons use the first operand, that is likely to
+		 simplify comparisons against constants.  */
+	      name = TREE_OPERAND (cond, 0);
+	      def_stmt = get_prop_source_stmt (name, false, &single_use0_p);
+	      if (def_stmt != NULL_TREE
+		  && can_propagate_from (def_stmt))
+		{
+		  tree op1 = TREE_OPERAND (cond, 1);
+		  rhs0 = TREE_OPERAND (def_stmt, 1);
+		  tmp = combine_cond_expr_cond (TREE_CODE (cond), boolean_type_node,
+						fold_convert (TREE_TYPE (op1), rhs0),
+						op1, !single_use0_p);
+		}
+
+	      /* If that wasn't successful, try the second operand.  */
+	      if (tmp == NULL_TREE
+		  && TREE_CODE (TREE_OPERAND (cond, 1)) == SSA_NAME)
+		{
+		  tree op0 = TREE_OPERAND (cond, 0);
+		  name = TREE_OPERAND (cond, 1);
+		  def_stmt = get_prop_source_stmt (name, false, &single_use1_p);
+		  if (def_stmt == NULL_TREE
+		      || !can_propagate_from (def_stmt))
     return NULL_TREE;
 
+		  rhs1 = TREE_OPERAND (def_stmt, 1);
+		  tmp = combine_cond_expr_cond (TREE_CODE (cond), boolean_type_node,
+						op0,
+						fold_convert (TREE_TYPE (op0), rhs1),
+						!single_use1_p);
+		}
+
+	      /* Extract the single variable used in the test into TEST_VAR.  */
+	      if (tmp) 
+		{
+		  *test_var_p = TREE_OPERAND (cond, 0);
+		  return tmp;
+		}
+	    }
+	}
+      
+      return NULL_TREE;
+    }
+
   /* Extract the single variable used in the test into TEST_VAR.  */
   if (cond_code == SSA_NAME)
     test_var = cond;
Index: gcc/common.opt
===================================================================
--- gcc/common.opt	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/common.opt	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,6 +1,7 @@
 ; Options for the language- and target-independent parts of the compiler.
 
 ; Copyright (C) 2003, 2004, 2005, 2006, 2007 Free Software Foundation, Inc.
+; Copyright (c) 2006  STMicroelectronics.
 ;
 ; This file is part of GCC.
 ;
@@ -635,6 +636,10 @@
 Common Report Var(flag_regmove)
 Do the full register move optimization pass
 
+foptimize-related-values
+Common Report Var(flag_optimize_related_values)
+Enable additional regmove optimization for base reg + offset expressions
+
 foptimize-sibling-calls
 Common Report Var(flag_optimize_sibling_calls)
 Optimize sibling and tail recursive calls
Index: gcc/ggc-zone.c
===================================================================
--- gcc/ggc-zone.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/ggc-zone.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -37,21 +37,6 @@
 #include "params.h"
 #include "bitmap.h"
 
-#ifdef ENABLE_VALGRIND_CHECKING
-# ifdef HAVE_VALGRIND_MEMCHECK_H
-#  include <valgrind/memcheck.h>
-# elif defined HAVE_MEMCHECK_H
-#  include <memcheck.h>
-# else
-#  include <valgrind.h>
-# endif
-#else
-/* Avoid #ifdef:s when we can help it.  */
-#define VALGRIND_DISCARD(x)
-#define VALGRIND_MALLOCLIKE_BLOCK(w,x,y,z)
-#define VALGRIND_FREELIKE_BLOCK(x,y)
-#endif
-
 /* Prefer MAP_ANON(YMOUS) to /dev/zero, since we don't need to keep a
    file open.  Prefer either to valloc.  */
 #ifdef HAVE_MMAP_ANON
@@ -787,7 +772,7 @@
   /* Pretend we don't have access to the allocated pages.  We'll enable
      access to smaller pieces of the area in ggc_alloc.  Discard the
      handle to avoid handle leak.  */
-  VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS (page, size));
+  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_NOACCESS (page, size));
 
   return page;
 }
@@ -903,7 +888,7 @@
 
   /* Mark the page as inaccessible.  Discard the handle to
      avoid handle leak.  */
-  VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS (entry->common.page,
+  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_NOACCESS (entry->common.page,
 					    SMALL_PAGE_SIZE));
 
   entry->next = entry->common.zone->free_pages;
@@ -978,18 +963,30 @@
   if (bin > NUM_FREE_BINS)
     {
       bin = 0;
-      VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (chunk, sizeof (struct alloc_chunk)));
+      VALGRIND_DISCARD (VALGRIND_MAKE_MEM_UNDEFINED (chunk,
+						     sizeof (struct
+							     alloc_chunk)));
       chunk->size = size;
       chunk->next_free = zone->free_chunks[bin];
-      VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS (ptr + sizeof (struct alloc_chunk),
-						size - sizeof (struct alloc_chunk)));
+      VALGRIND_DISCARD (VALGRIND_MAKE_MEM_NOACCESS (ptr
+						    + sizeof (struct
+							      alloc_chunk),
+						    size
+						    - sizeof (struct
+							      alloc_chunk)));
     }
   else
     {
-      VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (chunk, sizeof (struct alloc_chunk *)));
+      VALGRIND_DISCARD (VALGRIND_MAKE_MEM_UNDEFINED (chunk,
+						     sizeof (struct
+							     alloc_chunk *)));
       chunk->next_free = zone->free_chunks[bin];
-      VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS (ptr + sizeof (struct alloc_chunk *),
-						size - sizeof (struct alloc_chunk *)));
+      VALGRIND_DISCARD (VALGRIND_MAKE_MEM_NOACCESS (ptr
+						    + sizeof (struct
+							      alloc_chunk *),
+						    size
+						    - sizeof (struct
+							      alloc_chunk *)));
     }
 
   zone->free_chunks[bin] = chunk;
@@ -1213,16 +1210,16 @@
 
 #ifdef ENABLE_GC_CHECKING
   /* `Poison' the entire allocated object.  */
-  VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (result, size));
+  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_UNDEFINED (result, size));
   memset (result, 0xaf, size);
-  VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS (result + orig_size,
+  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_NOACCESS (result + orig_size,
 					    size - orig_size));
 #endif
 
   /* Tell Valgrind that the memory is there, but its content isn't
      defined.  The bytes at the end of the object are still marked
      unaccessible.  */
-  VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (result, orig_size));
+  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_UNDEFINED (result, orig_size));
 
   /* Keep track of how many bytes are being allocated.  This
      information is used in deciding when to collect.  */
@@ -1701,7 +1698,7 @@
 		{
 		  if (last_free)
 		    {
-		      VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (last_free,
+		      VALGRIND_DISCARD (VALGRIND_MAKE_MEM_UNDEFINED (last_free,
 								object
 								- last_free));
 		      poison_region (last_free, object - last_free);
@@ -1739,7 +1736,7 @@
 	{
 	  *spp = snext;
 #ifdef ENABLE_GC_CHECKING
-	  VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (sp->common.page, SMALL_PAGE_SIZE));
+	  VALGRIND_DISCARD (VALGRIND_MEM_UNDEFINED (sp->common.page, SMALL_PAGE_SIZE));
 	  /* Poison the page.  */
 	  memset (sp->common.page, 0xb5, SMALL_PAGE_SIZE);
 #endif
@@ -1748,7 +1745,7 @@
 	}
       else if (last_free)
 	{
-	  VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (last_free,
+	  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_UNDEFINED (last_free,
 						    object - last_free));
 	  poison_region (last_free, object - last_free);
 	  free_chunk (last_free, object - last_free, zone);
Index: gcc/ggc-page.c
===================================================================
--- gcc/ggc-page.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/ggc-page.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -31,18 +31,6 @@
 #include "timevar.h"
 #include "params.h"
 #include "tree-flow.h"
-#ifdef ENABLE_VALGRIND_CHECKING
-# ifdef HAVE_VALGRIND_MEMCHECK_H
-#  include <valgrind/memcheck.h>
-# elif defined HAVE_MEMCHECK_H
-#  include <memcheck.h>
-# else
-#  include <valgrind.h>
-# endif
-#else
-/* Avoid #ifdef:s when we can help it.  */
-#define VALGRIND_DISCARD(x)
-#endif
 
 /* Prefer MAP_ANON(YMOUS) to /dev/zero, since we don't need to keep a
    file open.  Prefer either to valloc.  */
@@ -688,7 +676,7 @@
   /* Pretend we don't have access to the allocated pages.  We'll enable
      access to smaller pieces of the area in ggc_alloc.  Discard the
      handle to avoid handle leak.  */
-  VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS (page, size));
+  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_NOACCESS (page, size));
 
   return page;
 }
@@ -932,7 +920,7 @@
 
   /* Mark the page as inaccessible.  Discard the handle to avoid handle
      leak.  */
-  VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS (entry->page, entry->bytes));
+  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_NOACCESS (entry->page, entry->bytes));
 
   set_page_table_entry (entry->page, NULL);
 
@@ -1207,7 +1195,7 @@
      exact same semantics in presence of memory bugs, regardless of
      ENABLE_VALGRIND_CHECKING.  We override this request below.  Drop the
      handle to avoid handle leak.  */
-  VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (result, object_size));
+  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_UNDEFINED (result, object_size));
 
   /* `Poison' the entire allocated object, including any padding at
      the end.  */
@@ -1215,14 +1203,14 @@
 
   /* Make the bytes after the end of the object unaccessible.  Discard the
      handle to avoid handle leak.  */
-  VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS ((char *) result + size,
+  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_NOACCESS ((char *) result + size,
 					    object_size - size));
 #endif
 
   /* Tell Valgrind that the memory is there, but its content isn't
      defined.  The bytes at the end of the object are still marked
      unaccessible.  */
-  VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (result, size));
+  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_UNDEFINED (result, size));
 
   /* Keep track of how many bytes are being allocated.  This
      information is used in deciding when to collect.  */
@@ -1357,11 +1345,11 @@
 
 #ifdef ENABLE_GC_CHECKING
   /* Poison the data, to indicate the data is garbage.  */
-  VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (p, size));
+  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_UNDEFINED (p, size));
   memset (p, 0xa5, size);
 #endif
   /* Let valgrind know the object is free.  */
-  VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS (p, size));
+  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_NOACCESS (p, size));
 
 #ifdef ENABLE_GC_ALWAYS_COLLECT
   /* In the completely-anal-checking mode, we do *not* immediately free
@@ -1814,11 +1802,12 @@
 		     so the exact same memory semantics is kept, in case
 		     there are memory errors.  We override this request
 		     below.  */
-		  VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (object, size));
+		  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_UNDEFINED (object,
+								 size));
 		  memset (object, 0xa5, size);
 
 		  /* Drop the handle to avoid handle leak.  */
-		  VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS (object, size));
+		  VALGRIND_DISCARD (VALGRIND_MAKE_MEM_NOACCESS (object, size));
 		}
 	    }
 	}
Index: gcc/sched-deps.c
===================================================================
--- gcc/sched-deps.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/sched-deps.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -976,6 +976,93 @@
 	  sched_analyze_reg (deps, FIRST_STACK_REG, mode, SET, insn);
 	}
 #endif
+
+	if (regno < FIRST_PSEUDO_REGISTER)
+	  {
+	    int i = hard_regno_nregs[regno][GET_MODE (x)];
+	    while (--i >= 0)
+	      {
+		rtx sets;
+  
+		/* We have to prevent extending the lifetime of
+		   CLASS_LIKELY_SPILLED registers, no matter where they are
+		   used within a basic block.  SH -m4-nofpu  -O -fschedule-insns
+		   testcase extracted from newlib/libm/math/k_rem_pio2.c :
+		void
+		__kernel_rem_pio2 ()
+		{
+		  int jz, jk, iq[20], i, j, k;
+		  double z, f[20], fq[20], q[20];
+		
+		  for (i = 0, j = jz, z = q[1]; j > 0; i++, j--)
+		    {
+		      iq[i] = (int) (z - 1.);
+		      z = q[0] + 1.;
+		    }
+		
+		  for (k = 1; iq[jk - k] == 0; k++);
+		
+		  for (i = 1; i <= 1 + k; i++)
+		    q[i] = 1.;
+		}
+		   Here, the problem is that a (reg:DF 0 r0) result from an
+		   sfunc conflicts with an R0_REGS requirement from a store
+		   that the scheduler has placed before the copy of the result
+		   to a pseudo.  */
+		if (next_nonnote_insn (insn)
+		    && !reload_completed && !fixed_regs[regno+i]
+		    && CLASS_LIKELY_SPILLED_P (REGNO_REG_CLASS (regno + i))
+		    && (sets = deps->reg_last[regno+i].sets))
+		  {
+		    rtx set = XEXP (sets, 0);
+		    rtx curr;
+
+		    for (curr = insn; curr != set;
+			 curr = prev_nonnote_insn (curr))
+		      CANT_MOVE (curr) = 1;
+		    CANT_MOVE (set) = 1;
+		    for (curr = next_nonnote_insn (set);
+			 curr != insn && SCHED_GROUP_P (curr);)
+		      curr = next_nonnote_insn (curr);
+		    for (; curr != insn; curr = next_nonnote_insn (curr))
+		      {
+			SCHED_GROUP_P (curr) = 1;
+			fixup_sched_groups (curr);
+		      }
+		    SCHED_GROUP_P (insn) = 1;
+		  }
+		SET_REGNO_REG_SET (reg_pending_uses, regno + i);
+	      }
+	  }
+	/* ??? Reload sometimes emits USEs and CLOBBERs of pseudos that
+	   it does not reload.  Ignore these as they have served their
+	   purpose already.  */
+	else if (regno >= deps->max_reg)
+	  {
+	    gcc_assert (GET_CODE (PATTERN (insn)) == USE
+			|| GET_CODE (PATTERN (insn)) == CLOBBER);
+	  }
+	else
+	  {
+	    SET_REGNO_REG_SET (reg_pending_uses, regno);
+
+	    /* Pseudos that are REG_EQUIV to something may be replaced
+	       by that during reloading.  We need only add dependencies for
+	       the address in the REG_EQUIV note.  */
+	    if (!reload_completed && get_reg_known_equiv_p (regno))
+	      {
+		rtx t = get_reg_known_value (regno);
+		if (MEM_P (t))
+		  sched_analyze_2 (deps, XEXP (t, 0), insn);
+	      }
+
+	    /* If the register does not already cross any calls, then add this
+	       insn to the sched_before_next_call list so that it will still
+	       not cross calls after scheduling.  */
+	    if (REG_N_CALLS_CROSSED (regno) == 0)
+	      deps->sched_before_next_call
+		= alloc_INSN_LIST (insn, deps->sched_before_next_call);
+	  }
 	return;
       }
 
Index: gcc/target-def.h
===================================================================
--- gcc/target-def.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/target-def.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -549,6 +549,10 @@
 #define TARGET_SECONDARY_RELOAD default_secondary_reload
 #endif
 
+#ifndef TARGET_MATCH_ADJUST
+#define TARGET_MATCH_ADJUST default_match_adjust
+#endif
+
 
 /* C++ specific.  */
 #ifndef TARGET_CXX_GUARD_TYPE
@@ -687,6 +691,7 @@
   TARGET_INVALID_UNARY_OP,			\
   TARGET_INVALID_BINARY_OP,			\
   TARGET_SECONDARY_RELOAD,			\
+  TARGET_MATCH_ADJUST,				\
   TARGET_CXX,					\
   TARGET_EXTRA_LIVE_ON_ENTRY,                    \
   TARGET_UNWIND_TABLES_DEFAULT,			\
Index: gcc/genchecksum.c
===================================================================
--- gcc/genchecksum.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/genchecksum.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -35,6 +35,7 @@
   unsigned char result[16];
   int i;
   
+  CYGPATH (file);
   f = fopen (file, "rb");
   if (!f)
     {
Index: gcc/output.h
===================================================================
--- gcc/output.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/output.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -3,6 +3,7 @@
    Copyright (C) 1987, 1991, 1994, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005, 2007
    Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -54,6 +55,9 @@
    any branches of variable length if possible.  */
 extern void shorten_branches (rtx);
 
+/* Returns the actual insn length without adjustment.  */
+int get_insn_current_length (rtx insn);
+
 /* Output assembler code for the start of a function,
    and initialize some of the variables in this file
    for the new function.  The label for the function and associated
Index: gcc/system.h
===================================================================
--- gcc/system.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/system.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -768,4 +768,29 @@
 
 #endif /* GCC >= 3.0 */
 
+#ifdef ENABLE_VALGRIND_CHECKING
+# ifdef HAVE_VALGRIND_MEMCHECK_H
+#  include <valgrind/memcheck.h>
+# elif defined HAVE_MEMCHECK_H
+#  include <memcheck.h>
+# else
+#  include <valgrind.h>
+# endif
+/* Compatibility macros to let valgrind 3.1 work.  */
+# ifndef VALGRIND_MAKE_MEM_NOACCESS
+#  define VALGRIND_MAKE_MEM_NOACCESS VALGRIND_MAKE_NOACCESS
+# endif
+# ifndef VALGRIND_MAKE_MEM_DEFINED
+#  define VALGRIND_MAKE_MEM_DEFINED VALGRIND_MAKE_READABLE
+# endif
+# ifndef VALGRIND_MAKE_MEM_UNDEFINED
+#  define VALGRIND_MAKE_MEM_UNDEFINED VALGRIND_MAKE_WRITABLE
+# endif
+#else
+/* Avoid #ifdef:s when we can help it.  */
+#define VALGRIND_DISCARD(x)
+#define VALGRIND_MALLOCLIKE_BLOCK(w,x,y,z)
+#define VALGRIND_FREELIKE_BLOCK(x,y)
+#endif
+
 #endif /* ! GCC_SYSTEM_H */
Index: gcc/cfgloop.h
===================================================================
--- gcc/cfgloop.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/cfgloop.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -103,10 +103,10 @@
   /* Auxiliary info specific to a pass.  */
   void *aux;
 
-  /* The probable number of times the loop is executed at runtime.
+  /* The number of times the latch of the loop is executed.
      This is an INTEGER_CST or an expression containing symbolic
      names.  Don't access this field directly:
-     number_of_iterations_in_loop computes and caches the computed
+     number_of_latch_executions computes and caches the computed
      information in this field.  */
   tree nb_iterations;
 
Index: gcc/config.gcc
===================================================================
--- gcc/config.gcc	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config.gcc	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,6 +1,7 @@
 # GCC target-specific configuration file.
 # Copyright 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007
 # Free Software Foundation, Inc.
+# Copyright (c) 2006  STMicroelectronics.
 
 #This file is part of GCC.
 
@@ -2016,7 +2017,8 @@
 				with_libgloss=yes
 				tm_file="${tm_file} sh/newlib.h"
 			fi
-			tm_file="${tm_file} sh/embed-elf.h"
+			tm_file="${tm_file} sh/embed-elf.h sh/superh.h"
+			tmake_file="${tmake_file} sh/t-superh"
 			extra_options="${extra_options} sh/superh.opt" ;;
 	*)		if test x$with_newlib = xyes \
 			   && test x$with_libgloss = xyes; then
@@ -2090,7 +2092,7 @@
 		esac
 		tm_defines="$tm_defines STRICT_NOFPU=1"
 	fi
-	sh_cpu_default="`echo $with_cpu|sed s/^m/sh/|tr A-Z_ a-z-`"
+	sh_cpu_default="`echo $with_cpu|sed 's/^m/sh/'|tr ABCDEFGHIJKLMNOPQRSTUVWXYZ_ abcdefghijklmnopqrstuvwxyz-`"
 	case $sh_cpu_default in
 	sh5-64media-nofpu | sh5-64media | \
 	  sh5-32media-nofpu | sh5-32media | sh5-compact-nofpu | sh5-compact | \
@@ -2113,7 +2115,7 @@
 		*) sh_multilibs=m1,m2,m2e,m4,m4-single,m4-single-only,m2a,m2a-single ;;
 		esac
 		if test x$with_fp = xno; then
-			sh_multilibs="`echo $sh_multilibs|sed -e s/m4/sh4-nofpu/ -e s/,m4-[^,]*//g -e s/,m[23]e// -e s/m2a,m2a-single/m2a-nofpu/ -e s/m5-..m....,//g`"
+			sh_multilibs="`echo $sh_multilibs|sed -e s/m4/sh4-nofpu/ -e 's/,m4-[^,]*//g' -e 's/,m[23]e//' -e s/m2a,m2a-single/m2a-nofpu/ -e s/m5-..m....,//g`"
 		fi
 	fi
 	target_cpu_default=SELECT_`echo ${sh_cpu_default}|tr abcdefghijklmnopqrstuvwxyz- ABCDEFGHIJKLMNOPQRSTUVWXYZ_`
@@ -2122,7 +2124,7 @@
 	for sh_multilib in ${sh_multilibs}; do
 		case ${sh_multilib} in
 		sh1 | sh2 | sh2e | sh3 | sh3e | \
-		sh4 | sh4-single | sh4-single-only | sh4-nofpu | \
+		sh4 | sh4-single | sh4-single-only | sh4-nofpu | sh4-300 | \
 		sh4a | sh4a-single | sh4a-single-only | sh4a-nofpu | sh4al | \
 		sh2a | sh2a-single | sh2a-single-only | sh2a-nofpu | \
 		sh5-64media | sh5-64media-nofpu | \
Index: gcc/Makefile.in
===================================================================
--- gcc/Makefile.in	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/Makefile.in	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -4,6 +4,7 @@
 # Copyright (C) 1987, 1988, 1990, 1991, 1992, 1993, 1994, 1995, 1996,
 # 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007,
 # 2008 Free Software Foundation, Inc.
+# Copyright (c) 2006  STMicroelectronics.
 
 #This file is part of GCC.
 
@@ -637,7 +638,7 @@
 LIBCONVERT =
 
 # Control whether header files are installed.
-INSTALL_HEADERS=install-headers install-mkheaders
+INSTALL_HEADERS=install-headers #broken: install-mkheaders
 
 # Control whether Info documentation is built and installed.
 BUILD_INFO = @BUILD_INFO@
@@ -1416,7 +1417,7 @@
 #
 # Build libgcc.a.
 
-LIB2ADD = $(LIB2FUNCS_EXTRA)
+LIB2ADD = $(LIB2FUNCS_EXTRA) @extra_libgcc_srcs@
 LIB2ADD_ST = $(LIB2FUNCS_STATIC_EXTRA)
 
 libgcc.mk: config.status Makefile mklibgcc $(LIB2ADD) $(LIB2ADD_ST) specs \
@@ -1777,6 +1778,8 @@
 
 dumpvers: dumpvers.c
 
+version.o: $(OBJS:version.o=)
+
 version.o: version.c version.h $(DATESTAMP) $(BASEVER) $(DEVPHASE)
 	$(CC) $(ALL_CFLAGS) $(ALL_CPPFLAGS) \
 	-DBASEVER=$(BASEVER_s) -DDATESTAMP=$(DATESTAMP_s) \
@@ -2145,7 +2148,7 @@
    coretypes.h intl.h
 targhooks.o : targhooks.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TREE_H) \
    $(EXPR_H) $(TM_H) $(RTL_H) $(TM_P_H) $(FUNCTION_H) output.h toplev.h \
-   $(MACHMODE_H) $(TARGET_DEF_H) $(TARGET_H) $(GGC_H) gt-targhooks.h \
+   $(MACHMODE_H) $(TARGET_DEF_H) $(TARGET_H) $(GGC_H) $(REGS_H) gt-targhooks.h \
    $(OPTABS_H)
 
 toplev.o : toplev.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(TREE_H) \
@@ -2547,7 +2550,8 @@
 regmove.o : regmove.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \
    insn-config.h $(TIMEVAR_H) tree-pass.h \
    $(RECOG_H) output.h $(REGS_H) hard-reg-set.h $(FLAGS_H) $(FUNCTION_H) \
-   $(EXPR_H) $(BASIC_BLOCK_H) toplev.h $(TM_P_H) except.h reload.h
+   $(EXPR_H) $(BASIC_BLOCK_H) toplev.h $(TM_P_H) except.h reload.h \
+   $(OPTABS_H) gt-regmove.h
 ddg.o : ddg.c $(DDG_H) $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TARGET_H) \
    toplev.h $(RTL_H) $(TM_P_H) $(REGS_H) $(FUNCTION_H) \
    $(FLAGS_H) insn-config.h $(INSN_ATTR_H) except.h $(RECOG_H) \
@@ -2871,7 +2875,7 @@
   $(srcdir)/tree-iterator.c $(srcdir)/gimplify.c \
   $(srcdir)/tree-chrec.h $(srcdir)/tree-vect-generic.c \
   $(srcdir)/tree-ssa-operands.h $(srcdir)/tree-ssa-operands.c \
-  $(srcdir)/tree-profile.c $(srcdir)/tree-nested.c \
+  $(srcdir)/tree-profile.c $(srcdir)/tree-nested.c $(srcdir)/regmove.c \
   $(srcdir)/ipa-reference.c $(srcdir)/tree-ssa-structalias.h \
   $(srcdir)/tree-ssa-structalias.c \
   $(srcdir)/c-pragma.h $(srcdir)/omp-low.c \
@@ -2900,7 +2904,7 @@
 gt-expr.h gt-sdbout.h gt-optabs.h gt-bitmap.h gt-dojump.h \
 gt-dwarf2out.h gt-dwarf2asm.h \
 gt-dbxout.h \
-gtype-c.h gt-cfglayout.h \
+gtype-c.h gt-cfglayout.h gt-regmove.h \
 gt-tree-mudflap.h gt-tree-vect-generic.h \
 gt-tree-profile.h gt-tree-ssa-address.h \
 gt-tree-ssanames.h gt-tree-iterator.h gt-gimplify.h \
@@ -3073,6 +3077,7 @@
   -DLOCAL_INCLUDE_DIR=\"$(local_includedir)\" \
   -DCROSS_INCLUDE_DIR=\"$(CROSS_SYSTEM_HEADER_DIR)\" \
   -DTOOL_INCLUDE_DIR=\"$(gcc_tooldir)/include\" \
+  -DSTANDARD_EXEC_PREFIX=\"$(libdir)/gcc/\" \
   @TARGET_SYSTEM_ROOT_DEFINE@
 
 cppdefault.o: cppdefault.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \
@@ -3993,6 +3998,7 @@
 	@echo "set HOSTCFLAGS \"$(CFLAGS)\"" >> ./tmp0
 	@echo "set TESTING_IN_BUILD_TREE 1" >> ./tmp0
 	@echo "set HAVE_LIBSTDCXX_V3 1" >> ./tmp0
+	@echo "set GCC_EXEC_PREFIX \"$(libdir)/gcc/\"" >> ./tmp0
 # If newlib has been configured, we need to pass -B to gcc so it can find
 # newlib's crt0.o if it exists.  This will cause a "path prefix not used"
 # message if it doesn't, but the testsuite is supposed to ignore the message -
Index: gcc/basic-block.h
===================================================================
--- gcc/basic-block.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/basic-block.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -861,6 +861,9 @@
 extern struct edge_list *pre_edge_lcm (int, sbitmap *, sbitmap *,
 				       sbitmap *, sbitmap *, sbitmap **,
 				       sbitmap **);
+extern struct edge_list *pre_edge_lcm_avs (int, sbitmap *, sbitmap *,
+					   sbitmap *, sbitmap *, sbitmap *,
+					   sbitmap *, sbitmap **, sbitmap **);
 extern struct edge_list *pre_edge_rev_lcm (int, sbitmap *,
 					   sbitmap *, sbitmap *,
 					   sbitmap *, sbitmap **,
@@ -1169,7 +1172,7 @@
 
 extern bool insns_match_p (rtx, rtx, struct equiv_info *);
 extern int struct_equiv_block_eq (int, struct equiv_info *);
-extern bool struct_equiv_init (int, struct equiv_info *);
+extern bool struct_equiv_init (int, struct equiv_info *, bool);
 extern bool rtx_equiv_p (rtx *, rtx, int, struct equiv_info *);
 
 /* In cfgrtl.c */
Index: gcc/c-incpath.c
===================================================================
--- gcc/c-incpath.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/c-incpath.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2,6 +2,7 @@
    Copyright (C) 1986, 1987, 1989, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007
    Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
    Broken out of cppinit.c and cppfiles.c and rewritten Mar 2003.
 
@@ -127,6 +128,7 @@
 		    const char *imultilib, int cxx_stdinc)
 {
   const struct default_include *p;
+  int relocated = cpp_relocated();
   size_t len;
 
   if (iprefix && (len = cpp_GCC_INCLUDE_DIR_len) != 0)
@@ -163,6 +165,22 @@
 	  /* Should this directory start with the sysroot?  */
 	  if (sysroot && p->add_sysroot)
 	    str = concat (sysroot, p->fname, NULL);
+	  else if (!p->add_sysroot && relocated &&
+		   p->fname[0] != '/')
+	    {
+	      /* If the compiler is relocated, and this is a configured
+		 prefix relative path, then we use gcc_exec_prefix instead
+		 of the configured prefix.  */
+	      if (strncmp (p->fname, cpp_STANDARD_EXEC_PREFIX,
+			   cpp_STANDARD_EXEC_PREFIX_len) == 0)
+		{
+		  str = concat (gcc_exec_prefix, p->fname
+				+ cpp_STANDARD_EXEC_PREFIX_len, NULL);
+		  str = update_path (str, p->component);
+		}
+	      else
+		str = update_path (p->fname, p->component);
+	    }
 	  else
 	    str = update_path (p->fname, p->component);
 
@@ -347,8 +365,23 @@
   char* c;
   for (c = path; *c; c++)
     if (*c == '\\') *c = '/';
+
+/* The native stat() on NT4 accepts trailing slashes, but not
+    backslashes.  Mingw's stat on Windows 2000/XP doesn't accept
+    either trailing slashes or backslashes.  (C:/ and C:\ are of
+    course fine.)  It is harmless to remove trailing slashes on
+    systems that do accept them, so we do it unconditionally.  */
+  while (c > path+1 && c[-1] == '/')
+    c--;
+  if (c == path+2
+      && ISALPHA (path[0]) && path[1] == ':' && path[2] == '/')
+    c++;
+  if (*c == '/')
+    *c = '\0';
 #endif
 
+  CYGPATH_REPLACE (&path);
+
   p = XNEW (cpp_dir);
   p->next = NULL;
   p->name = path;
Index: gcc/struct-equiv.c
===================================================================
--- gcc/struct-equiv.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/struct-equiv.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -916,12 +916,31 @@
     }
   else if (INSN_P (i1))
     {
+      rtx s1 = single_set (i1);
+      rtx s2 = single_set (i2);
+
       if (! set_dest_equiv_p (PATTERN (i1), PATTERN (i2), info))
 	{
 	  cancel_changes (0);
 	  return false;
 	}
+
+      if (reload_completed && s1 && s2)
+	{
+	  s1 = SET_SRC (s1);
+	  s2 = SET_SRC (s2);
+
+	  if (REG_P (s1) && REG_P (s2))
+	    {
+	      unsigned x_regno = REGNO (s1);
+	      unsigned y_regno = REGNO (s2);
+
+	      if (REGNO_REG_CLASS (x_regno) != REGNO_REG_CLASS(y_regno))
+		return false;
+	    }
     }
+    }
+
   rvalue_change_start = num_validated_changes ();
   struct_equiv_make_checkpoint (&before_rvalue_change, info);
   /* Check death_notes_match_p *after* the inputs have been processed,
@@ -983,38 +1002,47 @@
   return false;
 }
 
-/* Set up mode and register information in INFO.  Return true for success.  */
-bool
-struct_equiv_init (int mode, struct equiv_info *info)
+static bool
+struct_equiv_regs_eq_p (struct equiv_info *info)
 {
-  if ((info->x_block->flags | info->y_block->flags) & BB_DIRTY)
-    update_life_info_in_dirty_blocks (UPDATE_LIFE_GLOBAL_RM_NOTES,
-				      (PROP_DEATH_NOTES
-				       | ((mode & CLEANUP_POST_REGSTACK)
-					  ? PROP_POST_REGSTACK : 0)));
-  if (!REG_SET_EQUAL_P (info->x_block->il.rtl->global_live_at_end,
-			info->y_block->il.rtl->global_live_at_end))
-    {
 #ifdef STACK_REGS
-      unsigned rn;
-
-      if (!(mode & CLEANUP_POST_REGSTACK))
-	return false;
-      /* After reg-stack.  Remove bogus live info about stack regs.  N.B.
-	 these regs are not necessarily all dead - we swap random bogosity
-	 against constant bogosity.  However, clearing these bits at
-	 least makes the regsets comparable.  */
-      for (rn = FIRST_STACK_REG; rn <= LAST_STACK_REG; rn++)
+  if (info->mode & CLEANUP_POST_REGSTACK)
 	{
-	  CLEAR_REGNO_REG_SET (info->x_block->il.rtl->global_live_at_end, rn);
-	  CLEAR_REGNO_REG_SET (info->y_block->il.rtl->global_live_at_end, rn);
+      regset_head diff;
+      unsigned regnum;
+      bitmap_iterator rsi;
+
+      INIT_REG_SET (&diff);
+      bitmap_xor (&diff,
+		  info->x_block->il.rtl->global_live_at_end,
+		  info->y_block->il.rtl->global_live_at_end);
+      EXECUTE_IF_SET_IN_BITMAP (&diff, 0, regnum, rsi)
+	{
+	  if (regnum < FIRST_STACK_REG || regnum > LAST_STACK_REG)
+	    return false;
+	}
+      return true;
 	}
-      if (!REG_SET_EQUAL_P (info->x_block->il.rtl->global_live_at_end,
-			    info->y_block->il.rtl->global_live_at_end))
 #endif
+  return (REG_SET_EQUAL_P (info->x_block->il.rtl->global_live_at_end,
+			   info->y_block->il.rtl->global_live_at_end));
+}
+
+/* Set up mode and register information in INFO.  Return true for success.
+   Nonzero CHECK_REGS_EQ indicates that we might be called with blocks that
+   have non-matching successor sets, and thus need to check their live_at_end
+   regsets for match in the first pass.  */
+bool
+struct_equiv_init (int mode, struct equiv_info *info, bool check_regs_eq)
+{
+  info->mode = mode;
+  if (check_regs_eq && (mode & STRUCT_EQUIV_START))
+    {
+      if (!struct_equiv_regs_eq_p (info))
 	return false;
     }
-  info->mode = mode;
+  else if (mode & STRUCT_EQUIV_FINAL)
+    gcc_assert (struct_equiv_regs_eq_p (info));
   if (mode & STRUCT_EQUIV_START)
     {
       info->x_input = info->y_input = info->input_reg = NULL_RTX;
@@ -1096,8 +1124,7 @@
       x_stop = info->cur.x_start;
       y_stop = info->cur.y_start;
     }
-  if (!struct_equiv_init (mode, info))
-    gcc_unreachable ();
+  struct_equiv_init (mode, info, false);
 
   /* Skip simple jumps at the end of the blocks.  Complex jumps still
      need to be compared for equivalence, which we'll do below.  */
Index: gcc/config/i386/i386.h
===================================================================
--- gcc/config/i386/i386.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/i386/i386.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2207,7 +2207,7 @@
    is the set of hard registers live at the point where the insn(s)
    are to be inserted.  */
 
-#define EMIT_MODE_SET(ENTITY, MODE, HARD_REGS_LIVE) 			\
+#define EMIT_MODE_SET(ENTITY, MODE, FLIP, HARD_REGS_LIVE)		\
   ((MODE) != I387_CW_ANY && (MODE) != I387_CW_UNINITIALIZED		\
    ? emit_i387_cw_initialization (MODE), 0				\
    : 0)
Index: gcc/config/sh/t-mlib-sh4-300
===================================================================
--- gcc/config/sh/t-mlib-sh4-300	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/t-mlib-sh4-300	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1 @@
+ML_sh4_300=m4-300/
Index: gcc/config/sh/sh-protos.h
===================================================================
--- gcc/config/sh/sh-protos.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/sh-protos.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -4,6 +4,7 @@
    Free Software Foundation, Inc.
    Contributed by Steve Chamberlain (sac@cygnus.com).
    Improved by Jim Wilson (wilson@cygnus.com).
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -25,8 +26,13 @@
 #define GCC_SH_PROTOS_H
 
 enum sh_function_kind {
-  /* A function with normal C ABI  */
+  /* A function with normal C ABI, or an SH1..SH4 sfunc that may resolved via
+     a PLT.  */
   FUNCTION_ORDINARY,
+  /* A function that is a bit large to put it in every calling dso, but that's
+     typically used often enough so that calling via GOT makes sense for
+     speed.  */
+  SFUNC_FREQUENT,
   /* A special function that guarantees that some otherwise call-clobbered
      registers are not clobbered.  These can't go through the SH5 resolver,
      because it only saves argument passing registers.  */
@@ -53,6 +59,7 @@
 extern const char *output_far_jump (rtx, rtx);
 
 extern struct rtx_def *sfunc_uses_reg (rtx);
+extern int sh_jump_align (rtx);
 extern int barrier_align (rtx);
 extern int sh_loop_align (rtx);
 extern int fp_zero_operand (rtx);
@@ -68,6 +75,10 @@
 extern void output_pic_addr_const (FILE *, rtx);
 extern int expand_block_move (rtx *);
 extern int prepare_move_operands (rtx[], enum machine_mode mode);
+extern enum rtx_code prepare_cbranch_operands (rtx *, enum machine_mode mode,
+					       enum rtx_code comparison);
+extern void expand_cbranchsi4 (rtx *operands, enum rtx_code comparison, int);
+extern bool expand_cbranchdi4 (rtx *operands, enum rtx_code comparison);
 extern void from_compare (rtx *, int);
 extern int shift_insns_rtx (rtx);
 extern void gen_ashift (int, int, rtx);
@@ -109,7 +120,11 @@
 extern void expand_df_unop (rtx (*)(rtx, rtx, rtx), rtx *);
 extern void expand_df_binop (rtx (*)(rtx, rtx, rtx, rtx), rtx *);
 extern void expand_fp_branch (rtx (*)(void), rtx (*)(void));
-extern int sh_insn_length_adjustment (rtx);
+extern void expand_sfunc_unop (enum machine_mode, rtx (*) (rtx, rtx),
+			       const char *, enum rtx_code code, rtx *);
+extern void expand_sfunc_binop (enum machine_mode, rtx (*) (rtx, rtx),
+				const char *, enum rtx_code code, rtx *);
+extern int sh_insn_length_adjustment (rtx, const int, const int);
 extern int sh_can_redirect_branch (rtx, rtx);
 extern void sh_expand_unop_v2sf (enum rtx_code, rtx, rtx);
 extern void sh_expand_binop_v2sf (enum rtx_code, rtx, rtx, rtx);
@@ -127,6 +142,8 @@
 extern int sh_media_register_for_return (void);
 extern void sh_expand_prologue (void);
 extern void sh_expand_epilogue (bool);
+extern void sh_expand_float_cbranch (rtx operands[4]);
+extern void sh_expand_float_scc (rtx operands[4]);
 extern int sh_need_epilogue (void);
 extern void sh_set_return_address (rtx, rtx);
 extern int initial_elimination_offset (int, int);
@@ -146,6 +163,7 @@
 #ifdef HARD_CONST
 extern void fpscr_set_from_mem (int, HARD_REG_SET);
 #endif
+extern void emit_fpu_flip (void);
 
 extern void sh_pr_interrupt (struct cpp_reader *);
 extern void sh_pr_trapa (struct cpp_reader *);
@@ -158,17 +176,26 @@
 extern int sh_pass_in_reg_p (CUMULATIVE_ARGS *, enum machine_mode, tree);
 extern void sh_init_cumulative_args (CUMULATIVE_ARGS *, tree, rtx, tree, signed int, enum machine_mode);
 extern bool sh_promote_prototypes (tree);
+extern rtx sh_dwarf_register_span (rtx);
 
 extern rtx replace_n_hard_rtx (rtx, rtx *, int , int);
 extern int shmedia_cleanup_truncate (rtx *, void *);
 
 extern int sh_contains_memref_p (rtx);
+extern int sh_loads_bankedreg_p (rtx);
+
 extern rtx shmedia_prepare_call_address (rtx fnaddr, int is_sibcall);
 struct secondary_reload_info;
 extern enum reg_class sh_secondary_reload (bool, rtx, enum reg_class,
 					   enum machine_mode,
 					   struct secondary_reload_info *);
 
+extern bool sh_cfun_trap_exit_p (void);
+
+extern int sh_match_adjust (rtx, int);
+
+extern int sh_asm_count (const char *, int *);
+
 #endif /* ! GCC_SH_PROTOS_H */
 
 #ifdef SYMBIAN
Index: gcc/config/sh/linux.h
===================================================================
--- gcc/config/sh/linux.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/linux.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -33,6 +33,12 @@
    %{pthread:-D_REENTRANT -D_PTHREADS} \
 "
 
+/* Enable tls support if the assembler supports it. */
+#ifdef HAVE_AS_TLS
+#undef TARGET_HAVE_TLS
+#define TARGET_HAVE_TLS true
+#endif
+
 #define TARGET_OS_CPP_BUILTINS() \
   do						\
     {						\
Index: gcc/config/sh/linux-atomic.asm
===================================================================
--- gcc/config/sh/linux-atomic.asm	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/linux-atomic.asm	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,4 +1,5 @@
 /* Copyright (C) 2006 Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
    This file is part of GCC.
 
@@ -35,6 +36,7 @@
 
 #if ! __SH5__
 
+#ifdef	DB_ST40300_BUG_WORKAROUND
 #define ATOMIC_TEST_AND_SET(N,T) \
 	.global	__sync_lock_test_and_set_##N; \
 	HIDDEN_FUNC(__sync_lock_test_and_set_##N); \
@@ -47,19 +49,38 @@
 0:	mov.##T	@r4, r2; \
 	mov.##T	r5, @r4; \
 1:	mov	r1, r15; \
+	nop; \
 	rts; \
 	 mov	r2, r0; \
 	ENDFUNC(__sync_lock_test_and_set_##N)
+#else
+#define ATOMIC_TEST_AND_SET(N,T) \
+	.global	__sync_lock_test_and_set_##N; \
+	HIDDEN_FUNC(__sync_lock_test_and_set_##N); \
+	.align	2; \
+__sync_lock_test_and_set_##N:; \
+	mova	1f, r0; \
+	nop; \
+	mov	r15, r1; \
+	mov	#(0f-1f), r15; \
+0:	mov.##T	@r4, r2; \
+	mov.##T	r5, @r4; \
+1:	mov	r1, r15; \
+	rts; \
+	mov	r2, r0; \
+	ENDFUNC(__sync_lock_test_and_set_##N)
+#endif
 
 ATOMIC_TEST_AND_SET (1,b)
 ATOMIC_TEST_AND_SET (2,w)
 ATOMIC_TEST_AND_SET (4,l)
 
-#define ATOMIC_COMPARE_AND_SWAP(N,T) \
-	.global	__sync_compare_and_swap_##N; \
-	HIDDEN_FUNC(__sync_compare_and_swap_##N); \
+#ifdef	DB_ST40300_BUG_WORKAROUND
+#define ATOMIC_COMPARE_AND_SWAP(OP,N,T) \
+	.global	__sync_##OP##_compare_and_swap_##N; \
+	HIDDEN_FUNC(__sync_##OP##_compare_and_swap_##N); \
 	.align	2; \
-__sync_compare_and_swap_##N:; \
+__sync_##OP##_compare_and_swap_##N:; \
 	mova	1f, r0; \
 	nop; \
 	mov	r15, r1; \
@@ -69,34 +90,101 @@
 	bf	1f; \
 	mov.##T	r6, @r4; \
 1:	mov	r1, r15; \
+	nop	       ; \
 	rts; \
+	.ifc OP, bool; \
+	 movt r0; \
+	.else; \
 	 mov	r2, r0; \
-	ENDFUNC(__sync_compare_and_swap_##N)
-
-ATOMIC_COMPARE_AND_SWAP (1,b)
-ATOMIC_COMPARE_AND_SWAP (2,w)
-ATOMIC_COMPARE_AND_SWAP (4,l)
+	.endif; \
+	ENDFUNC(__sync_##OP##_compare_and_swap_##N)
+#else
+#define ATOMIC_COMPARE_AND_SWAP(OP,N,T) \
+	.global	__sync_##OP##_compare_and_swap_##N; \
+	HIDDEN_FUNC(__sync_##OP##_compare_and_swap_##N); \
+	.align	2; \
+__sync_##OP##_compare_and_swap_##N:; \
+	mova	1f, r0; \
+	nop; \
+	mov	r15, r1; \
+	mov	#(0f-1f), r15; \
+0:	mov.##T	@r4, r2; \
+	cmp/eq	r2, r5; \
+	bf	1f; \
+	mov.##T	r6, @r4; \
+1:	mov	r1, r15; \
+	rts; \
+	.ifc OP, bool; \
+	 movt r0; \
+	.else; \
+	 mov r2, r0; \
+	.endif; \
+	ENDFUNC(__sync_##OP##_compare_and_swap_##N)	
+#endif
+	
+ATOMIC_COMPARE_AND_SWAP (bool,1,b)
+ATOMIC_COMPARE_AND_SWAP (bool,2,w)
+ATOMIC_COMPARE_AND_SWAP (bool,4,l)
+
+ATOMIC_COMPARE_AND_SWAP (val,1,b)
+ATOMIC_COMPARE_AND_SWAP (val,2,w)
+ATOMIC_COMPARE_AND_SWAP (val,4,l)
 
+#ifdef	DB_ST40300_BUG_WORKAROUND	
 #define ATOMIC_FETCH_AND_OP(OP,N,T) \
 	.global	__sync_fetch_and_##OP##_##N; \
 	HIDDEN_FUNC(__sync_fetch_and_##OP##_##N); \
 	.align	2; \
 __sync_fetch_and_##OP##_##N:; \
 	mova	1f, r0; \
+	nop; \
 	mov	r15, r1; \
 	mov	#(0f-1f), r15; \
 0:	mov.##T	@r4, r2; \
-	OP	r2, r5; \
-	mov.##T	r5, @r4; \
+	mov	r2, r3; \
+	OP	r5, r3; \
+	mov.##T	r3, @r4; \
+1:	mov	r1, r15; \
+	nop 	       ; \
+	rts; \
+	.if N == 4 ; \
+	 mov	r2, r0; \
+	.else; \
+	 extu.##T r2, r0; \
+	.endif; \
+	ENDFUNC(__sync_fetch_and_##OP##_##N)
+#else
+#define ATOMIC_FETCH_AND_OP(OP,N,T) \
+	.global	__sync_fetch_and_##OP##_##N; \
+	HIDDEN_FUNC(__sync_fetch_and_##OP##_##N); \
+	.align	2; \
+__sync_fetch_and_##OP##_##N:; \
+	mova	1f, r0; \
+	nop; \
+	mov	r15, r1; \
+	mov	#(0f-1f), r15; \
+0:	mov.##T	@r4, r2; \
+	mov	r2, r3; \
+	OP	r5, r3; \
+	mov.##T	r3, @r4; \
 1:	mov	r1, r15; \
 	rts; \
+	.if N == 4 ; \
 	 mov	r2, r0; \
+	.else; \
+	 extu.##T r2, r0; \
+	.endif; \
 	ENDFUNC(__sync_fetch_and_##OP##_##N)
+#endif
 
 ATOMIC_FETCH_AND_OP(add,1,b)
 ATOMIC_FETCH_AND_OP(add,2,w)
 ATOMIC_FETCH_AND_OP(add,4,l)
 
+ATOMIC_FETCH_AND_OP(sub,1,b)
+ATOMIC_FETCH_AND_OP(sub,2,w)
+ATOMIC_FETCH_AND_OP(sub,4,l)
+
 ATOMIC_FETCH_AND_OP(or,1,b)
 ATOMIC_FETCH_AND_OP(or,2,w)
 ATOMIC_FETCH_AND_OP(or,4,l)
@@ -109,6 +197,7 @@
 ATOMIC_FETCH_AND_OP(xor,2,w)
 ATOMIC_FETCH_AND_OP(xor,4,l)
 
+#ifdef	DB_ST40300_BUG_WORKAROUND	
 #define ATOMIC_FETCH_AND_COMBOP(OP,OP0,OP1,N,T) \
 	.global	__sync_fetch_and_##OP##_##N; \
 	HIDDEN_FUNC(__sync_fetch_and_##OP##_##N); \
@@ -119,20 +208,158 @@
 	mov	r15, r1; \
 	mov	#(0f-1f), r15; \
 0:	mov.##T	@r4, r2; \
-	OP0	r2, r5; \
-	OP1	r5, r5; \
-	mov.##T	r5, @r4; \
+	OP0	r2, r3; \
+	OP1	r5, r3; \
+	mov.##T	r3, @r4; \
+1:	mov	r1, r15; \
+	nop; \
+	rts; \
+	.if N == 4 ; \
+	 mov	r2, r0; \
+	.else; \
+	 extu.##T r2, r0; \
+	.endif; \
+	ENDFUNC(__sync_fetch_and_##OP##_##N)
+#else
+#define ATOMIC_FETCH_AND_COMBOP(OP,OP0,OP1,N,T) \	
+	.global	__sync_fetch_and_##OP##_##N; \
+	HIDDEN_FUNC(__sync_fetch_and_##OP##_##N); \
+	.align	2; \
+__sync_fetch_and_##OP##_##N:; \
+	mova	1f, r0; \
+	nop; \
+	mov	r15, r1; \
+	mov	#(0f-1f), r15; \
+0:	mov.##T	@r4, r2; \
+	OP0	r2, r3; \
+	OP1	r5, r3; \
+	mov.##T	r3, @r4; \
 1:	mov	r1, r15; \
 	rts; \
+	.if N == 4 ; \
 	 mov	r2, r0; \
+	.else; \
+	 extu.##T r2, r0; \
+	.endif; \
 	ENDFUNC(__sync_fetch_and_##OP##_##N)
+#endif	
 
-ATOMIC_FETCH_AND_COMBOP(sub,sub,neg,1,b)
-ATOMIC_FETCH_AND_COMBOP(sub,sub,neg,2,w)
-ATOMIC_FETCH_AND_COMBOP(sub,sub,neg,4,l)
-
-ATOMIC_FETCH_AND_COMBOP(nand,and,not,1,b)
-ATOMIC_FETCH_AND_COMBOP(nand,and,not,2,w)
-ATOMIC_FETCH_AND_COMBOP(nand,and,not,4,l)
+ATOMIC_FETCH_AND_COMBOP(nand,not,and,1,b)
+ATOMIC_FETCH_AND_COMBOP(nand,not,and,2,w)
+ATOMIC_FETCH_AND_COMBOP(nand,not,and,4,l)
+
+#ifdef	DB_ST40300_BUG_WORKAROUND
+#define ATOMIC_OP_AND_FETCH(OP,N,T) \
+	.global	__sync_##OP##_and_fetch_##N; \
+	HIDDEN_FUNC(__sync_##OP##_and_fetch_##N); \
+	.align	2; \
+__sync_##OP##_and_fetch_##N:; \
+	mova	1f, r0; \
+	mov	r15, r1; \
+	mov	#(0f-1f), r15; \
+0:	mov.##T	@r4, r2; \
+	OP	r5, r2; \
+	mov.##T	r2, @r4; \
+1:	mov	r1, r15; \
+	nop; \
+	rts; \
+	.if N == 4 ; \
+	 mov	r2, r0; \
+	.else; \
+	 extu.##T r2, r0; \
+	.endif; \
+	ENDFUNC(__sync_##OP##_and_fetch_##N)
+#else
+#define ATOMIC_OP_AND_FETCH(OP,N,T) \
+	.global	__sync_##OP##_and_fetch_##N; \
+	HIDDEN_FUNC(__sync_##OP##_and_fetch_##N); \
+	.align	2; \
+__sync_##OP##_and_fetch_##N:; \
+	mova	1f, r0; \
+	mov	r15, r1; \
+	mov	#(0f-1f), r15; \
+0:	mov.##T	@r4, r2; \
+	OP	r5, r2; \
+	mov.##T	r2, @r4; \
+1:	mov	r1, r15; \
+	rts; \
+	.if N == 4 ; \
+	 mov	r2, r0; \
+	.else; \
+	 extu.##T r2, r0; \
+	.endif; \
+	ENDFUNC(__sync_##OP##_and_fetch_##N)	
+#endif
+	
+ATOMIC_OP_AND_FETCH(add,1,b)
+ATOMIC_OP_AND_FETCH(add,2,w)
+ATOMIC_OP_AND_FETCH(add,4,l)
+
+ATOMIC_OP_AND_FETCH(sub,1,b)
+ATOMIC_OP_AND_FETCH(sub,2,w)
+ATOMIC_OP_AND_FETCH(sub,4,l)
+
+ATOMIC_OP_AND_FETCH(or,1,b)
+ATOMIC_OP_AND_FETCH(or,2,w)
+ATOMIC_OP_AND_FETCH(or,4,l)
+
+ATOMIC_OP_AND_FETCH(and,1,b)
+ATOMIC_OP_AND_FETCH(and,2,w)
+ATOMIC_OP_AND_FETCH(and,4,l)
+
+ATOMIC_OP_AND_FETCH(xor,1,b)
+ATOMIC_OP_AND_FETCH(xor,2,w)
+ATOMIC_OP_AND_FETCH(xor,4,l)
+
+#ifdef	DB_ST40300_BUG_WORKAROUND
+#define ATOMIC_COMBOP_AND_FETCH(OP,OP0,OP1,N,T) \
+	.global	__sync_##OP##_and_fetch_##N; \
+	HIDDEN_FUNC(__sync_##OP##_and_fetch_##N); \
+	.align	2; \
+__sync_##OP##_and_fetch_##N:; \
+	mova	1f, r0; \
+	nop; \
+	mov	r15, r1; \
+	mov	#(0f-1f), r15; \
+0:	mov.##T	@r4, r2; \
+	OP0	r2, r2; \
+	OP1	r5, r2; \
+	mov.##T	r2, @r4; \
+1:	mov	r1, r15; \
+	nop; \
+	rts; \
+	.if N == 4 ; \
+	 mov	r2, r0; \
+	.else; \
+	 extu.##T r2, r0; \
+	.endif; \
+	ENDFUNC(__sync_##OP##_and_fetch_##N)
+#else
+#define ATOMIC_COMBOP_AND_FETCH(OP,OP0,OP1,N,T) \
+	.global	__sync_##OP##_and_fetch_##N; \
+	HIDDEN_FUNC(__sync_##OP##_and_fetch_##N); \
+	.align	2; \
+__sync_##OP##_and_fetch_##N:; \
+	mova	1f, r0; \
+	nop; \
+	mov	r15, r1; \
+	mov	#(0f-1f), r15; \
+0:	mov.##T	@r4, r2; \
+	OP0	r2, r2; \
+	OP1	r5, r2; \
+	mov.##T	r2, @r4; \
+1:	mov	r1, r15; \
+	rts; \
+	.if N == 4 ; \
+	 mov	r2, r0; \
+	.else; \
+	 extu.##T r2, r0; \
+	.endif; \
+	ENDFUNC(__sync_##OP##_and_fetch_##N)	
+#endif
+	
+ATOMIC_COMBOP_AND_FETCH(nand,not,and,1,b)
+ATOMIC_COMBOP_AND_FETCH(nand,not,and,2,w)
+ATOMIC_COMBOP_AND_FETCH(nand,not,and,4,l)
 
 #endif /* ! __SH5__ */
Index: gcc/config/sh/elf.h
===================================================================
--- gcc/config/sh/elf.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/elf.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -79,7 +79,7 @@
 
 #undef STARTFILE_SPEC
 #define STARTFILE_SPEC \
-  "%{!shared: crt1.o%s} crti.o%s \
+  "%{!shared: crt1.o%s trap-handler.o%s} crti.o%s \
    %{!shared:crtbegin.o%s} %{shared:crtbeginS.o%s}"
 
 #undef ENDFILE_SPEC
@@ -89,3 +89,8 @@
 /* ASM_OUTPUT_CASE_LABEL is defined in elfos.h.  With it,
    a redundant .align was generated.  */
 #undef  ASM_OUTPUT_CASE_LABEL
+
+#undef MAX_OFILE_ALIGNMENT
+#define MAX_OFILE_ALIGNMENT (((unsigned int) 1 << 20) * 8)
+
+
Index: gcc/config/sh/superh.h
===================================================================
--- gcc/config/sh/superh.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/superh.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,5 +1,6 @@
 /* Definitions of target machine for gcc for Super-H using sh-superh-elf.
    Copyright (C) 2001, 2006, 2007 Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GNU CC.
 
@@ -74,17 +75,12 @@
    on newlib and provide the runtime support */
 #undef SUBTARGET_CPP_SPEC
 #define SUBTARGET_CPP_SPEC \
-"-D__EMBEDDED_CROSS__ %{m4-100*:-D__SH4_100__} %{m4-200*:-D__SH4_200__} %{m4-400:-D__SH4_400__} %{m4-500:-D__SH4_500__} \
+"-D__EMBEDDED_CROSS__ %{m4-100*:-D__SH4_100__} %{m4-200*:-D__SH4_200__} %{m4-300*:-D__SH4_300__} %{m4-340:-D__SH4_340__} %{m4-400:-D__SH4_400__} %{m4-500:-D__SH4_500__} \
 %(cppruntime)"
 
 /* Override the SUBTARGET_ASM_SPEC to add the runtime support */
 #undef SUBTARGET_ASM_SPEC
-#define SUBTARGET_ASM_SPEC "%{m4-100*|m4-200*:-isa=sh4} %{m4-400:-isa=sh4-nommu-nofpu} %{m4-500:-isa=sh4-nofpu} %(asruntime)"
-
-/* Override the SUBTARGET_ASM_RELAX_SPEC so it doesn't interfere with the
-   runtime support by adding -isa=sh4 in the wrong place.  */
-#undef SUBTARGET_ASM_RELAX_SPEC
-#define SUBTARGET_ASM_RELAX_SPEC "%{!m4-100*:%{!m4-200*:%{!m4-400:%{!m4-500:-isa=sh4}}}}"
+#define SUBTARGET_ASM_SPEC "%{m4-100*|m4-200*:-isa=sh4} %{m4-400|m4-340:-isa=sh4-nommu-nofpu} %{m4-500:-isa=sh4-nofpu} %(asruntime)"
 
 /* Create the CC1_SPEC to add the runtime support */
 #undef CC1_SPEC
@@ -96,12 +92,12 @@
 
 /* Override the LIB_SPEC to add the runtime support */
 #undef LIB_SPEC
-#define LIB_SPEC "%{!shared:%{!symbolic:%(libruntime) -lc}} %{pg:-lprofile -lc}"
+#define LIB_SPEC "%{!shared:%{!symbolic:%{pg:-lprofile} %(libruntime) -lc}}"
 
 /* Override STARTFILE_SPEC to add profiling and MMU support.  */
 #undef STARTFILE_SPEC
 #define STARTFILE_SPEC \
-  "%{!shared: %{!m4-400*: %{pg:gcrt1-mmu.o%s}%{!pg:crt1-mmu.o%s}}} \
-   %{!shared: %{m4-400*: %{pg:gcrt1.o%s}%{!pg:crt1.o%s}}} \
+  "%{!shared: %{!m4-400*:%{!m4-340*: %{pg:gcrt1-mmu.o%s}%{!pg:crt1-mmu.o%s}}}} \
+   %{!shared: %{m4-340*|m4-400*: %{pg:gcrt1.o%s}%{!pg:crt1.o%s}}} \
    crti.o%s \
-   %{!shared:crtbegin.o%s} %{shared:crtbeginS.o%s}"
+   %{!shared:crtbegin.o%s trap-handler.o%s} %{shared:crtbeginS.o%s}"
Index: gcc/config/sh/sh4.md
===================================================================
--- gcc/config/sh/sh4.md	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/sh4.md	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,5 +1,6 @@
 ;; DFA scheduling description for SH4.
 ;; Copyright (C) 2004, 2007 Free Software Foundation, Inc.
+;; Copyright (c) 2006  STMicroelectronics.
 
 ;; This file is part of GCC.
 
@@ -208,9 +209,14 @@
 
 (define_insn_reservation "sh4_store" 1
   (and (eq_attr "pipe_model" "sh4")
-       (eq_attr "type" "store"))
+       (eq_attr "type" "store,fstore"))
   "issue+load_store,nothing,memory")
 
+(define_insn_reservation "mac_mem" 1
+  (and (eq_attr "pipe_model" "sh4")
+       (eq_attr "type" "mac_mem"))
+  "d_lock,nothing,memory")
+
 ;; Load Store instructions.
 ;; Group:	LS
 ;; Latency: 	1
@@ -371,35 +377,42 @@
 ;; Fixed point multiplication (DMULS.L DMULU.L MUL.L MULS.W,MULU.W)
 ;; Group:	CO
 ;; Latency: 	4 / 4
-;; Issue Rate: 	1
+;; Issue Rate: 	2
 
 (define_insn_reservation "multi" 4
   (and (eq_attr "pipe_model" "sh4")
        (eq_attr "type" "smpy,dmpy"))
   "d_lock,(d_lock+f1_1),(f1_1|f1_2)*3,F2")
 
-;; Fixed STS from MACL / MACH
+;; Fixed STS from, and LDS to MACL / MACH
 ;; Group:	CO
 ;; Latency: 	3
 ;; Issue Rate: 	1
 
 (define_insn_reservation "sh4_mac_gp" 3
   (and (eq_attr "pipe_model" "sh4")
-       (eq_attr "type" "mac_gp"))
+       (eq_attr "type" "mac_gp,gp_mac,mem_mac"))
   "d_lock")
 
 
 ;; Single precision floating point computation FCMP/EQ,
-;; FCMP/GT, FADD, FLOAT, FMAC, FMUL, FSUB, FTRC, FRVHG, FSCHG
+;; FCMP/GT, FADD, FLOAT, FMAC, FMUL, FSUB, FTRC, FRCHG, FSCHG
 ;; Group:	FE
 ;; Latency: 	3/4
 ;; Issue Rate: 	1
 
 (define_insn_reservation "fp_arith"  3
   (and (eq_attr "pipe_model" "sh4")
-       (eq_attr "type" "fp"))
+       (eq_attr "type" "fp,fp_cmp"))
   "issue,F01,F2")
 
+;; We don't model the resource usage of this exactly because that would
+;; introduce a bogus latency.
+(define_insn_reservation "sh4_fpscr_toggle"  1
+  (and (eq_attr "pipe_model" "sh4")
+       (eq_attr "type" "fpscr_toggle"))
+  "issue")
+
 (define_insn_reservation "fp_arith_ftrc"  3
   (and (eq_attr "pipe_model" "sh4")
        (eq_attr "type" "ftrc_s"))
@@ -436,7 +449,7 @@
 
 (define_insn_reservation "fp_double_arith" 8
   (and (eq_attr "pipe_model" "sh4")
-       (eq_attr "type" "dfp_arith"))
+       (eq_attr "type" "dfp_arith,dfp_mul"))
   "issue,F01,F1+F2,fpu*4,F2")
 
 ;; Double-precision FCMP (FCMP/EQ,FCMP/GT)
Index: gcc/config/sh/lib1funcs.asm
===================================================================
--- gcc/config/sh/lib1funcs.asm	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/lib1funcs.asm	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,6 +1,7 @@
 /* Copyright (C) 1994, 1995, 1997, 1998, 1999, 2000, 2001, 2002, 2003,
    2004, 2005, 2006
    Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is free software; you can redistribute it and/or modify it
 under the terms of the GNU General Public License as published by the
@@ -868,7 +869,7 @@
 	HIDDEN_ALIAS(movstrSI12_i4,movmemSI12_i4)
 
 	.p2align	5
-L_movmem_2mod4_end:
+LOCAL(movmem_2mod4_end):
 	mov.l	r0,@(16,r4)
 	rts
 	mov.l	r1,@(20,r4)
@@ -877,7 +878,7 @@
 
 GLOBAL(movmem_i4_even):
 	mov.l	@r5+,r0
-	bra	L_movmem_start_even
+	bra	LOCAL(movmem_start_even)
 	mov.l	@r5+,r1
 
 GLOBAL(movmem_i4_odd):
@@ -888,20 +889,20 @@
 	mov.l	r1,@(4,r4)
 	mov.l	r2,@(8,r4)
 
-L_movmem_loop:
+LOCAL(movmem_loop):
 	mov.l	r3,@(12,r4)
 	dt	r6
 	mov.l	@r5+,r0
-	bt/s	L_movmem_2mod4_end
+	bt/s	LOCAL(movmem_2mod4_end)
 	mov.l	@r5+,r1
 	add	#16,r4
-L_movmem_start_even:
+LOCAL(movmem_start_even):
 	mov.l	@r5+,r2
 	mov.l	@r5+,r3
 	mov.l	r0,@r4
 	dt	r6
 	mov.l	r1,@(4,r4)
-	bf/s	L_movmem_loop
+	bf/s	LOCAL(movmem_loop)
 	mov.l	r2,@(8,r4)
 	rts
 	mov.l	r3,@(12,r4)
@@ -939,17 +940,23 @@
 ! aa = bb*dd + (aa*dd*65536) + (cc*bb*65536)
 !
 
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif
 GLOBAL(mulsi3):
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	nop
+#endif	
 	mulu.w  r4,r5		! multiply the lsws  macl=bb*dd
 	mov     r5,r3		! r3 = ccdd
 	swap.w  r4,r2		! r2 = bbaa
 	xtrct   r2,r3		! r3 = aacc
 	tst  	r3,r3		! msws zero ?
-	bf      hiset
+	bf      LOCAL(hiset)
 	rts			! yes - then we have the answer
 	sts     macl,r0
 
-hiset:	sts	macl,r0		! r0 = bb*dd
+LOCAL(hiset):	sts	macl,r0		! r0 = bb*dd
 	mulu.w	r2,r5		! brewing macl = aa*dd
 	sts	macl,r1
 	mulu.w	r3,r4		! brewing macl = cc*bb
@@ -970,6 +977,9 @@
 
 	.global	GLOBAL(sdivsi3_i4)
 	HIDDEN_FUNC(GLOBAL(sdivsi3_i4))
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	5
+#endif	
 GLOBAL(sdivsi3_i4):
 	lds r4,fpul
 	float fpul,dr0
@@ -1241,13 +1251,18 @@
 	blink tr2,r63
 	ENDFUNC(GLOBAL(sdivsi3))
 #else /* ! __SHMEDIA__ */
+	
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif
+	
 	FUNC(GLOBAL(sdivsi3))
 GLOBAL(sdivsi3):
 	mov	r4,r1
 	mov	r5,r0
 
 	tst	r0,r0
-	bt	div0
+	bt	LOCAL(div0)
 	mov	#0,r2
 	div0s	r2,r1
 	subc	r3,r3
@@ -1322,8 +1337,10 @@
 	rts
 	mov	r1,r0
 
-
-div0:	rts
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	nop
+#endif		
+LOCAL(div0):	rts
 	mov	#0,r0
 
 	ENDFUNC(GLOBAL(sdivsi3))
@@ -1338,16 +1355,19 @@
 !! args in r4 and r5, result in fpul, clobber r0, r1, r4, r5, dr0, dr2, dr4,
 !! and t bit
 
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif
 	.global	GLOBAL(udivsi3_i4)
 	HIDDEN_FUNC(GLOBAL(udivsi3_i4))
 GLOBAL(udivsi3_i4):
 	mov #1,r1
 	cmp/hi r1,r5
-	bf trivial
+	bf LOCAL(trivial)
 	rotr r1
 	xor r1,r4
 	lds r4,fpul
-	mova L1,r0
+	mova 1f,r0
 #ifdef FMOVD_WORKS
 	fmov.d @r0+,dr4
 #else
@@ -1364,7 +1384,7 @@
 	rts
 	ftrc dr0,fpul
 
-trivial:
+LOCAL(trivial):
 	rts
 	lds r4,fpul
 
@@ -1372,7 +1392,7 @@
 #ifdef FMOVD_WORKS
 	.align 3	! make double below 8 byte aligned.
 #endif
-L1:
+1:
 	.double 2147483648
 
 	ENDFUNC(GLOBAL(udivsi3_i4))
@@ -1399,15 +1419,14 @@
 #endif /* ! __SH5__ || __SH5__ == 32 */
 #elif defined(__SH4_SINGLE__) || defined(__SH4_SINGLE_ONLY__)
 !! args in r4 and r5, result in fpul, clobber r0, r1, r4, r5, dr0, dr2, dr4
-
 	.global	GLOBAL(udivsi3_i4)
 	HIDDEN_FUNC(GLOBAL(udivsi3_i4))
 GLOBAL(udivsi3_i4):
 	mov #1,r1
 	cmp/hi r1,r5
-	bf trivial
+	bf LOCAL(trivial)
 	sts.l fpscr,@-r15
-	mova L1,r0
+	mova 1f,r0
 	lds.l @r0+,fpscr
 	rotr r1
 	xor r1,r4
@@ -1432,12 +1451,12 @@
 #ifdef FMOVD_WORKS
 	.align 3	! make double below 8 byte aligned.
 #endif
-trivial:
+LOCAL(trivial):
 	rts
 	lds r4,fpul
 
 	.align 2
-L1:
+1:
 #ifndef FMOVD_WORKS
 	.long 0x80000
 #else
@@ -1453,7 +1472,6 @@
 /* __SH4_SINGLE_ONLY__ keeps this part for link compatibility with
    sh2e/sh3e code.  */
 #if (! defined(__SH4__) && ! defined (__SH4_SINGLE__)) || defined (__linux__)
-
 !! args in r4 and r5, result in r0, clobbers r4, pr, and t bit
 	.global	GLOBAL(udivsi3)
 	HIDDEN_FUNC(GLOBAL(udivsi3))
@@ -1595,7 +1613,15 @@
  div1 r5,r4; rotcl r0
  div1 r5,r4; rotcl r0
  div1 r5,r4; rotcl r0
- rts; div1 r5,r4
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	nop
+#endif		
+ rts
+ div1 r5,r4
+
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif
 
 GLOBAL(udivsi3):
  sts.l pr,@-r15
@@ -1609,6 +1635,9 @@
  div0u
  swap.w r4,r0
  shlr16 r4
+#ifdef	DB_ST40300_BUG_WORKAROUND
+ nop
+#endif			
  bsr LOCAL(div8)
  shll16 r5
  bsr LOCAL(div7)
@@ -1633,6 +1662,9 @@
  mov #0,r0
  xtrct r4,r0
  xtrct r0,r4
+#ifdef	DB_ST40300_BUG_WORKAROUND
+ nop
+#endif		
  bsr LOCAL(divx4)
  rotcl r0
  bsr LOCAL(divx4)
@@ -1961,6 +1993,10 @@
 #ifdef __SH5__
 	.mode	SHcompact
 #endif
+	
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif	
 	.global GLOBAL(set_fpscr)
 	HIDDEN_FUNC(GLOBAL(set_fpscr))
 GLOBAL(set_fpscr):
@@ -1995,6 +2031,9 @@
 #endif
 #if defined(__SH4__) || defined (__SH2A_DOUBLE__)
 	swap.w r0,r2
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	nop
+#endif		
 	rts
 	mov.l r2,@r1
 #else /* defined(__SH2E__) || defined(__SH3E__) || defined(__SH4_SINGLE*__) */
@@ -2057,7 +2096,7 @@
 	synci
 	blink	tr0, r63
 	ENDFUNC(GLOBAL(ic_invalidate))
-#elif defined(__SH4A__)
+#elif defined(__SH4A__) || defined(__FORCE_SH4A__)
 	.global GLOBAL(ic_invalidate)
 	HIDDEN_FUNC(GLOBAL(ic_invalidate))
 GLOBAL(ic_invalidate):
@@ -2079,6 +2118,9 @@
 	   repetitive.  */
 	.global GLOBAL(ic_invalidate)
 	HIDDEN_FUNC(GLOBAL(ic_invalidate))
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif	
 GLOBAL(ic_invalidate):
 	mov.l	0f,r1
 #ifdef __pic__
@@ -2108,7 +2150,6 @@
 
 #ifdef L_ic_invalidate_array
 #if defined(__SH4A__) || (defined (__FORCE_SH4A__) && (defined(__SH4_SINGLE__) || defined(__SH4__) || defined(__SH4_SINGLE_ONLY__) || (defined(__SH4_NOFPU__) && !defined(__SH5__))))
-	.global GLOBAL(ic_invalidate_array)
 	/* This is needed when an SH4 dso with trampolines is used on SH4A.  */
 	.global GLOBAL(ic_invalidate_array)
 	FUNC(GLOBAL(ic_invalidate_array))
@@ -3009,7 +3050,7 @@
  .global GLOBAL(sdivsi3)
 GLOBAL(sdivsi3):
 #ifdef TEXT_DATA_BUG
- ptb datalabel Local_div_table,tr0
+ ptb datalabel LOCAL(Ldiv_table),tr0
 #else
  ptb GLOBAL(div_table_internal),tr0
 #endif
@@ -3063,8 +3104,8 @@
 #endif /* __pic__ */
 #if defined(TEXT_DATA_BUG) && defined(__pic__) && defined(__SHMEDIA__)
 	.balign 2
-	.type	Local_div_table,@object
-	.size	Local_div_table,128
+	.type	LOCAL(Ldiv_table),@object
+	.size	LOCAL(Ldiv_table),128
 /* negative division constants */
 	.word	-16638
 	.word	-17135
@@ -3100,7 +3141,7 @@
 	.byte	214
 	.byte	241
 	.skip 16
-Local_div_table:
+LOCAL(Ldiv_table):
 	.skip 16
 /* positive division factors */
 	.byte	241
@@ -3232,11 +3273,19 @@
 #define L_MSWLSB 1
 #endif
 
+	
 	.balign 4
 	.global	GLOBAL(udivsi3_i4i)
 	FUNC(GLOBAL(udivsi3_i4i))
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	5
+#endif			
 GLOBAL(udivsi3_i4i):
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	mov.w LOCAL(c128_lw), r1
+#else
 	mov.w LOCAL(c128_w), r1
+#endif
 	div0u
 	mov r4,r0
 	shlr8 r0
@@ -3284,8 +3333,14 @@
 	rts
 	shld r1,r0
 
+#ifdef	DB_ST40300_BUG_WORKAROUND	
+LOCAL(c128_lw):
+	.word 128
+#else
 LOCAL(div_by_1_neg):
 	neg r4,r0
+#endif
+	
 LOCAL(div_by_1):
 	mov.l @r15+,r5
 	rts
@@ -3350,11 +3405,21 @@
 	rotcl r0
 
 	ENDFUNC(GLOBAL(udivsi3_i4i))
-
 	.global	GLOBAL(sdivsi3_i4i)
 	FUNC(GLOBAL(sdivsi3_i4i))
 	/* This is link-compatible with a GLOBAL(sdivsi3) call,
 	   but we effectively clobber only r1.  */
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	5
+	
+LOCAL(div_by_1_neg):
+	neg r4,r0
+LOCAL(div_by_12):
+	mov.l @r15+,r5
+	rts
+	mov.l @r15+,r4
+#endif			
+	
 GLOBAL(sdivsi3_i4i):
 	mov.l r4,@-r15
 	cmp/pz r5
@@ -3851,6 +3916,9 @@
 	/* n1 < d, but n1 might be larger than d1.  */
 	.global GLOBAL(udiv_qrnnd_16)
 	.balign 8
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	5
+#endif		
 GLOBAL(udiv_qrnnd_16):
 	div0u
 	cmp/hi r6,r0
@@ -3891,3 +3959,7 @@
 	ENDFUNC(GLOBAL(udiv_qrnnd_16))
 #endif /* !__SHMEDIA__ */
 #endif /* L_udiv_qrnnd_16 */
+
+#include "ieee-754-sf.S"
+#include "ieee-754-df.S"
+
Index: gcc/config/sh/embed-elf.h
===================================================================
--- gcc/config/sh/embed-elf.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/embed-elf.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2,6 +2,7 @@
    non-Linux embedded targets.
    Copyright (C) 2002, 2003, 2007 Free Software Foundation, Inc.
    Contributed by J"orn Rennecke <joern.rennecke@superh.com>
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -29,9 +30,12 @@
    libgcc-Os-4-200 are.  Thus, when not optimizing for space, link
    libgcc-Os-4-200 after libgcc, so that -mdiv=call-table works for -m2.  */
 #define LIBGCC_SPEC "%{!shared: \
-  %{m4-100*:-lic_invalidate_array_4-100} \
-  %{m4-200*:-lic_invalidate_array_4-200} \
-  %{m4a*:-lic_invalidate_array_4a}} \
+ %{!m4-100*:%{!m4-200*:%{!m4-300*:%{!m4a*:-lic_invalidate}}}} \
+  %{m4-100*:-lic_invalidate_4-100} \
+  %{m4-200*:-lic_invalidate_4-200} \
+  %{m4-300*|-m4-340:-lic_invalidate_4a %{!Os: -lgcc-4-300}} \
+  %{m4a*:-lic_invalidate_4a}} \
   %{Os: -lgcc-Os-4-200} \
   -lgcc \
   %{!Os: -lgcc-Os-4-200}"
+
Index: gcc/config/sh/t-sh
===================================================================
--- gcc/config/sh/t-sh	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/t-sh	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -2,17 +2,48 @@
   $(CONFIG_H) $(SYSTEM_H) $(TREE_H) $(TM_H) $(TM_P_H) coretypes.h
 	$(CC) -c $(ALL_CFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) $(srcdir)/config/sh/sh-c.c
 
+# _nesf2f _nedf2f _gtsf2t _gtdf2t _gesf2f _gedf2f: no-finite-math-only optimized
+# versions of _ne_sf _ne_df _gt_sf _gt_df _ge_sf _ge_df from fp-bit.c (built with
+# -ffinite-math-only. (see TARGET_LIBGCC2_CFLAGS).
+
 LIB1ASMSRC = sh/lib1funcs.asm
 LIB1ASMFUNCS = _ashiftrt _ashiftrt_n _ashiftlt _lshiftrt _movmem \
   _movmem_i4 _mulsi3 _sdivsi3 _sdivsi3_i4 _udivsi3 _udivsi3_i4 _set_fpscr \
   _div_table _udiv_qrnnd_16 \
+  _nesf2f _nedf2f _gtsf2t _gtdf2t _gesf2f _gedf2f \
+  _addsub_sf _mul_sf _addsub_df _mul_df \
+  _hypotf \
+  _sf_to_df _df_to_sf \
+  _fixunssfsi _sf_to_si _usi_to_sf _si_to_sf \
+  _usi_to_df _si_to_df \
+  _div_sf \
   $(LIB1ASMFUNCS_CACHE)
+LIB1ASMFUNCS_CACHE = _ic_invalidate _ic_invalidate_array
+
+# notyet. move above and remove bellow!
+# unord_sf/unord_df
+# _div_df \
+# _df_to_usi \
+# _df_to_si \
 
 # We want fine grained libraries, so use the new code to build the
 # floating point emulation libraries.
 FPBIT = fp-bit.c
 DPBIT = dp-bit.c
 
+FPBIT_FUNCS = _pack_sf _unpack_sf \
+    _fpcmp_parts_sf _compare_sf _eq_sf _ne_sf _gt_sf _ge_sf \
+    _lt_sf _le_sf _unord_sf _negate_sf _make_sf \
+    _sf_to_tf _thenan_sf
+
+DPBIT_FUNCS = _pack_df _unpack_df _div_df \
+    _fpcmp_parts_df _compare_df _eq_df _ne_df _gt_df _ge_df \
+    _lt_df _le_df _unord_df _negate_df _make_df \
+    _df_to_tf _thenan_df _df_to_usi _df_to_si
+
+# Make sure they are compiled in optimized mode.
+TARGET_LIBGCC2_CFLAGS = -ffinite-math-only
+
 dp-bit.c: $(srcdir)/config/fp-bit.c
 	echo '#ifdef __LITTLE_ENDIAN__' > dp-bit.c
 	echo '#define FLOAT_BIT_ORDER_MISMATCH' >>dp-bit.c
@@ -38,11 +69,12 @@
 # is why sh2a and sh2a-single need their own multilibs.
 MULTILIB_MATCHES = $(shell \
   multilibs="$(MULTILIB_OPTIONS)" ; \
-  for abi in m1,m2,m3,m4-nofpu,m4-400,m4-500,m4al,m4a-nofpu m1,m2,m2a-nofpu \
-             m2e,m3e,m4-single-only,m4-100-single-only,m4-200-single-only,m4a-single-only \
+  for abi in m1,m2,m3,m4-nofpu,m4-100-nofpu,m4-200-nofpu,m4-400,m4-500,m4-340,m4-300-nofpu,m4al,m4a-nofpu \
+             m1,m2,m2a-nofpu \
+             m2e,m3e,m4-single-only,m4-100-single-only,m4-200-single-only,m4-300-single-only,m4a-single-only \
              m2e,m2a-single-only \
-             m4-single,m4-100-single,m4-200-single,m4a-single \
-             m4,m4-100,m4-200,m4a \
+             m4-single,m4-100-single,m4-200-single,m4-300-single,m4a-single \
+             m4,m4-100,m4-200,m4-300,m4a \
              m5-32media,m5-compact,m5-32media \
              m5-32media-nofpu,m5-compact-nofpu,m5-32media-nofpu; do \
     subst= ; \
@@ -67,32 +99,46 @@
 	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $(T)crti.o -x assembler-with-cpp $(srcdir)/config/sh/crti.asm
 $(T)crtn.o: $(srcdir)/config/sh/crtn.asm $(GCC_PASSES)
 	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $(T)crtn.o -x assembler-with-cpp $(srcdir)/config/sh/crtn.asm
+$(T)trap-handler.o : $(srcdir)/config/sh/trap-handler.c
+	$(GCC_FOR_TARGET) $(CFLAGS_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $(T)trap-handler.o -g $(srcdir)/config/sh/trap-handler.c
 
 $(out_object_file): gt-sh.h
 gt-sh.h : s-gtype ; @true
 
 # These are not suitable for COFF.
-# EXTRA_MULTILIB_PARTS= crt1.o crti.o crtn.o crtbegin.o crtend.o
+# EXTRA_MULTILIB_PARTS= crt1.o crti.o crtn.o crtbegin.o crtend.o trap-handler.o
+
+IC_EXTRA_PARTS= libic_invalidate.a libic_invalidate_4-100.a \
+	libic_invalidate_4-200.a libic_invalidate_4a.a
 
-IC_EXTRA_PARTS= libic_invalidate_array_4-100.a libic_invalidate_array_4-200.a \
-libic_invalidate_array_4a.a
-OPT_EXTRA_PARTS= libgcc-Os-4-200.a
+OPT_EXTRA_PARTS= libgcc-Os-4-200.a libgcc-4-300.a
 EXTRA_MULTILIB_PARTS= $(IC_EXTRA_PARTS) $(OPT_EXTRA_PARTS)
 
+$(T)ic_invalidate.o: $(srcdir)/config/sh/lib1funcs.asm $(GCC_PASSES)
+	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -I. -c -o $(T)ic_invalidate.o -DL_ic_invalidate -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
+
+$(T)ic_invalidate_4a.o: $(srcdir)/config/sh/lib1funcs.asm $(GCC_PASSES)
+	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -I. -c -o $(T)ic_invalidate_4a.o -DL_ic_invalidate -D__FORCE_SH4A__ -Wa,-isa=st40-300 -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
+
+$(T)ic_invalidate_array.o: $(srcdir)/config/sh/lib1funcs.asm $(GCC_PASSES)
+	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -I. -c -o $(T)ic_invalidate_array.o -DL_ic_invalidate_array  -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
+$(T)libic_invalidate.a: $(T)ic_invalidate_array.o $(T)ic_invalidate.o $(GCC_PASSES)
+	$(AR_CREATE_FOR_TARGET) $(T)libic_invalidate.a $(T)ic_invalidate_array.o $(T)ic_invalidate.o 
+
 $(T)ic_invalidate_array_4-100.o: $(srcdir)/config/sh/lib1funcs.asm $(GCC_PASSES)
-	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $(T)ic_invalidate_array_4-100.o -DL_ic_invalidate_array -DWAYS=1 -DWAY_SIZE=0x2000 -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
-$(T)libic_invalidate_array_4-100.a: $(T)ic_invalidate_array_4-100.o $(GCC_PASSES)
-	$(AR_CREATE_FOR_TARGET) $(T)libic_invalidate_array_4-100.a $(T)ic_invalidate_array_4-100.o
+	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -I. -c -o $(T)ic_invalidate_array_4-100.o -DL_ic_invalidate_array -DWAYS=1 -DWAY_SIZE=0x2000 -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
+$(T)libic_invalidate_4-100.a: $(T)ic_invalidate_array_4-100.o $(T)ic_invalidate.o $(GCC_PASSES)
+	$(AR_CREATE_FOR_TARGET) $(T)libic_invalidate_4-100.a $(T)ic_invalidate_array_4-100.o $(T)ic_invalidate.o 
 
 $(T)ic_invalidate_array_4-200.o: $(srcdir)/config/sh/lib1funcs.asm $(GCC_PASSES)
-	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $(T)ic_invalidate_array_4-200.o -DL_ic_invalidate_array -DWAYS=2 -DWAY_SIZE=0x2000 -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
-$(T)libic_invalidate_array_4-200.a: $(T)ic_invalidate_array_4-200.o $(GCC_PASSES)
-	$(AR_CREATE_FOR_TARGET) $(T)libic_invalidate_array_4-200.a $(T)ic_invalidate_array_4-200.o
+	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -I. -c -o $(T)ic_invalidate_array_4-200.o -DL_ic_invalidate_array -DWAYS=2 -DWAY_SIZE=0x2000 -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
+$(T)libic_invalidate_4-200.a: $(T)ic_invalidate_array_4-200.o $(T)ic_invalidate.o $(GCC_PASSES)
+	$(AR_CREATE_FOR_TARGET) $(T)libic_invalidate_4-200.a $(T)ic_invalidate_array_4-200.o $(T)ic_invalidate.o 
 
 $(T)ic_invalidate_array_4a.o: $(srcdir)/config/sh/lib1funcs.asm $(GCC_PASSES)
-	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $(T)ic_invalidate_array_4a.o -DL_ic_invalidate_array -D__FORCE_SH4A__ -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
-$(T)libic_invalidate_array_4a.a: $(T)ic_invalidate_array_4a.o $(GCC_PASSES)
-	$(AR_CREATE_FOR_TARGET) $(T)libic_invalidate_array_4a.a $(T)ic_invalidate_array_4a.o
+	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -I. -c -o $(T)ic_invalidate_array_4a.o -DL_ic_invalidate_array -D__FORCE_SH4A__ -Wa,-isa=st40-300 -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
+$(T)libic_invalidate_4a.a: $(T)ic_invalidate_array_4a.o $(T)ic_invalidate_4a.o $(GCC_PASSES)
+	$(AR_CREATE_FOR_TARGET) $(T)libic_invalidate_4a.a $(T)ic_invalidate_array_4a.o $(T)ic_invalidate_4a.o 
 
 $(T)sdivsi3_i4i-Os-4-200.o: $(srcdir)/config/sh/lib1funcs-Os-4-200.asm $(GCC_PASSES)
 	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $@ -DL_sdivsi3_i4i -x assembler-with-cpp $<
@@ -104,6 +150,12 @@
 $(T)libgcc-Os-4-200.a: $(OBJS_Os_4_200) $(GCC_PASSES)
 	$(AR_CREATE_FOR_TARGET) $@ $(OBJS_Os_4_200)
 
+$(T)div_table-4-300.o: $(srcdir)/config/sh/lib1funcs-4-300.asm $(GCC_PASSES)
+	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $@ -DL_div_table -x assembler-with-cpp $<
+
+$(T)libgcc-4-300.a: $(T)div_table-4-300.o $(GCC_PASSES)
+	$(AR_CREATE_FOR_TARGET) $@ $(T)div_table-4-300.o
+
 # Local Variables:
 # mode: Makefile
 # End:
Index: gcc/config/sh/crtn.asm
===================================================================
--- gcc/config/sh/crtn.asm	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/crtn.asm	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -44,9 +44,15 @@
 	rts
 	add	#8,r15
 #else
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif	
 	mov	r14,r15
 	lds.l	@r15+,pr
 	mov.l	@r15+,r14
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	nop
+#endif		
 	rts
 #ifdef __ELF__
 	mov.l	@r15+,r12
@@ -70,9 +76,15 @@
 	rts
 	add	#8,r15
 #else
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif		
 	mov	r14,r15
 	lds.l	@r15+,pr
 	mov.l	@r15+,r14
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	nop
+#endif			
 	rts
 #ifdef __ELF__
 	mov.l	@r15+,r12
Index: gcc/config/sh/divcost-analysis
===================================================================
--- gcc/config/sh/divcost-analysis	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/divcost-analysis	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -38,12 +38,17 @@
 div_le128_neg -> div_by_1_neg: 4
 div_le128_neg -> rts          18
 
-                    absolute divisor range:
+         sh4-200    absolute divisor range:
             1  [2..128]  [129..64K) [64K..|divident|/256] >=64K,>|divident/256|
 udiv       18     22         38            32                   30
 sdiv pos:  20     24         41            35                   32
 sdiv neg:  15     25         42            36                   33
 
+         sh4-300    absolute divisor range:
+                 8 bit      16 bit       24 bit              > 24 bit
+udiv              15         35            28                   25
+sdiv              14         36            34                   31
+
 
 fp-based:
 
@@ -74,3 +79,4 @@
 mov.l @(r0,r1),r0
 jmp @r0
 ; 2 cycles worse than SFUNC_STATIC
+
Index: gcc/config/sh/sh4a.md
===================================================================
--- gcc/config/sh/sh4a.md	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/sh4a.md	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,5 +1,6 @@
 ;; Scheduling description for Renesas SH4a
 ;; Copyright (C) 2003, 2004, 2007 Free Software Foundation, Inc.
+;; Copyright (c) 2006  STMicroelectronics.
 ;;
 ;; This file is part of GCC.
 ;;
@@ -97,9 +98,11 @@
 ;; MOV
 ;; Group: MT
 ;; Latency: 0
+;; ??? not sure if movi8 belongs here, but that's where it was
+;; effectively before.
 (define_insn_reservation "sh4a_mov" 0
   (and (eq_attr "cpu" "sh4a")
-       (eq_attr "type" "move"))
+       (eq_attr "type" "move,movi8,gp_mac"))
   "ID_or")
 
 ;; Load
@@ -107,7 +110,7 @@
 ;; Latency: 3
 (define_insn_reservation "sh4a_load" 3
   (and (eq_attr "cpu" "sh4a")
-       (eq_attr "type" "load,pcload"))
+       (eq_attr "type" "load,pcload,mem_mac"))
   "sh4a_ls+sh4a_memory")
 
 (define_insn_reservation "sh4a_load_si" 3
@@ -120,7 +123,7 @@
 ;; Latency: 0
 (define_insn_reservation "sh4a_store" 0
   (and (eq_attr "cpu" "sh4a")
-       (eq_attr "type" "store"))
+       (eq_attr "type" "store,fstore,mac_mem"))
   "sh4a_ls+sh4a_memory")
 
 ;; CWB TYPE
@@ -176,7 +179,7 @@
 ;; Latency: 	3
 (define_insn_reservation "sh4a_fp_arith"  3
   (and (eq_attr "cpu" "sh4a")
-       (eq_attr "type" "fp"))
+       (eq_attr "type" "fp,fp_cmp,fpscr_toggle"))
   "ID_or,sh4a_fex")
 
 (define_insn_reservation "sh4a_fp_arith_ftrc"  3
@@ -206,7 +209,7 @@
 ;; Latency: 	5
 (define_insn_reservation "sh4a_fp_double_arith" 5
   (and (eq_attr "cpu" "sh4a")
-       (eq_attr "type" "dfp_arith"))
+       (eq_attr "type" "dfp_arith,dfp_mul"))
   "ID_or,sh4a_fex*3")
 
 ;; Double precision FDIV/SQRT
Index: gcc/config/sh/lib1funcs-4-300.asm
===================================================================
--- gcc/config/sh/lib1funcs-4-300.asm	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/lib1funcs-4-300.asm	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,939 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+   Copyright (C) 2006 STMicroelectronics
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+/* libgcc routines for the STMicroelectronics ST40-300 CPU.
+   Contributed by J"orn Rennecke joern.rennecke@st.com.  */
+
+#include "lib1funcs.h"
+
+#ifdef L_div_table
+#if defined (__SH3__) || defined (__SH3E__) || defined (__SH4__) || defined (__SH4_SINGLE__) || defined (__SH4_SINGLE_ONLY__) || defined (__SH4_NOFPU__)
+/* This code used shld, thus is not suitable for SH1 / SH2.  */
+
+/* Signed / unsigned division without use of FPU, optimized for SH4-300.
+   Uses a lookup table for divisors in the range -128 .. +127, and
+   div1 with case distinction for larger divisors in three more ranges.
+   The code is lumped together with the table to allow the use of mova.  */
+#ifdef __LITTLE_ENDIAN__
+#define L_LSB 0
+#define L_LSWMSB 1
+#define L_MSWLSB 2
+#else
+#define L_LSB 3
+#define L_LSWMSB 2
+#define L_MSWLSB 1
+#endif
+
+	.global	GLOBAL(udivsi3_i4i)
+	.global	GLOBAL(sdivsi3_i4i)
+	FUNC(GLOBAL(udivsi3_i4i))
+	FUNC(GLOBAL(sdivsi3_i4i))
+
+	.balign 4
+LOCAL(div_ge8m): ! 10 cycles up to here
+	rotcr r1 ! signed shift must use original sign from r4
+	div0s r5,r4
+	mov #24,r7
+	shld r7,r6
+	shad r0,r1
+	rotcl r6
+	div1 r5,r1
+	swap.w r5,r0 ! detect -0x80000000 : 0x800000
+	rotcl r6
+	swap.w r4,r7
+	div1 r5,r1
+	swap.b r7,r7
+	rotcl r6
+	or r7,r0
+	div1 r5,r1
+	swap.w r0,r7
+	rotcl r6
+	or r7,r0
+	div1 r5,r1
+	add #-0x80,r0
+	rotcl r6
+	extu.w r0,r0
+	div1 r5,r1
+	neg r0,r0
+	rotcl r6
+	swap.w r0,r0
+	div1 r5,r1
+	mov.l @r15+,r7
+	and r6,r0
+	rotcl r6
+	div1 r5,r1
+	shll2 r0
+	rotcl r6
+	exts.b r0,r0
+	div1 r5,r1
+	swap.w r0,r0
+	exts.w r0,r1
+	exts.b r6,r0
+	mov.l @r15+,r6
+	rotcl r0
+	rts
+	sub r1,r0
+	! 31 cycles up to here
+
+	.balign 4
+LOCAL(udiv_ge64k): ! 3 cycles up to here
+	mov r4,r0
+	shlr8 r0
+	div0u
+	cmp/hi r0,r5
+	bt LOCAL(udiv_r8)
+	mov.l r5,@-r15
+	shll8 r5
+	! 7 cycles up to here
+	.rept 8
+	div1 r5,r0
+	.endr
+	extu.b r4,r1 ! 15 cycles up to here
+	extu.b r0,r6
+	xor r1,r0
+	xor r6,r0
+	swap.b r6,r6
+	.rept 8
+	div1 r5,r0
+	.endr ! 25 cycles up to here
+	extu.b r0,r0
+	mov.l @r15+,r5
+	or r6,r0
+	mov.l @r15+,r6
+	rts
+	rotcl r0 ! 28 cycles up to here
+
+	.balign 4
+LOCAL(udiv_r8): ! 6 cycles up to here
+	mov.l r4,@-r15
+	shll16 r4
+	shll8 r4
+	!
+	shll r4
+	mov r0,r1
+	div1 r5,r1
+	mov r4,r0
+	rotcl r0
+	mov.l @r15+,r4
+	div1 r5,r1
+	! 12 cycles up to here
+	.rept 6
+	rotcl r0; div1 r5,r1
+	.endr
+	mov.l @r15+,r6 ! 24 cycles up to here
+	rts
+	rotcl r0
+
+	.balign 4
+LOCAL(div_ge32k): ! 6 cycles up to here
+	mov.l r7,@-r15
+	swap.w r5,r6
+	exts.b r6,r7
+	exts.w r6,r6
+	cmp/eq r6,r7
+	extu.b r1,r6
+	bf/s LOCAL(div_ge8m)
+	cmp/hi r1,r4 ! copy sign bit of r4 into T
+	rotcr r1 ! signed shift must use original sign from r4
+	div0s r5,r4
+	shad r0,r1
+	shll8 r5
+	div1 r5,r1
+	mov r5,r7 ! detect r4 == 0x80000000 && r5 == 0x8000(00)
+	div1 r5,r1
+	shlr8 r7
+	div1 r5,r1
+	swap.w r4,r0
+	div1 r5,r1
+	swap.b r0,r0
+	div1 r5,r1
+	or r0,r7
+	div1 r5,r1
+	add #-80,r7
+	div1 r5,r1
+	swap.w r7,r0
+	div1 r5,r1
+	or r0,r7
+	extu.b r1,r0
+	xor r6,r1
+	xor r0,r1
+	exts.b r0,r0
+	div1 r5,r1
+	extu.w r7,r7
+	div1 r5,r1
+	neg r7,r7 ! upper 16 bit of r7 == 0 if r4 == 0x80000000 && r5 == 0x8000
+	div1 r5,r1
+	and r0,r7
+	div1 r5,r1
+	swap.w r7,r7 ! 26 cycles up to here.
+	div1 r5,r1
+	shll8 r0
+	div1 r5,r1
+	exts.w r7,r7
+	div1 r5,r1
+	add r0,r0
+	div1 r5,r1
+	sub r7,r0
+	extu.b r1,r1
+	mov.l @r15+,r7
+	rotcl r1
+	mov.l @r15+,r6
+	add r1,r0
+	mov #-8,r1
+	rts
+	shad r1,r5 ! 34 cycles up to here
+
+	.balign 4
+GLOBAL(udivsi3_i4i):
+	mov.l r6,@-r15
+	extu.w r5,r6
+	cmp/eq r5,r6
+	mov #0x7f,r0
+	bf LOCAL(udiv_ge64k)
+	cmp/hi r0,r5
+	bf LOCAL(udiv_le128)
+	mov r4,r1
+	shlr8 r1
+	div0u
+	shlr r1
+	shll16 r6
+	div1 r6,r1
+	extu.b r4,r0 ! 7 cycles up to here
+	.rept 8
+	div1 r6,r1
+	.endr     ! 15 cycles up to here
+	xor r1,r0 ! xor dividend with result lsb
+	.rept 6
+	div1 r6,r1
+	.endr
+	mov.l r7,@-r15 ! 21 cycles up to here
+	div1 r6,r1
+	extu.b r0,r7
+	div1 r6,r1
+	shll8 r7
+	extu.w r1,r0
+	xor r7,r1 ! replace lsb of result with lsb of dividend
+	div1 r6,r1
+	mov #0,r7
+	div1 r6,r1
+	!
+	div1 r6,r1
+	bra LOCAL(div_end)
+	div1 r6,r1 ! 28 cycles up to here
+
+	/* This is link-compatible with a GLOBAL(sdivsi3) call,
+	   but we effectively clobber only r1, macl and mach  */
+        /* Because negative quotients are calculated as one's complements,
+	   -0x80000000 divided by the smallest positive number of a number
+	   range (0x80, 0x8000, 0x800000) causes saturation in the one's
+           complement representation, and we have to suppress the
+	   one's -> two's complement adjustment.  Since positive numbers
+	   don't get such an adjustment, it's OK to also compute one's -> two's
+	   complement adjustment suppression for a dividend of 0.  */
+	.balign 4
+GLOBAL(sdivsi3_i4i):
+	mov.l r6,@-r15
+	exts.b r5,r6
+	cmp/eq r5,r6
+	mov #-1,r1
+	bt/s LOCAL(div_le128)
+	cmp/pz r4
+	addc r4,r1
+	exts.w r5,r6
+	cmp/eq r5,r6
+	mov #-7,r0
+	bf/s LOCAL(div_ge32k)
+	cmp/hi r1,r4 ! copy sign bit of r4 into T
+	rotcr r1
+	shll16 r6  ! 7 cycles up to here
+	shad r0,r1
+	div0s r5,r4
+	div1 r6,r1
+	mov.l r7,@-r15
+	div1 r6,r1
+	mov r4,r0 ! re-compute adjusted dividend
+	div1 r6,r1
+	mov #-31,r7
+	div1 r6,r1
+	shad r7,r0
+	div1 r6,r1
+	add r4,r0 ! adjusted dividend
+	div1 r6,r1
+	mov.l r8,@-r15
+	div1 r6,r1
+	swap.w r4,r8 ! detect special case r4 = 0x80000000, r5 = 0x80
+	div1 r6,r1
+	swap.b r8,r8
+	xor r1,r0 ! xor dividend with result lsb
+	div1 r6,r1
+	div1 r6,r1
+	or r5,r8
+	div1 r6,r1
+	add #-0x80,r8 ! r8 is 0 iff there is a match
+	div1 r6,r1
+	swap.w r8,r7 ! or upper 16 bits...
+	div1 r6,r1
+	or r7,r8 !...into lower 16 bits
+	div1 r6,r1
+	extu.w r8,r8
+	div1 r6,r1
+	extu.b r0,r7
+	div1 r6,r1
+	shll8 r7
+	exts.w r1,r0
+	xor r7,r1 ! replace lsb of result with lsb of dividend
+	div1 r6,r1
+	neg r8,r8 ! upper 16 bits of r8 are now 0xffff iff we want end adjm.
+	div1 r6,r1
+	and r0,r8
+	div1 r6,r1
+	swap.w r8,r7
+	div1 r6,r1
+	mov.l @r15+,r8 ! 58 insns, 29 cycles up to here
+LOCAL(div_end):
+	div1 r6,r1
+	shll8 r0
+	div1 r6,r1
+	exts.w r7,r7
+	div1 r6,r1
+	add r0,r0
+	div1 r6,r1
+	sub r7,r0
+	extu.b r1,r1
+	mov.l @r15+,r7
+	rotcl r1
+	mov.l @r15+,r6
+	rts
+	add r1,r0
+
+	.balign 4
+LOCAL(udiv_le128): ! 4 cycles up to here (or 7 for mispredict)
+	mova LOCAL(div_table_inv),r0
+	shll2 r6
+	mov.l @(r0,r6),r1
+	mova LOCAL(div_table_clz),r0
+	lds r4,mach
+	!
+	!
+	!
+	tst r1,r1
+	!
+	bt 0f
+	dmulu.l r1,r4
+0:	mov.b @(r0,r5),r1
+	clrt
+	!
+	!
+	sts mach,r0
+	addc r4,r0
+	rotcr r0
+	mov.l @r15+,r6
+	rts
+	shld r1,r0
+
+	.balign 4
+LOCAL(div_le128): ! 3 cycles up to here (or 6 for mispredict)
+	mova LOCAL(div_table_inv),r0
+	shll2 r6
+	mov.l @(r0,r6),r1
+	mova LOCAL(div_table_clz),r0
+	neg r4,r6
+	bf 0f
+	mov r4,r6
+0:	lds r6,mach
+	tst r1,r1
+	bt 0f
+	dmulu.l r1,r6
+0:	div0s r4,r5
+	mov.b @(r0,r5),r1
+	bt/s LOCAL(le128_neg)
+	clrt
+	!
+	sts mach,r0
+	addc r6,r0
+	rotcr r0
+	mov.l @r15+,r6
+	rts
+	shld r1,r0
+
+/* Could trap divide by zero for the cost of one cycle more mispredict penalty:
+...
+	dmulu.l r1,r6
+0:	div0s r4,r5
+	bt/s LOCAL(le128_neg)
+	tst r5,r5
+	bt LOCAL(div_by_zero)
+	mov.b @(r0,r5),r1
+	sts mach,r0
+	addc r6,r0
+...
+LOCAL(div_by_zero):
+	trapa #
+	.balign 4
+LOCAL(le128_neg):
+	bt LOCAL(div_by_zero)
+	mov.b @(r0,r5),r1
+	sts mach,r0
+	addc r6,r0
+...  */
+
+	.balign 4
+LOCAL(le128_neg):
+	sts mach,r0
+	addc r6,r0
+	rotcr r0
+	mov.l @r15+,r6
+	shld r1,r0
+	rts
+	neg r0,r0
+	ENDFUNC(GLOBAL(udivsi3_i4i))
+	ENDFUNC(GLOBAL(sdivsi3_i4i))
+
+/* This table has been generated by divtab-sh4.c.  */
+	.balign 4
+	.byte	-7
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-2
+	.byte	-2
+	.byte	-2
+	.byte	-2
+	.byte	-1
+	.byte	-1
+	.byte	0
+LOCAL(div_table_clz):
+	.byte	0
+	.byte	0
+	.byte	-1
+	.byte	-1
+	.byte	-2
+	.byte	-2
+	.byte	-2
+	.byte	-2
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+/* 1/-128 .. 1/127, normalized.  There is an implicit leading 1 in bit 32,
+   or in bit 33 for powers of two.  */
+	.balign 4
+	.long   0x0
+	.long	0x2040811
+	.long	0x4104105
+	.long	0x624DD30
+	.long	0x8421085
+	.long	0xA6810A7
+	.long	0xC9714FC
+	.long	0xECF56BF
+	.long	0x11111112
+	.long	0x135C8114
+	.long	0x15B1E5F8
+	.long	0x18118119
+	.long	0x1A7B9612
+	.long	0x1CF06ADB
+	.long	0x1F7047DD
+	.long	0x21FB7813
+	.long	0x24924925
+	.long	0x27350B89
+	.long	0x29E4129F
+	.long	0x2C9FB4D9
+	.long	0x2F684BDB
+	.long	0x323E34A3
+	.long	0x3521CFB3
+	.long	0x38138139
+	.long	0x3B13B13C
+	.long	0x3E22CBCF
+	.long	0x41414142
+	.long	0x446F8657
+	.long	0x47AE147B
+	.long	0x4AFD6A06
+	.long	0x4E5E0A73
+	.long	0x51D07EAF
+	.long	0x55555556
+	.long	0x58ED2309
+	.long	0x5C9882BA
+	.long	0x60581606
+	.long	0x642C8591
+	.long	0x68168169
+	.long	0x6C16C16D
+	.long	0x702E05C1
+	.long	0x745D1746
+	.long	0x78A4C818
+	.long	0x7D05F418
+	.long	0x81818182
+	.long	0x86186187
+	.long	0x8ACB90F7
+	.long	0x8F9C18FA
+	.long	0x948B0FCE
+	.long	0x9999999A
+	.long	0x9EC8E952
+	.long	0xA41A41A5
+	.long	0xA98EF607
+	.long	0xAF286BCB
+	.long	0xB4E81B4F
+	.long	0xBACF914D
+	.long	0xC0E07039
+	.long	0xC71C71C8
+	.long	0xCD856891
+	.long	0xD41D41D5
+	.long	0xDAE6076C
+	.long	0xE1E1E1E2
+	.long	0xE9131AC0
+	.long	0xF07C1F08
+	.long	0xF81F81F9
+	.long	0x0
+	.long	0x4104105
+	.long	0x8421085
+	.long	0xC9714FC
+	.long	0x11111112
+	.long	0x15B1E5F8
+	.long	0x1A7B9612
+	.long	0x1F7047DD
+	.long	0x24924925
+	.long	0x29E4129F
+	.long	0x2F684BDB
+	.long	0x3521CFB3
+	.long	0x3B13B13C
+	.long	0x41414142
+	.long	0x47AE147B
+	.long	0x4E5E0A73
+	.long	0x55555556
+	.long	0x5C9882BA
+	.long	0x642C8591
+	.long	0x6C16C16D
+	.long	0x745D1746
+	.long	0x7D05F418
+	.long	0x86186187
+	.long	0x8F9C18FA
+	.long	0x9999999A
+	.long	0xA41A41A5
+	.long	0xAF286BCB
+	.long	0xBACF914D
+	.long	0xC71C71C8
+	.long	0xD41D41D5
+	.long	0xE1E1E1E2
+	.long	0xF07C1F08
+	.long	0x0
+	.long	0x8421085
+	.long	0x11111112
+	.long	0x1A7B9612
+	.long	0x24924925
+	.long	0x2F684BDB
+	.long	0x3B13B13C
+	.long	0x47AE147B
+	.long	0x55555556
+	.long	0x642C8591
+	.long	0x745D1746
+	.long	0x86186187
+	.long	0x9999999A
+	.long	0xAF286BCB
+	.long	0xC71C71C8
+	.long	0xE1E1E1E2
+	.long	0x0
+	.long	0x11111112
+	.long	0x24924925
+	.long	0x3B13B13C
+	.long	0x55555556
+	.long	0x745D1746
+	.long	0x9999999A
+	.long	0xC71C71C8
+	.long	0x0
+	.long	0x24924925
+	.long	0x55555556
+	.long	0x9999999A
+	.long	0x0
+	.long	0x55555556
+	.long	0x0
+	.long	0x0
+LOCAL(div_table_inv):
+	.long	0x0
+	.long	0x0
+	.long	0x0
+	.long	0x55555556
+	.long	0x0
+	.long	0x9999999A
+	.long	0x55555556
+	.long	0x24924925
+	.long	0x0
+	.long	0xC71C71C8
+	.long	0x9999999A
+	.long	0x745D1746
+	.long	0x55555556
+	.long	0x3B13B13C
+	.long	0x24924925
+	.long	0x11111112
+	.long	0x0
+	.long	0xE1E1E1E2
+	.long	0xC71C71C8
+	.long	0xAF286BCB
+	.long	0x9999999A
+	.long	0x86186187
+	.long	0x745D1746
+	.long	0x642C8591
+	.long	0x55555556
+	.long	0x47AE147B
+	.long	0x3B13B13C
+	.long	0x2F684BDB
+	.long	0x24924925
+	.long	0x1A7B9612
+	.long	0x11111112
+	.long	0x8421085
+	.long	0x0
+	.long	0xF07C1F08
+	.long	0xE1E1E1E2
+	.long	0xD41D41D5
+	.long	0xC71C71C8
+	.long	0xBACF914D
+	.long	0xAF286BCB
+	.long	0xA41A41A5
+	.long	0x9999999A
+	.long	0x8F9C18FA
+	.long	0x86186187
+	.long	0x7D05F418
+	.long	0x745D1746
+	.long	0x6C16C16D
+	.long	0x642C8591
+	.long	0x5C9882BA
+	.long	0x55555556
+	.long	0x4E5E0A73
+	.long	0x47AE147B
+	.long	0x41414142
+	.long	0x3B13B13C
+	.long	0x3521CFB3
+	.long	0x2F684BDB
+	.long	0x29E4129F
+	.long	0x24924925
+	.long	0x1F7047DD
+	.long	0x1A7B9612
+	.long	0x15B1E5F8
+	.long	0x11111112
+	.long	0xC9714FC
+	.long	0x8421085
+	.long	0x4104105
+	.long	0x0
+	.long	0xF81F81F9
+	.long	0xF07C1F08
+	.long	0xE9131AC0
+	.long	0xE1E1E1E2
+	.long	0xDAE6076C
+	.long	0xD41D41D5
+	.long	0xCD856891
+	.long	0xC71C71C8
+	.long	0xC0E07039
+	.long	0xBACF914D
+	.long	0xB4E81B4F
+	.long	0xAF286BCB
+	.long	0xA98EF607
+	.long	0xA41A41A5
+	.long	0x9EC8E952
+	.long	0x9999999A
+	.long	0x948B0FCE
+	.long	0x8F9C18FA
+	.long	0x8ACB90F7
+	.long	0x86186187
+	.long	0x81818182
+	.long	0x7D05F418
+	.long	0x78A4C818
+	.long	0x745D1746
+	.long	0x702E05C1
+	.long	0x6C16C16D
+	.long	0x68168169
+	.long	0x642C8591
+	.long	0x60581606
+	.long	0x5C9882BA
+	.long	0x58ED2309
+	.long	0x55555556
+	.long	0x51D07EAF
+	.long	0x4E5E0A73
+	.long	0x4AFD6A06
+	.long	0x47AE147B
+	.long	0x446F8657
+	.long	0x41414142
+	.long	0x3E22CBCF
+	.long	0x3B13B13C
+	.long	0x38138139
+	.long	0x3521CFB3
+	.long	0x323E34A3
+	.long	0x2F684BDB
+	.long	0x2C9FB4D9
+	.long	0x29E4129F
+	.long	0x27350B89
+	.long	0x24924925
+	.long	0x21FB7813
+	.long	0x1F7047DD
+	.long	0x1CF06ADB
+	.long	0x1A7B9612
+	.long	0x18118119
+	.long	0x15B1E5F8
+	.long	0x135C8114
+	.long	0x11111112
+	.long	0xECF56BF
+	.long	0xC9714FC
+	.long	0xA6810A7
+	.long	0x8421085
+	.long	0x624DD30
+	.long	0x4104105
+	.long	0x2040811
+	/* maximum error: 0.987342 scaled: 0.921875*/
+
+#endif /* SH3 / SH4 */
+
+#endif /* L_div_table */
Index: gcc/config/sh/sh.opt
===================================================================
--- gcc/config/sh/sh.opt	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/sh.opt	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,6 +1,7 @@
 ; Options for the SH port of the compiler.
 
 ; Copyright (C) 2005, 2006, 2007 Free Software Foundation, Inc.
+; Copyright (c) 2009  STMicroelectronics.
 ;
 ; This file is part of GCC.
 ;
@@ -21,7 +22,7 @@
 ;; Used for various architecture options.
 Mask(SH_E)
 
-;; Set if the default precision of th FPU is single.
+;; Set if the default precision of the FPU is single.
 Mask(FPU_SINGLE)
 
 ;; Set if we should generate code using type 2A insns.
@@ -56,11 +57,11 @@
 Generate SH2a FPU-less code
 
 m2a-single
-Target RejectNegative Condition (SUPPORT_SH2A_SINGLE)
+Target RejectNegative Condition(SUPPORT_SH2A_SINGLE)
 Generate default single-precision SH2a code
 
 m2a-single-only
-Target RejectNegative Condition (SUPPORT_SH2A_SINGLE_ONLY)
+Target RejectNegative Condition(SUPPORT_SH2A_SINGLE_ONLY)
 Generate only single-precision SH2a code
 
 m2e
@@ -87,10 +88,33 @@
 Target RejectNegative Condition(SUPPORT_SH4)
 Generate SH4-200 code
 
+;; TARGET_SH4_300 indicates if we have the ST40-300 instruction set and
+;; pipeline - irrespective of ABI.
+m4-300
+Target RejectNegative Condition(SUPPORT_SH4) Var(TARGET_SH4_300)
+Generate SH4-300 code
+
 m4-nofpu
 Target RejectNegative Condition(SUPPORT_SH4_NOFPU)
 Generate SH4 FPU-less code
 
+m4-100-nofpu
+Target RejectNegative Condition(SUPPORT_SH4_NOFPU)
+Generate SH4-100 FPU-less code
+
+m4-200-nofpu
+Target RejectNegative Condition(SUPPORT_SH4_NOFPU)
+Generate SH4-200 FPU-less code
+
+m4-300-nofpu
+Target RejectNegative Condition(SUPPORT_SH4_NOFPU) Var(TARGET_SH4_300) VarExists
+Generate SH4-300 FPU-less code
+
+m4-340
+Target RejectNegative Condition(SUPPORT_SH4_NOFPU) Var(TARGET_SH4_300) VarExists
+Generate code for SH4 340 series (MMU/FPU-less)
+;; passes -isa=sh4-nommu-nofpu to the assembler.
+
 m4-400
 Target RejectNegative Condition(SUPPORT_SH4_NOFPU)
 Generate code for SH4 400 series (MMU/FPU-less)
@@ -113,6 +137,10 @@
 Target RejectNegative Condition(SUPPORT_SH4_SINGLE)
 Generate default single-precision SH4-200 code
 
+m4-300-single
+Target RejectNegative Condition(SUPPORT_SH4_SINGLE) Var(TARGET_SH4_300) VarExists
+Generate default single-precision SH4-300 code
+
 m4-single-only
 Target RejectNegative Condition(SUPPORT_SH4_SINGLE_ONLY)
 Generate only single-precision SH4 code
@@ -125,6 +153,10 @@
 Target RejectNegative Condition(SUPPORT_SH4_SINGLE_ONLY)
 Generate only single-precision SH4-200 code
 
+m4-300-single-only
+Target RejectNegative Condition(SUPPORT_SH4_SINGLE_ONLY) Var(TARGET_SH4_300) VarExists
+Generate only single-precision SH4-300 code
+
 m4a
 Target RejectNegative Mask(SH4A) Condition(SUPPORT_SH4A)
 Generate SH4a code
@@ -173,6 +205,10 @@
 Target Report Mask(ADJUST_UNROLL) Condition(SUPPORT_ANY_SH5)
 Throttle unrolling to avoid thrashing target registers unless the unroll benefit outweighs this
 
+malign-small-blocks=
+Target RejectNegative Joined UInteger Var(sh_align_small_blocks) Init(16)
+Honor align-jump-loops for basic block bigger than number of instructions.
+
 mb
 Target Report RejectNegative InverseMask(LITTLE_ENDIAN)
 Generate code in big endian mode
@@ -181,6 +217,22 @@
 Target Report RejectNegative Mask(BIGTABLE)
 Generate 32-bit offsets in switch tables
 
+mbranch-cost=
+Target RejectNegative Joined UInteger Var(sh_branch_cost) Init(-1)
+Cost to assume for a branch insn
+
+mcbranchdi
+Target Var(TARGET_CBRANCHDI4)
+Enable cbranchdi4 pattern
+
+mexpand-cbranchdi
+Target Var(TARGET_EXPAND_CBRANCHDI4)
+Expand cbranchdi4 pattern early into separate comparisons and branches.
+
+mcmpeqdi
+Target Var(TARGET_CMPEQDI_T)
+Emit cmpeqdi_t pattern even when -mcbranchdi and -mexpand-cbranchdi are in effect.
+
 mcut2-workaround
 Target RejectNegative Var(TARGET_SH5_CUT2_WORKAROUND)
 Enable SH5 cut2 workaround
@@ -189,16 +241,25 @@
 Target Report RejectNegative Mask(ALIGN_DOUBLE)
 Align doubles at 64-bit boundaries
 
+mdb-page-bug
+Target Report RejectNegative Mask(DBHWBUG)
+Undocumented
+
 mdiv=
 Target RejectNegative Joined Var(sh_div_str) Init("")
-Division strategy, one of: call, call2, fp, inv, inv:minlat, inv20u, inv20l, inv:call, inv:call2, inv:fp call-div1 call-fp call-table
+Division strategy, one of: call, call2, fp, inv, inv:minlat, inv20u, inv20l, inv:call, inv:call2, inv:fp, call-div1, call-fp, call-table
 
 mdivsi3_libfunc=
 Target RejectNegative Joined Var(sh_divsi3_libfunc) Init("")
-Specify name for 32 bit signed division function
+Specify name for 32-bit signed division function
 
 mfmovd
-Target RejectNegative Mask(FMOVD) Undocumented
+Target RejectNegative Mask(FMOVD)
+Enable the use of 64-bit floating point registers in fmov instructions
+
+mfused-madd
+Target Var(TARGET_FMAC)
+Enable the use of the fused floating point multiply-accumulate operation
 
 mgettrcost=
 Target RejectNegative Joined UInteger Var(sh_gettrcost) Init(-1)
@@ -216,6 +277,10 @@
 Target Report Mask(ALLOW_INDEXED_ADDRESS) Condition(SUPPORT_ANY_SH5_32MEDIA)
 Enable the use of the indexed addressing mode for SHmedia32/SHcompact
 
+minline-ic_invalidate
+Target Report Var(TARGET_INLINE_IC_INVALIDATE)
+inline code to invalidate instruction cache entries after setting up nested function trampolines
+
 minvalid-symbols
 Target Report Mask(INVALID_SYMBOLS) Condition(SUPPORT_ANY_SH5)
 Assume symbols might be invalid
@@ -228,6 +293,14 @@
 Target Report RejectNegative Mask(LITTLE_ENDIAN)
 Generate code in little endian mode
 
+mlate-r0r3-to-reg-mul
+Target RejectNegative Var(TARGET_R0R3_TO_REG_MUL, 1) VarExists
+Assume availability of integer multiply instruction (src only opd in r0-r3), but only try to use this instruction after register allocation.
+
+mdead-delay
+Target Report Var(TARGET_DEAD_DELAY) Init(0)
+Try to eliminate a dead delay slot instruction.
+
 mnomacsave
 Target Report RejectNegative Mask(NOMACSAVE)
 Mark MAC register as call-clobbered
@@ -246,6 +319,10 @@
 Target Report Mask(PT_FIXED) Condition(SUPPORT_ANY_SH5)
 Assume pt* instructions won't trap
 
+mr0r3-to-reg-mul
+Target Var(TARGET_R0R3_TO_REG_MUL, 2) Init(-1)
+Assume availability of integer multiply instruction (src only opd in r0-r3)
+
 mrelax
 Target Report RejectNegative Mask(RELAX)
 Shorten address references during linking
@@ -254,10 +331,6 @@
 Target Mask(HITACHI) MaskExists
 Follow Renesas (formerly Hitachi) / SuperH calling conventions
 
-mspace
-Target Report RejectNegative Mask(SMALLCODE)
-Deprecated. Use -Os instead
-
 multcost=
 Target RejectNegative Joined UInteger Var(sh_multcost) Init(-1)
 Cost to assume for a multiply insn
Index: gcc/config/sh/ieee-754-df.S
===================================================================
--- gcc/config/sh/ieee-754-df.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/ieee-754-df.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,789 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!! libgcc software floating-point routines for Renesas SH /
+!! STMicroelectronics ST40 CPUs
+!! Contributed by J"orn Rennecke joern.rennecke@st.com
+
+#ifndef __SH_FPU_DOUBLE__
+
+#include "lib1funcs.h"
+#include "insn-constants.h"
+
+/* Double-precision floating-point emulation.
+   We handle NANs, +-infinity, and +-zero.
+   However, we assume that for NANs, the topmost bit of the fraction is set.  */
+
+#ifdef __LITTLE_ENDIAN__
+#define DBL0L r4
+#define DBL0H r5
+#define DBL1L r6
+#define DBL1H r7
+#define DBLRL r0
+#define DBLRH r1
+#else
+#define DBL0L r5
+#define DBL0H r4
+#define DBL1L r7
+#define DBL1H r6
+#define DBLRL r1
+#define DBLRH r0
+#endif
+
+#ifdef __SH_FPU_ANY__
+#define RETURN_R0_MAIN
+#define RETURN_R0 bra LOCAL(return_r0)
+#define RETURN_FR0 \
+LOCAL(return_r0): \
+ lds r0,fpul; \
+ rts; \
+ fsts fpul,fr0
+#define ARG_TO_R4 \
+ flds fr4,fpul; \
+ sts fpul,r4
+#else /* ! __SH_FPU_ANY__ */
+#define RETURN_R0_MAIN rts
+#define RETURN_R0 rts
+#define RETURN_FR0
+#define ARG_TO_R4
+#endif /* ! __SH_FPU_ANY__ */
+
+#ifdef L_nedf2f
+/* -fno-finite-math-only -mb inline version, T := r4:DF == r6:DF
+	cmp/eq	r5,r7
+	mov	r4,r0
+	bf	0f
+	cmp/eq	r4,r6
+	bt	0f
+	or	r6,r0
+	add	r0,r0
+	or	r5,r0
+	tst	r0,r0
+	0:			*/
+	.balign 4
+	.global GLOBAL(nedf2f)
+	HIDDEN_FUNC(GLOBAL(nedf2f))
+GLOBAL(nedf2f):
+	cmp/eq	DBL0L,DBL1L
+	mov.l   LOCAL(c_DF_NAN_MASK),r1
+	bf LOCAL(ne)
+	cmp/eq	DBL0H,DBL1H
+	not	DBL0H,r0
+	bt	LOCAL(check_nan)
+	mov	DBL0H,r0
+	or	DBL1H,r0
+	add	r0,r0
+	rts
+	or	DBL0L,r0
+LOCAL(check_nan):
+	tst	r1,r0
+	rts
+	movt	r0
+LOCAL(ne):
+	rts
+	mov #1,r0
+	.balign 4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(GLOBAL(nedf2f))
+#endif /* L_nedf2f */
+
+#ifdef L_unord_df
+	.balign 4
+	.global GLOBAL(unorddf2)
+	HIDDEN_FUNC(GLOBAL(unorddf2))
+GLOBAL(unorddf2):
+	mov.l	LOCAL(c_DF_NAN_MASK),r1
+	not	DBL0H,r0
+	tst	r1,r0
+	not	r6,r0
+	bt	LOCAL(unord)
+	tst	r1,r0
+LOCAL(unord):
+	rts
+	movt	r0
+	.balign	4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(GLOBAL(unorddf2))
+#endif /* L_unord_df */
+
+#if defined(L_gtdf2t) || defined(L_gtdf2t_trap)
+/* -fno-finite-math-only version of _gt_df */
+#ifdef L_gtdf2t
+#define fun_label GLOBAL(gtdf2t)
+#else
+#define fun_label GLOBAL(gtdf2t_trap)
+#endif
+	.balign 4
+	.global fun_label
+	HIDDEN_FUNC(fun_label)
+fun_label:
+	/* If the raw values compare greater, the result true, unless
+	   any of them is a nan (but infinity is fine), or both values are
+	   +- zero.  Otherwise, the result false.  */
+	mov.l	LOCAL(c_DF_NAN_MASK),r1
+	cmp/pz	DBL0H
+	not	DBL1H,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	DBL0H,r0
+	bt	LOCAL(nan) /* return zero if DBL1 is NAN.  */
+	cmp/eq	DBL1H,DBL0H
+	bt	LOCAL(cmp_low)
+	cmp/gt	DBL1H,DBL0H
+	or	DBL1H,r0
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/gt	DBL0H,r1)
+	add	r0,r0
+	bf	LOCAL(nan) /* return zero if DBL0 is NAN.  */
+	or	DBL0L,r0
+	rts
+	or	DBL1L,r0 /* non-zero unless both DBL0 and DBL1 are +-zero.  */
+LOCAL(cmp_low):
+	cmp/hi	DBL1L,DBL0L
+	rts
+	movt	r0
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan) /* return zero if DBL1 is NAN.  */
+	cmp/eq	DBL1H,DBL0H
+	SLC(bt,	LOCAL(neg_cmp_low),
+	 cmp/hi	DBL0L,DBL1L)
+	not	DBL0H,r0
+	tst	r1,r0
+	bt	LOCAL(nan) /* return zero if DBL0 is NAN.  */
+	cmp/hi	DBL0H,DBL1H
+	SLI(rts	!,)
+	SLI(movt r0 !,)
+LOCAL(neg_cmp_low):
+	SLI(cmp/hi	DBL0L,DBL1L)
+	rts
+	movt	r0
+LOCAL(check_nan):
+#ifdef L_gtdf2t
+LOCAL(nan):
+	rts
+	mov	#0,r0
+#else
+	SLI(cmp/gt DBL0H,r1)
+	bf	LOCAL(nan) /* return zero if DBL0 is NAN.  */
+	rts
+	mov	#0,r0
+LOCAL(nan):
+	mov	#0,r0
+	trapa	#0
+#endif
+	.balign	4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(fun_label)
+#endif /* defined(L_gtdf2t) || defined(L_gtdf2t_trap) */
+
+#ifdef L_gedf2f
+	.balign 4
+	.global GLOBAL(gedf2f)
+	HIDDEN_FUNC(GLOBAL(gedf2f))
+GLOBAL(gedf2f):
+	/* -fno-finite-math-only version of _ge_df */
+	/* If the raw values compare greater or equal, the result is
+	   true, unless any of them is a nan, or both are the
+	   same infinity.  If both are -+zero, the result is true;
+	   otherwise, it is false.
+	   We use 0 as true and nonzero as false for this function.  */
+	mov.l	LOCAL(c_DF_NAN_MASK),r1
+	cmp/pz	DBL1H
+	not	DBL0H,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	DBL0H,r0
+	bt	LOCAL(nan)
+	cmp/eq	DBL0H,DBL1H
+	bt	LOCAL(cmp_low)
+	cmp/gt	DBL0H,DBL1H
+	or	DBL1H,r0
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/ge	r1,DBL1H)
+	add	r0,r0
+	bt	LOCAL(nan)
+	or	DBL0L,r0
+	rts
+	or	DBL1L,r0
+LOCAL(cmp_low):
+	cmp/hi	DBL0L,DBL1L
+#if defined(L_gedf2f) && defined(DELAYED_BRANCHES)
+LOCAL(nan): LOCAL(check_nan):
+#endif
+	rts
+	movt	r0
+#if defined(L_gedf2f) && ! defined(DELAYED_BRANCHES)
+LOCAL(check_nan):
+	SLI(cmp/ge	r1,DBL1H)
+LOCAL(nan):
+	rts
+	movt	r0
+#elif defined(L_gedf2f_trap)
+LOCAL(check_nan):
+	SLI(cmp/ge	r1,DBL1H)
+	bt	LOCAL(nan)
+	rts
+LOCAL(nan):
+	movt	r0
+	trapa	#0
+#endif /* L_gedf2f_trap */
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan)
+	cmp/eq	DBL0H,DBL1H
+	not	DBL1H,r0
+	SLC(bt,	LOCAL(neg_cmp_low),
+	 cmp/hi	DBL1L,DBL0L)
+	tst	r1,r0
+	bt	LOCAL(nan)
+	cmp/hi	DBL1H,DBL0H
+	SLI(rts !,)
+	SLI(movt	r0 !,)
+LOCAL(neg_cmp_low):
+	SLI(cmp/hi	DBL1L,DBL0L)
+	rts
+	movt	r0
+	.balign	4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(GLOBAL(gedf2f))
+#endif /* L_gedf2f */
+
+#ifndef DYN_SHIFT /* SH1 / SH2 code */
+#ifdef L_sf_to_df
+	.balign 4
+	.global GLOBAL(extendsfdf2)
+	FUNC(GLOBAL(extendsfdf2))
+GLOBAL(extendsfdf2):
+	ARG_TO_R4
+	mov.l	LOCAL(x7f800000),r3
+	mov	r4,DBLRL
+	tst	r3,r4
+	bt	LOCAL(zero_denorm)
+	mov.l	LOCAL(xe0000000),r2
+	rotr	DBLRL
+	rotr	DBLRL
+	rotr	DBLRL
+	and	r2,DBLRL
+	mov	r4,DBLRH
+	not	r4,r2
+	tst	r3,r2
+	mov.l	LOCAL(x38000000),r2
+	bf	0f
+	add	r2,r2	! infinity / NaN adjustment
+0:	shll	DBLRH
+	shlr2	DBLRH
+	shlr2	DBLRH
+	add	DBLRH,DBLRH
+	rotcr	DBLRH
+	rts
+	add	r2,DBLRH
+LOCAL(zero_denorm):
+	mov.l	r4,@-r15
+	add	r4,r4
+	tst	r4,r4
+	bt	LOCAL(zero)
+	shlr8	r3	/* 0x007f8000 */
+	mov.w	LOCAL(x389),r2
+LOCAL(shift_byte):
+	tst	r3,r4
+	shll8	r4
+	SL(bt,	LOCAL(shift_byte),
+	 add	#-8,r2)
+LOCAL(shift_bit):
+	shll	r4
+	SL(bf,	LOCAL(shift_bit),
+	 add	#-1,r2)
+	mov	#0,DBLRL
+	mov	r4,DBLRH
+	mov.l	@r15+,r4
+	shlr8	DBLRH
+	shlr2	DBLRH
+	shlr	DBLRH
+	rotcr	DBLRL
+	cmp/gt	r4,DBLRH	! get sign
+	rotcr	DBLRH
+	rotcr	DBLRL
+	shll16	r2
+	shll8	r2
+	rts
+	add	r2,DBLRH
+LOCAL(zero):
+	mov.l	@r15+,DBLRH
+	rts
+	mov	#0,DBLRL
+LOCAL(x389):	.word 0x389
+	.balign	4
+LOCAL(x7f800000):
+	.long	0x7f800000
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(xe0000000):
+	.long	0xe0000000
+	ENDFUNC(GLOBAL(extendsfdf2))
+#endif /* L_sf_to_df */
+
+#ifdef L_df_to_sf
+	.balign 4
+	.global GLOBAL(truncdfsf2)
+	FUNC(GLOBAL(truncdfsf2))
+GLOBAL(truncdfsf2):
+	mov.l	LOCAL(x38000000),r3	! exponent adjustment DF -> SF
+	mov	DBL0H,r1
+	mov.l	LOCAL(x70000000),r2	! mask for out-of-range exponent bits
+	mov	DBL0H,r0
+	mov.l	DBL0L,@-r15
+	sub	r3,r1
+	tst	r2,r1
+	shll8	r0			!
+	shll2	r0			! Isolate highpart fraction.
+	shll2	r0			!
+	bf	LOCAL(ill_exp)
+	shll2	r1
+	mov.l	LOCAL(x2fffffff),r2 /* Fraction lsb | lower guard bits.  */
+	shll2	r1
+	mov.l	LOCAL(xff000000),r3
+	shlr8	r0
+	tst	r2,DBL0L /* Check if msb guard bit wants rounding up.  */
+	shlr16	DBL0L
+	shlr8	DBL0L
+	shlr2	DBL0L
+	SL1(bt,	LOCAL(add_frac),
+	 shlr2	DBL0L)
+	add	#1,DBL0L
+LOCAL(add_frac):
+	add	DBL0L,r0
+	mov.l	LOCAL(x01000000),r2
+	and	r3,r1
+	mov.l	@r15+,DBL0L
+	add	r1,r0
+	tst	r3,r0
+	bt	LOCAL(inf_denorm0)
+	cmp/hs	r3,r0
+LOCAL(denorm_noup_sh1):
+	bt	LOCAL(inf)
+	div0s	DBL0H,r2	/* copy orig. sign into T.  */
+RETURN_R0_MAIN
+	rotcr	r0
+RETURN_FR0
+LOCAL(inf_denorm0):	!  We might need to undo previous rounding.
+	mov.l	LOCAL(x2fffffff),r3 /* Old fraction lsb | lower guard bits.  */
+	tst	r1,r1
+	bf	LOCAL(inf)
+	add	#-1,r0
+	tst	r3,DBL0L /* Check if msb guard bit was rounded up.  */
+	mov.l	LOCAL(x5fffffff),r3 /* Fraction lsb | lower guard bits.  */
+	addc	r2,r0
+	shlr	r0
+	tst	r3,DBL0L /* Check if msb guard bit wants rounding up.  */
+#ifdef DELAYED_BRANCHES
+	bt/s	LOCAL(denorm_noup)
+#else
+	bt	LOCAL(denorm_noup_sh1)
+#endif
+	div0s	DBL0H,r2	/* copy orig. sign into T.  */
+	add	#1,r0
+LOCAL(denorm_noup):
+	RETURN_R0
+	rotcr	r0
+LOCAL(ill_exp):
+	div0s	DBL0H,r1
+	mov.l	LOCAL(x7ff80000),r2
+	add	r1,r1
+	bf	LOCAL(inf_nan)
+	mov.w	LOCAL(m32),r3 /* Handle denormal or zero.  */
+	shlr16	r1
+	exts.w	r1,r1
+	shll2	r1
+	add	r1,r1
+	shlr8	r1
+	exts.w	r1,r1
+	add	#-8,r1	/* Go from 9 to 1 guard bit in MSW.  */
+	cmp/gt	r3,r1
+	mov.l	@r15+,r3 /* DBL0L */
+	bf	LOCAL(zero)
+	mov.l	DBL0L, @-r15
+	shll8	DBL0L
+	rotcr	r0	/* Insert leading 1.  */
+	shlr16	r3
+	shll2	r3
+	add	r3,r3
+	shlr8	r3
+	cmp/pl	DBL0L	/* Check lower 23 guard bits if guard bit 23 is 0.  */
+	addc	r3,r0	/* Assemble fraction with compressed guard bits.  */
+	mov.l	@r15+,DBL0L
+	mov	#0,r2
+	neg	r1,r1
+LOCAL(denorm_loop):
+	shlr	r0
+	rotcl	r2
+	dt	r1
+	bf	LOCAL(denorm_loop)
+	tst	#2,r0
+	rotcl	r0
+	tst	r2,r2
+	rotcl	r0
+	xor	#3,r0
+	add	#3,r0	/* Even overflow gives the correct result.  */
+	shlr2	r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(zero):
+	mov	#0,r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(inf_nan):
+	not	DBL0H,r0
+	tst	r2,r0
+	mov.l	@r15+,DBL0L
+	bf	LOCAL(inf)
+	RETURN_R0
+	mov	#-1,r0	/* NAN */
+LOCAL(inf):	/* r2 must be positive here.  */
+	mov.l	LOCAL(xff000000),r0
+	div0s	r2,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(m32):
+	.word	-32
+	.balign	4
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(x70000000):
+	.long	0x70000000
+LOCAL(x2fffffff):
+	.long	0x2fffffff
+LOCAL(x01000000):
+	.long	0x01000000
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x5fffffff):
+	.long	0x5fffffff
+LOCAL(x7ff80000):
+	.long	0x7ff80000
+	ENDFUNC(GLOBAL(truncdfsf2))
+#endif /*  L_df_to_sf */
+#ifdef L_addsub_df
+#include "IEEE-754/adddf3.S"
+#endif /* _addsub_df */
+
+#ifdef L_mul_df
+#include "IEEE-754/muldf3.S"
+#endif /* L_mul_df */
+
+#ifdef L_df_to_usi
+#include "IEEE-754/fixunsdfsi.S"
+#endif /* L_df_to_usi */
+
+#ifdef L_df_to_si
+#include "IEEE-754/fixdfsi.S"
+#endif /* L_df_to_si */
+
+#ifdef L_usi_to_df
+#include "IEEE-754/floatunssidf.S"
+#endif /* L_usi_to_df */
+
+#ifdef L_si_to_df
+#include "IEEE-754/floatsidf.S"
+#endif /* L_si_df */
+
+#ifdef L_div_df
+#include "IEEE-754/divdf3.S"
+#endif /* L_div_df */
+#endif /* ! DYN_SHIFT */
+
+/* The actual arithmetic uses dynamic shift.  Supporting SH1 / SH2 here would
+   make this code too hard to maintain, so if you want to add SH1 / SH2
+   support, do it in a separate copy.  */
+#ifdef DYN_SHIFT
+#ifdef L_sf_to_df
+	.balign 4
+	.global GLOBAL(extendsfdf2)
+	FUNC(GLOBAL(extendsfdf2))
+GLOBAL(extendsfdf2):
+	ARG_TO_R4
+	mov.l	LOCAL(x7f800000),r2
+	mov	#29,r3
+	mov	r4,DBLRL
+	not	r4,DBLRH
+	tst	r2,r4
+	shld	r3,DBLRL
+	bt	LOCAL(zero_denorm)
+	mov	#-3,r3
+	tst	r2,DBLRH
+	mov	r4,DBLRH
+	mov.l	LOCAL(x38000000),r2
+	bt/s	LOCAL(inf_nan)
+	 shll	DBLRH
+	shld	r3,DBLRH
+	rotcr	DBLRH
+	rts
+	add	r2,DBLRH
+	.balign	4
+LOCAL(inf_nan):
+	shld	r3,DBLRH
+	add	r2,r2
+	rotcr	DBLRH
+	rts
+	add	r2,DBLRH
+LOCAL(zero_denorm):
+	mov.l	r4,@-r15
+	add	r4,r4
+	tst	r4,r4
+	extu.w	r4,r2
+	bt	LOCAL(zero)
+	cmp/eq	r4,r2
+	extu.b	r4,r1
+	mov.l	LOCAL(c__clz_tab),r0
+	bf	LOCAL(three_bytes)
+	nop
+	cmp/eq	r4,r1
+	mov	#22,DBLRH
+	bt	LOCAL(one_byte)
+	shlr8	r2
+	mov	#14,DBLRH
+LOCAL(one_byte):
+#ifdef __pic__
+	add	r0,r2
+	mova  LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r2),r2
+	mov	#21,r3
+	mov.w	LOCAL(x0),DBLRL
+	sub	r2,DBLRH
+LOCAL(norm_shift):
+	shld	DBLRH,r4
+	neg	DBLRH,DBLRH
+	mov.l	@r15+,r2
+	shld	r3,DBLRH
+	mov.l	LOCAL(x6fa00000),r3
+	add	r4,DBLRH
+	mov r2,r4
+	add	r3,DBLRH
+
+	div0s	r3,r4
+	rts
+	rotcr	DBLRH
+LOCAL(three_bytes):
+	mov	r4,r2
+	shlr16	r2
+#ifdef __pic__
+	add	r0,r2
+	mova  LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r2),r2
+	mov	#21,r3
+	mov	#6+10,DBLRH
+	sub	r2,DBLRH
+	mov	r4,DBLRL
+	shld	r3,DBLRL
+	shld	DBLRH,DBLRL
+	bra	LOCAL(norm_shift)
+	add	#-10,DBLRH
+LOCAL(zero):
+	rts	/* DBLRL has already been zeroed above.  */
+	mov.l @r15+,DBLRH
+LOCAL(x0):
+	.word 0
+	.balign	4
+LOCAL(x7f800000):
+	.long	0x7f800000
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(x6fa00000):
+	/* Flip sign back, do exponent adjustment, and remove leading one.  */
+	.long 0x6fa00000 
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+	ENDFUNC(GLOBAL(extendsfdf2))
+#endif /* L_sf_to_df */
+
+#ifdef L_df_to_sf
+	.balign 4
+	.global GLOBAL(truncdfsf2)
+	FUNC(GLOBAL(truncdfsf2))
+GLOBAL(truncdfsf2):
+	mov.l	LOCAL(x38000000),r3
+	mov	DBL0H,r1
+	mov.l	LOCAL(x70000000),r2
+	mov	DBL0H,r0
+	sub	r3,r1
+	mov.l	DBL0L,@-r15
+	tst	r2,r1
+	mov	#12,r3
+	shld	r3,r0			! Isolate highpart fraction.
+	bf	LOCAL(ill_exp)
+	shll2	r1
+	mov.l	LOCAL(x2fffffff),r2 /* Fraction lsb | lower guard bits.  */
+	shll2	r1
+	mov.l	LOCAL(xff000000),r3
+	shlr8	r0
+	tst	r2,DBL0L /* Check if msb guard bit wants rounding up.  */
+	mov	#-28,r2
+	bt/s	LOCAL(add_frac)
+	 shld	r2,DBL0L
+	add	#1,DBL0L
+LOCAL(add_frac):
+	add	DBL0L,r0
+	mov.l	LOCAL(x01000000),r2
+	and	r3,r1
+	mov.l	@r15+,DBL0L
+	add	r1,r0
+	tst	r3,r0
+	bt	LOCAL(inf_denorm0)
+	cmp/hs	r3,r0
+	bt	LOCAL(inf)
+	div0s	DBL0H,r2	/* copy orig. sign into T.  */
+	RETURN_R0_MAIN
+	rotcr	r0
+RETURN_FR0
+LOCAL(inf_denorm0):	! We might need to undo previous rounding.
+	mov.l	LOCAL(x2fffffff),r3 /* Old fraction lsb | lower guard bits.  */
+	tst	r1,r1
+	bf	LOCAL(inf)
+	add	#-1,r0
+	tst	r3,DBL0L /* Check if msb guard bit was rounded up.  */
+	mov.l	LOCAL(x5fffffff),r3 /* Fraction lsb | lower guard bits.  */
+	addc	r2,r0
+	shlr	r0
+	tst	r3,DBL0L /* Check if msb guard bit wants rounding up.  */
+	bt/s	LOCAL(denorm_noup)
+	 div0s	DBL0H,r2	/* copy orig. sign into T.  */
+	add	#1,r0
+LOCAL(denorm_noup):
+	RETURN_R0
+	rotcr	r0
+LOCAL(ill_exp):
+	div0s	DBL0H,r1
+	mov.l	LOCAL(x7ff80000),r2
+	add	r1,r1
+	bf	LOCAL(inf_nan)
+	mov.w	LOCAL(m32),r3 /* Handle denormal or zero.  */
+	mov	#-21,r2
+	shad	r2,r1
+	add	#-8,r1	/* Go from 9 to 1 guard bit in MSW.  */
+	cmp/gt	r3,r1
+	mov.l	@r15+,r3 /* DBL0L */
+	bf	LOCAL(zero)
+	mov.l	DBL0L, @-r15
+	shll8	DBL0L
+	rotcr	r0	/* Insert leading 1.  */
+	shld	r2,r3
+	cmp/pl	DBL0L	/* Check lower 23 guard bits if guard bit 23 is 0.  */
+	addc	r3,r0	/* Assemble fraction with compressed guard bits.  */
+	mov	r0,r2
+	shld	r1,r0
+	mov.l	@r15+,DBL0L
+	add	#32,r1
+	shld	r1,r2
+	tst	#2,r0
+	rotcl	r0
+	tst	r2,r2
+	rotcl	r0
+	xor	#3,r0
+	add	#3,r0	/* Even overflow gives the correct result.  */
+	shlr2	r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(zero):
+	mov	#0,r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(inf_nan):
+	not	DBL0H,r0
+	tst	r2,r0
+	mov.l	@r15+,DBL0L
+	bf	LOCAL(inf)
+	RETURN_R0
+	mov	#-1,r0	/* NAN */
+LOCAL(inf):	/* r2 must be positive here.  */
+	mov.l	LOCAL(xff000000),r0
+	div0s	r2,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(m32):
+	.word	-32
+	.balign	4
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(x70000000):
+	.long	0x70000000
+LOCAL(x2fffffff):
+	.long	0x2fffffff
+LOCAL(x01000000):
+	.long	0x01000000
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x5fffffff):
+	.long	0x5fffffff
+LOCAL(x7ff80000):
+	.long	0x7ff80000
+	ENDFUNC(GLOBAL(truncdfsf2))
+#endif /* L_df_to_sf */
+
+
+#ifdef L_addsub_df
+#include "IEEE-754/m3/adddf3.S"
+#endif /* _addsub_df */
+
+#ifdef L_mul_df
+#include "IEEE-754/m3/muldf3.S"
+#endif /* L_mul_df */
+
+#ifdef L_df_to_usi
+#include "IEEE-754/m3/fixunsdfsi.S"
+#endif /* L_df_to_usi */
+
+#ifdef L_df_to_si
+#include "IEEE-754/m3/fixdfsi.S"
+#endif /* L_df_to_si */
+
+#ifdef L_usi_to_df
+#include "IEEE-754/m3/floatunssidf.S"
+#endif /* L_usi_to_df */
+
+#ifdef L_si_to_df
+#include "IEEE-754/m3/floatsidf.S"
+#endif /* L_si_to_df */
+
+#ifdef L_div_df
+#include "IEEE-754/m3/divdf3.S"
+#endif /* L_div_df */
+#endif /* DYN_SHIFT */
+
+#endif /* __SH_FPU_DOUBLE__ */
Index: gcc/config/sh/predicates.md
===================================================================
--- gcc/config/sh/predicates.md	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/predicates.md	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,5 +1,6 @@
 ;; Predicate definitions for Renesas / SuperH SH.
 ;; Copyright (C) 2005, 2006, 2007 Free Software Foundation, Inc.
+;; Copyright (c) 2006  STMicroelectronics.
 ;;
 ;; This file is part of GCC.
 ;;
@@ -694,6 +695,33 @@
 (define_predicate "symbol_ref_operand"
   (match_code "symbol_ref"))
 
+(define_special_predicate "soft_fp_comparison_operand"
+  (match_code "subreg,reg")
+{
+  switch (GET_MODE (op))
+    {
+    default:
+      return 0;
+    case CC_FP_NEmode: case CC_FP_GTmode: case CC_FP_UNLTmode:
+      break;
+    }
+  return register_operand (op, mode);
+})
+
+(define_predicate "soft_fp_comparison_operator"
+  (match_code "eq, unle, ge")
+{
+  switch (GET_CODE (op))
+    {
+    default:
+      return 0;
+    case EQ:  mode = CC_FP_NEmode;    break;
+    case UNLE:        mode = CC_FP_GTmode;    break;
+    case GE:  mode = CC_FP_UNLTmode;  break;
+    }
+  return register_operand (XEXP (op, 0), mode);
+})
+
 ;; Same as target_reg_operand, except that label_refs and symbol_refs
 ;; are accepted before reload.
 
Index: gcc/config/sh/ushmedia.h
===================================================================
Index: gcc/config/sh/linux-unwind.h
===================================================================
--- gcc/config/sh/linux-unwind.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/linux-unwind.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -45,7 +45,7 @@
 #define SH_DWARF_FRAME_FP0	25
 #define SH_DWARF_FRAME_XD0	87
 #define SH_DWARF_FRAME_PR	17
-#define SH_DWARF_FRAME_GBR	19
+#define SH_DWARF_FRAME_GBR	18
 #define SH_DWARF_FRAME_MACH	20
 #define SH_DWARF_FRAME_MACL	21
 #define SH_DWARF_FRAME_PC	16
@@ -230,8 +230,8 @@
   r = SH_DWARF_FRAME_XD0;
   for (i = 0; i < 8; i++)
     {
-      fs->regs.reg[i].how = REG_SAVED_OFFSET;
-      fs->regs.reg[i].loc.offset
+      fs->regs.reg[r+i].how = REG_SAVED_OFFSET;
+      fs->regs.reg[r+i].loc.offset
 	= (long)&(sc->sc_xfpregs[2*i]) - new_cfa;
     }
 
Index: gcc/config/sh/sh.c
===================================================================
--- gcc/config/sh/sh.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/sh.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -3,6 +3,7 @@
    2003, 2004, 2005, 2006, 2007 Free Software Foundation, Inc.
    Contributed by Steve Chamberlain (sac@cygnus.com).
    Improved by Jim Wilson (wilson@cygnus.com).
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -195,7 +196,9 @@
 static HOST_WIDE_INT rounded_frame_size (int);
 static rtx mark_constant_pool_use (rtx);
 const struct attribute_spec sh_attribute_table[];
+static int sh_cfun_naked_p (void);
 static tree sh_handle_interrupt_handler_attribute (tree *, tree, tree, int, bool *);
+static tree  sh_handle_fndecl_attribute (tree *, tree, tree, int, bool *);
 static tree sh_handle_sp_switch_attribute (tree *, tree, tree, int, bool *);
 static tree sh_handle_trap_exit_attribute (tree *, tree, tree, int, bool *);
 static tree sh_handle_renesas_attribute (tree *, tree, tree, int, bool *);
@@ -270,6 +273,7 @@
 			         tree, bool);
 static int sh_dwarf_calling_convention (tree);
 static int hard_regs_intersect_p (HARD_REG_SET *, HARD_REG_SET *);
+static void sh_expand_float_condop (rtx operands[4], rtx (*[2]) (rtx));
 
 
 /* Initialize the GCC target structure.  */
@@ -403,10 +407,8 @@
 #undef TARGET_MACHINE_DEPENDENT_REORG
 #define TARGET_MACHINE_DEPENDENT_REORG sh_reorg
 
-#ifdef HAVE_AS_TLS
-#undef TARGET_HAVE_TLS
-#define TARGET_HAVE_TLS true
-#endif
+#undef TARGET_DWARF_REGISTER_SPAN
+#define TARGET_DWARF_REGISTER_SPAN sh_dwarf_register_span
 
 #undef TARGET_PROMOTE_PROTOTYPES
 #define TARGET_PROMOTE_PROTOTYPES sh_promote_prototypes
@@ -476,6 +478,9 @@
 #undef TARGET_SECONDARY_RELOAD
 #define TARGET_SECONDARY_RELOAD sh_secondary_reload
 
+#undef TARGET_MATCH_ADJUST
+#define TARGET_MATCH_ADJUST sh_match_adjust
+
 struct gcc_target targetm = TARGET_INITIALIZER;
 
 /* Implement TARGET_HANDLE_OPTION.  */
@@ -525,10 +530,15 @@
     case OPT_m4:
     case OPT_m4_100:
     case OPT_m4_200:
+    case OPT_m4_300:
       target_flags = (target_flags & ~MASK_ARCH) | SELECT_SH4;
       return true;
 
     case OPT_m4_nofpu:
+    case OPT_m4_100_nofpu:
+    case OPT_m4_200_nofpu:
+    case OPT_m4_300_nofpu:
+    case OPT_m4_340:
     case OPT_m4_400:
     case OPT_m4_500:
       target_flags = (target_flags & ~MASK_ARCH) | SELECT_SH4_NOFPU;
@@ -537,12 +547,14 @@
     case OPT_m4_single:
     case OPT_m4_100_single:
     case OPT_m4_200_single:
+    case OPT_m4_300_single:
       target_flags = (target_flags & ~MASK_ARCH) | SELECT_SH4_SINGLE;
       return true;
 
     case OPT_m4_single_only:
     case OPT_m4_100_single_only:
     case OPT_m4_200_single_only:
+    case OPT_m4_300_single_only:
       target_flags = (target_flags & ~MASK_ARCH) | SELECT_SH4_SINGLE_ONLY;
       return true;
 
@@ -648,6 +660,12 @@
     }
 }
 
+static int deleted_delay_slot_p (rtx insn)
+{
+  return (GET_CODE (PATTERN (insn)) == UNSPEC_VOLATILE
+	  && XINT (PATTERN (insn), 1) == UNSPEC_DB_INSN);
+}
+
 /* Print operand x (an rtx) in assembler syntax to file stream
    according to modifier code.
 
@@ -683,7 +701,8 @@
     case '.':
       if (final_sequence
 	  && ! INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0))
-	  && get_attr_length (XVECEXP (final_sequence, 0, 1)))
+	  && (get_attr_length (XVECEXP (final_sequence, 0, 1))
+	      || deleted_delay_slot_p (XVECEXP (final_sequence, 0, 1))))
 	fprintf (stream, ASSEMBLER_DIALECT ? "/s" : ".s");
       break;
     case ',':
@@ -1054,6 +1073,89 @@
   if (! constp)
     return 0;
 
+  if ((TARGET_SH4 || TARGET_SH2A_DOUBLE)
+      && TARGET_FMOVD
+      && !optimize_size && align == 8 && bytes > 4)
+  {
+      rtx dest = copy_rtx (operands[0]);
+      rtx src = copy_rtx (operands[1]);
+      rtx temp = gen_reg_rtx (DFmode);
+      rtx temp1 = gen_reg_rtx (DFmode);
+      rtx temp2 = gen_reg_rtx (DFmode);
+      rtx src_addr = copy_addr_to_reg (XEXP (src, 0));
+      int copied = 0;
+
+      emit_insn (gen_toggle_sz ());
+
+      while (copied + 24 <= bytes)
+	{
+	  rtx to = adjust_address (dest, DFmode, copied);
+	  rtx to1 = adjust_address (dest, DFmode, copied + 8);
+	  rtx to2 = adjust_address (dest, DFmode, copied + 16);
+	  rtx from = adjust_automodify_address (src, DFmode, src_addr, copied);
+	  rtx from1 = adjust_automodify_address (src, DFmode, src_addr, copied + 8);
+	  rtx from2 = adjust_automodify_address (src, DFmode, src_addr, copied + 16);
+
+	  emit_move_insn (temp, from);
+	  emit_move_insn (src_addr, plus_constant (src_addr, 8));
+
+	  emit_move_insn (temp1, from1);
+	  emit_move_insn (src_addr, plus_constant (src_addr, 8));
+
+	  emit_move_insn (temp2, from2);
+	  emit_move_insn (src_addr, plus_constant (src_addr, 8));
+
+	  emit_move_insn (to, temp);
+	  emit_move_insn (to1, temp1);
+	  emit_move_insn (to2, temp2);
+
+	  copied += 24;
+	}
+
+      while (copied + 16 <= bytes)
+	{
+	  rtx to = adjust_address (dest, DFmode, copied);
+	  rtx to1 = adjust_address (dest, DFmode, copied + 8);
+	  rtx from = adjust_automodify_address (src, DFmode, src_addr, copied);
+	  rtx from1 = adjust_automodify_address (src, DFmode, src_addr, copied + 8);
+
+	  emit_move_insn (temp, from);
+	  emit_move_insn (src_addr, plus_constant (src_addr, 8));
+
+	  emit_move_insn (temp1, from1);
+	  emit_move_insn (src_addr, plus_constant (src_addr, 8));
+
+	  emit_move_insn (to, temp);
+	  emit_move_insn (to1, temp1);
+
+	  copied += 16;
+	}
+
+      while (copied + 8 <= bytes)
+	{
+	  rtx to = adjust_address (dest, DFmode, copied);
+	  rtx from = adjust_automodify_address (src, DFmode, src_addr, copied);
+
+	  emit_move_insn (temp, from);
+	  emit_move_insn (src_addr, plus_constant (src_addr, 8));
+	  emit_move_insn (to, temp);
+	  copied += 8;
+	}
+
+      emit_insn (gen_toggle_sz ());
+
+      if (copied < bytes)
+	move_by_pieces (adjust_address (dest, BLKmode, copied),
+			adjust_automodify_address (src, BLKmode,
+						   src_addr, copied),
+			bytes - copied, align, 0);
+
+
+
+      return 1;
+  }
+
+
   /* If we could use mov.l to move words and dest is word-aligned, we
      can use movua.l for loads and still generate a relatively short
      and efficient sequence.  */
@@ -1111,7 +1213,7 @@
 	  emit_insn (gen_block_move_real_i4 (func_addr_rtx));
 	  return 1;
 	}
-      else if (! TARGET_SMALLCODE)
+      else if (! optimize_size)
 	{
 	  const char *entry_name;
 	  rtx func_addr_rtx = gen_reg_rtx (Pmode);
@@ -1150,7 +1252,7 @@
 
   /* This is the same number of bytes as a memcpy call, but to a different
      less common function name, so this will occasionally use more space.  */
-  if (! TARGET_SMALLCODE)
+  if (! optimize_size)
     {
       rtx func_addr_rtx = gen_reg_rtx (Pmode);
       int final_switch, while_loop;
@@ -1338,6 +1440,308 @@
   return 0;
 }
 
+enum rtx_code
+prepare_cbranch_operands (rtx *operands, enum machine_mode mode,
+			  enum rtx_code comparison)
+{
+  rtx op1;
+  rtx scratch = NULL_RTX;
+
+  if (comparison == CODE_FOR_nothing)
+    comparison = GET_CODE (operands[0]);
+  else
+    scratch = operands[4];
+  if (GET_CODE (operands[1]) == CONST_INT
+      && GET_CODE (operands[2]) != CONST_INT)
+    {
+      rtx tmp = operands[1];
+
+      operands[1] = operands[2];
+      operands[2] = tmp;
+      comparison = swap_condition (comparison);
+    }
+  if (GET_CODE (operands[2]) == CONST_INT)
+    {
+      HOST_WIDE_INT val = INTVAL (operands[2]);
+      if ((val == -1 || val == -0x81)
+	  && (comparison == GT || comparison == LE))
+	{
+	  comparison = (comparison == GT) ? GE : LT;
+	  operands[2] = gen_int_mode (val + 1, mode);
+	}
+      else if ((val == 1 || val == 0x80)
+	       && (comparison == GE || comparison == LT))
+	{
+	  comparison = (comparison == GE) ? GT : LE;
+	  operands[2] = gen_int_mode (val - 1, mode);
+	}
+      else if (val == 1 && (comparison == GEU || comparison == LTU))
+	{
+	  comparison = (comparison == GEU) ? NE : EQ;
+	  operands[2] = CONST0_RTX (mode);
+	}
+      else if (val == 0x80 && (comparison == GEU || comparison == LTU))
+	{
+	  comparison = (comparison == GEU) ? GTU : LEU;
+	  operands[2] = gen_int_mode (val - 1, mode);
+	}
+      else if (val == 0 && (comparison == GTU || comparison == LEU))
+	comparison = (comparison == GTU) ? NE : EQ;
+      else if (mode == SImode
+	       && ((val == 0x7fffffff
+		    && (comparison == GTU || comparison == LEU))
+		   || ((unsigned HOST_WIDE_INT) val
+			== (unsigned HOST_WIDE_INT) 0x7fffffff + 1
+		       && (comparison == GEU || comparison == LTU))))
+	{
+	  comparison = (comparison == GTU || comparison == GEU) ? LT : GE;
+	  operands[2] = CONST0_RTX (mode);
+	}
+    }
+  op1 = operands[1];
+  if (!no_new_pseudos)
+    operands[1] = force_reg (mode, op1);
+  /* When we are handling DImode comparisons, we want to keep constants so
+     that we can optimize the component comparisons; however, memory loads
+     are better issued as a whole so that they can be scheduled well.
+     SImode equality comparisons allow I08 constants, but only when they
+     compare r0.  Hence, if operands[1] has to be loaded from somewhere else
+     into a register, that register might as well be r0, and we allow the
+     constant.  If it is already in a register, this is likely to be
+     allocatated to a different hard register, thus we load the constant into
+     a register unless it is zero.  */
+  if (!REG_P (operands[2])
+      && (GET_CODE (operands[2]) != CONST_INT
+	  || (mode == SImode && operands[2] != CONST0_RTX (SImode)
+	      && ((comparison != EQ && comparison != NE)
+		  || (REG_P (op1) && REGNO (op1) != R0_REG)
+		  || !CONST_OK_FOR_I08 (INTVAL (operands[2]))))))
+    {
+      if (scratch && GET_MODE (scratch) == mode)
+	{
+	  emit_move_insn (scratch, operands[2]);
+	  operands[2] = scratch;
+	}
+      else if (!no_new_pseudos)
+	operands[2] = force_reg (mode, operands[2]);
+    }
+  return comparison;
+}
+
+void
+expand_cbranchsi4 (rtx *operands, enum rtx_code comparison, int probability)
+{
+  rtx (*branch_expander) (rtx) = gen_branch_true;
+  rtx jump;
+
+  comparison = prepare_cbranch_operands (operands, SImode, comparison);
+  switch (comparison)
+    {
+    case NE: case LT: case LE: case LTU: case LEU:
+      comparison = reverse_condition (comparison);
+      branch_expander = gen_branch_false;
+    default: ;
+    }
+  emit_insn (gen_rtx_SET (VOIDmode, gen_rtx_REG (SImode, T_REG),
+                          gen_rtx_fmt_ee (comparison, SImode,
+                                          operands[1], operands[2])));
+  jump = emit_jump_insn (branch_expander (operands[3]));
+  if (probability >= 0)
+    REG_NOTES (jump)
+      = gen_rtx_EXPR_LIST (REG_BR_PROB, GEN_INT (probability),
+                           REG_NOTES (jump));
+}
+
+/* ??? How should we distribute probabilities when more than one branch
+   is generated.  So far we only have soem ad-hoc observations:
+   - If the operands are random, they are likely to differ in both parts.
+   - If comparing items in a hash chain, the operands are random or equal;
+     operation should be EQ or NE.
+   - If items are searched in an ordered tree from the root, we can expect
+     the highpart to be unequal about half of the time; operation should be
+     an unequality comparison, operands non-constant, and overall probability
+     about 50%.  Likewise for quicksort.
+   - Range checks will be often made against constants.  Even if we assume for
+     simplicity an even distribution of the non-constant operand over a
+     sub-range here, the same probability could be generated with differently
+     wide sub-ranges - as long as the ratio of the part of the subrange that
+     is before the threshold to the part that comes after the threshold stays
+     the same.  Thus, we can't really tell anything here;
+     assuming random distribution is at least simple.
+ */
+
+bool
+expand_cbranchdi4 (rtx *operands, enum rtx_code comparison)
+{
+  enum rtx_code msw_taken, msw_skip, lsw_taken;
+  rtx skip_label = NULL_RTX;
+  rtx op1h, op1l, op2h, op2l;
+  int num_branches;
+  int prob, rev_prob;
+  int msw_taken_prob = -1, msw_skip_prob = -1, lsw_taken_prob = -1;
+  rtx scratch = operands[4];
+
+  comparison = prepare_cbranch_operands (operands, DImode, comparison);
+  op1h = gen_highpart_mode (SImode, DImode, operands[1]);
+  op2h = gen_highpart_mode (SImode, DImode, operands[2]);
+  op1l = gen_lowpart (SImode, operands[1]);
+  op2l = gen_lowpart (SImode, operands[2]);
+  msw_taken = msw_skip = lsw_taken = CODE_FOR_nothing;
+  prob = split_branch_probability;
+  rev_prob = REG_BR_PROB_BASE - prob;
+  switch (comparison)
+    {
+    /* ??? Should we use the cmpeqdi_t pattern for equality comparisons?
+       That costs 1 cycle more when the first branch can be predicted taken,
+       but saves us mispredicts because only one branch needs prediction.
+       It also enables generating the cmpeqdi_t-1 pattern.  */
+    case EQ:
+      if (TARGET_CMPEQDI_T)
+	{
+	  emit_insn (gen_cmpeqdi_t (operands[1], operands[2]));
+	  emit_jump_insn (gen_branch_true (operands[3]));
+	  return true;
+	}
+      msw_skip = NE;
+      lsw_taken = EQ;
+      if (prob >= 0)
+	{
+	  /* If we had more precision, we'd use rev_prob - (rev_prob >> 32) .
+	   */
+	  msw_skip_prob = rev_prob;
+	  if (REG_BR_PROB_BASE <= 65535)
+	    lsw_taken_prob = prob ? REG_BR_PROB_BASE : 0;
+	  else
+	    {
+	      gcc_assert (HOST_BITS_PER_WIDEST_INT >= 64);
+	      lsw_taken_prob
+		= (prob
+		   ? (REG_BR_PROB_BASE
+		      - ((HOST_WIDEST_INT) REG_BR_PROB_BASE * rev_prob
+			 / ((HOST_WIDEST_INT) prob << 32)))
+		   : 0);
+	    }
+	}
+      break;
+    case NE:
+      if (TARGET_CMPEQDI_T)
+	{
+	  emit_insn (gen_cmpeqdi_t (operands[1], operands[2]));
+	  emit_jump_insn (gen_branch_false (operands[3]));
+	  return true;
+	}
+      msw_taken = NE;
+      msw_taken_prob = prob;
+      lsw_taken = NE;
+      lsw_taken_prob = 0;
+      break;
+    case GTU: case GT:
+      msw_taken = comparison;
+      if (GET_CODE (op2l) == CONST_INT && INTVAL (op2l) == -1)
+	break;
+      if (comparison != GTU || op2h != CONST0_RTX (SImode))
+	msw_skip = swap_condition (msw_taken);
+      lsw_taken = GTU;
+      break;
+    case GEU: case GE:
+      if (op2l == CONST0_RTX (SImode))
+	msw_taken = comparison;
+      else
+	{
+	  msw_taken = comparison == GE ? GT : GTU;
+	  msw_skip = swap_condition (msw_taken);
+	  lsw_taken = GEU;
+	}
+      break;
+    case LTU: case LT:
+      msw_taken = comparison;
+      if (op2l == CONST0_RTX (SImode))
+	break;
+      msw_skip = swap_condition (msw_taken);
+      lsw_taken = LTU;
+      break;
+    case LEU: case LE:
+      if (GET_CODE (op2l) == CONST_INT && INTVAL (op2l) == -1)
+	msw_taken = comparison;
+      else
+	{
+	  lsw_taken = LEU;
+	  if (comparison == LE)
+	    msw_taken = LT;
+	  else if (op2h != CONST0_RTX (SImode))
+	    msw_taken = LTU;
+	  else
+	    break;
+	  msw_skip = swap_condition (msw_taken);
+	}
+      break;
+    default: return false;
+    }
+  num_branches = ((msw_taken != CODE_FOR_nothing)
+		  + (msw_skip != CODE_FOR_nothing)
+		  + (lsw_taken != CODE_FOR_nothing));
+  if (comparison != EQ && comparison != NE && num_branches > 1)
+    {
+      if (!CONSTANT_P (operands[2])
+	  && prob >= (int) (REG_BR_PROB_BASE * 3 / 8U)
+	  && prob <= (int) (REG_BR_PROB_BASE * 5 / 8U))
+	{
+	  msw_taken_prob = prob / 2U;
+	  msw_skip_prob
+	    = REG_BR_PROB_BASE * rev_prob / (REG_BR_PROB_BASE + rev_prob);
+	  lsw_taken_prob = prob;
+	}
+      else
+	{
+	  msw_taken_prob = prob;
+	  msw_skip_prob = REG_BR_PROB_BASE;
+	  /* ??? If we have a constant op2h, should we use that when
+	     calculating lsw_taken_prob?  */
+	  lsw_taken_prob = prob;
+	}
+    }
+  operands[1] = op1h;
+  operands[2] = op2h;
+  operands[4] = NULL_RTX;
+  if (reload_completed
+      && ! arith_reg_or_0_operand (op2h, SImode) && true_regnum (op1h)
+      && (msw_taken != CODE_FOR_nothing || msw_skip != CODE_FOR_nothing))
+    {
+      emit_move_insn (scratch, operands[2]);
+      operands[2] = scratch;
+    }
+  if (msw_taken != CODE_FOR_nothing)
+    expand_cbranchsi4 (operands, msw_taken, msw_taken_prob);
+  if (msw_skip != CODE_FOR_nothing)
+    {
+      rtx taken_label = operands[3];
+
+      /* operands were modified, but msw_skip doesn't expect this.
+	 use the original ones. */
+      if (msw_taken != CODE_FOR_nothing)
+	{
+	  operands[1] = op1h;
+	  operands[2] = op2h;
+	}
+
+      operands[3] = skip_label = gen_label_rtx ();
+      expand_cbranchsi4 (operands, msw_skip, msw_skip_prob);
+      operands[3] = taken_label;
+    }
+  operands[1] = op1l;
+  operands[2] = op2l;
+  if (lsw_taken != CODE_FOR_nothing)
+    {
+      if (reload_completed
+	  && ! arith_reg_or_0_operand (op2l, SImode) && true_regnum (op1l))
+	operands[4] = scratch;
+      expand_cbranchsi4 (operands, lsw_taken, lsw_taken_prob);
+    }
+  if (skip_label)
+    emit_label (skip_label);
+  return true;
+}
+
 /* Prepare the operands for an scc instruction; make sure that the
    compare has been done.  */
 rtx
@@ -1402,6 +1806,76 @@
   return t_reg;
 }
 
+static rtx
+sh_soft_fp_cmp (int code, enum machine_mode op_mode, rtx op0, rtx op1)
+{
+  const char *name = NULL;
+  rtx (*fun) (rtx, rtx), addr, tmp, first, last, equiv;
+  int df = op_mode == DFmode;
+  enum machine_mode mode = CODE_FOR_nothing; /* shut up warning.  */
+
+  switch (code)
+    {
+    case EQ:
+      if (!flag_finite_math_only)
+	{
+	  name = df ? "__nedf2f" : "__nesf2f";
+	  fun = df ? gen_cmpnedf_i1 : gen_cmpnesf_i1;
+	  mode = CC_FP_NEmode;
+	  break;
+	} /* Fall through.  */
+    case UNEQ:
+      fun = gen_cmpuneq_sdf;
+      break;
+    case UNLE:
+      if (flag_finite_math_only && !df)
+	{
+	  fun = gen_cmplesf_i1_finite;
+	  break;
+	}
+      name = df ? "__gtdf2t" : "__gtsf2t";
+      fun = df ? gen_cmpgtdf_i1 : gen_cmpgtsf_i1;
+      mode = CC_FP_GTmode;
+      break;
+    case GE:
+      if (flag_finite_math_only && !df)
+	{
+	  tmp = op0; op0 = op1; op1 = tmp;
+	  fun = gen_cmplesf_i1_finite;
+	  break;
+	}
+      name = df ? "__gedf2f" : "__gesf2f";
+      fun = df ? gen_cmpunltdf_i1 : gen_cmpunltsf_i1;
+      mode = CC_FP_UNLTmode;
+      break;
+    case UNORDERED:
+      fun = gen_cmpun_sdf;
+      break;
+    default: gcc_unreachable ();
+    }
+
+  if (!name)
+    return fun (force_reg (op_mode, op0), force_reg (op_mode, op1));
+
+  tmp = gen_reg_rtx (mode);
+  addr = gen_reg_rtx (Pmode);
+  function_symbol (addr, name, SFUNC_STATIC);
+  first = emit_move_insn (gen_rtx_REG (op_mode, R4_REG), op0);
+  emit_move_insn (gen_rtx_REG (op_mode, R5_REG + df), op1);
+  last = emit_insn (fun (tmp, addr));
+  equiv = gen_rtx_fmt_ee (COMPARE, mode, op0, op1);
+  REG_NOTES (last) = gen_rtx_EXPR_LIST (REG_EQUAL, equiv, REG_NOTES (last));
+  /* Wrap the sequence in REG_LIBCALL / REG_RETVAL notes so that loop
+     invariant code motion can move it.  */
+  REG_NOTES (first) = gen_rtx_INSN_LIST (REG_LIBCALL, last, REG_NOTES (first));
+  REG_NOTES (last) = gen_rtx_INSN_LIST (REG_RETVAL, first, REG_NOTES (last));
+  /* Use fpcmp_i1 rather than cmpeqsi_t, so that the optimizers can grok
+     the computation.  */
+  return gen_rtx_SET (VOIDmode,
+		      gen_rtx_REG (SImode, T_REG),
+		      gen_rtx_fmt_ee (code, SImode, tmp, CONST0_RTX (mode)));
+}
+
 /* Called from the md file, set up the operands of a compare instruction.  */
 
 void
@@ -1411,22 +1885,35 @@
   rtx insn;
   if (mode == VOIDmode)
     mode = GET_MODE (sh_compare_op1);
-  if (code != EQ
-      || mode == DImode
-      || (TARGET_SH2E && GET_MODE_CLASS (mode) == MODE_FLOAT))
+  if (GET_MODE_CLASS (mode) == MODE_FLOAT
+      ? !TARGET_SH1_SOFTFP_MODE (mode)
+      : (code != EQ || mode == DImode))
     {
       /* Force args into regs, since we can't use constants here.  */
       sh_compare_op0 = force_reg (mode, sh_compare_op0);
       if (sh_compare_op1 != const0_rtx
 	  || code == GTU  || code == GEU
-	  || (TARGET_SH2E && GET_MODE_CLASS (mode) == MODE_FLOAT))
+	  || GET_MODE_CLASS (mode) == MODE_FLOAT)
 	sh_compare_op1 = force_reg (mode, sh_compare_op1);
     }
-  if (TARGET_SH2E && GET_MODE_CLASS (mode) == MODE_FLOAT && code == GE)
+  if (GET_MODE_CLASS (mode) == MODE_FLOAT
+      && flag_finite_math_only
+      && !TARGET_SH1_SOFTFP_MODE (mode))
+    switch (code)
+      {
+      case UNGT: code = GT; break;
+      case UNEQ: code = EQ; break;
+      }
+  if (GET_MODE_CLASS (mode) == MODE_FLOAT && code == GE
+      && !TARGET_SH1_SOFTFP_MODE (mode))
     {
       from_compare (operands, GT);
-      insn = gen_ieee_ccmpeqsf_t (sh_compare_op0, sh_compare_op1);
+      insn = gen_ieee_ccmpeqsf_t (copy_rtx (sh_compare_op0),
+				  copy_rtx (sh_compare_op1));
     }
+  else if (GET_MODE_CLASS (mode) == MODE_FLOAT
+	   && TARGET_SH1_SOFTFP_MODE (mode))
+    insn = sh_soft_fp_cmp (code, mode, sh_compare_op0, sh_compare_op1);
   else
     insn = gen_rtx_SET (VOIDmode,
 			gen_rtx_REG (SImode, T_REG),
@@ -1557,7 +2044,7 @@
       && offset - get_attr_length (insn) <= 32766)
     {
       far = 0;
-      jump = "mov.w	%O0,%1; braf	%1";
+      jump = "mov.w	%O0,%1\n\tbraf	%1";
     }
   else
     {
@@ -1565,12 +2052,12 @@
       if (flag_pic)
 	{
 	  if (TARGET_SH2)
-	    jump = "mov.l	%O0,%1; braf	%1";
+	    jump = "mov.l	%O0,%1\n\tbraf	%1";
 	  else
-	    jump = "mov.l	r0,@-r15; mova	%O0,r0; mov.l	@r0,%1; add	r0,%1; mov.l	@r15+,r0; jmp	@%1";
+	    jump = "mov.l	r0,@-r15\n\tmova	%O0,r0\n\t mov.l	@r0,%1\n\tadd	r0,%1\n\t mov.l	@r15+,r0\n\tjmp	@%1";
 	}
       else
-	jump = "mov.l	%O0,%1; jmp	@%1";
+	jump = "mov.l	%O0,%1\n\tjmp	@%1";
     }
   /* If we have a scratch register available, use it.  */
   if (GET_CODE ((prev = prev_nonnote_insn (insn))) == INSN
@@ -1578,7 +2065,7 @@
     {
       this.reg = SET_DEST (XVECEXP (PATTERN (prev), 0, 0));
       if (REGNO (this.reg) == R0_REG && flag_pic && ! TARGET_SH2)
-	jump = "mov.l	r1,@-r15; mova	%O0,r0; mov.l	@r0,r1; add	r1,r0; mov.l	@r15+,r1; jmp	@%1";
+	jump = "mov.l	r1,@-r15\n\t mova	%O0,r0\n\t mov.l	@r0,r1\n\t add	r1,r0\n\t mov.l	@r15+,r1\n\tjmp	@%1";
       output_asm_insn (jump, &this.lab);
       if (dbr_sequence_length ())
 	print_slot (final_sequence);
@@ -1662,8 +2149,12 @@
 	      && ! INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0))
 	      && get_attr_length (XVECEXP (final_sequence, 0, 1)))
 	    {
-	      asm_fprintf (asm_out_file, "\tb%s%ss\t%LLF%d\n", logic ? "f" : "t",
+	      asm_fprintf (asm_out_file, "\tb%s%ss\t%LLF%d", logic ? "f" : "t",
 	                   ASSEMBLER_DIALECT ? "/" : ".", label);
+
+	      asm_fprintf (asm_out_file, "\t%s %d\t[length = %d]\n",
+			   ASM_COMMENT_START, INSN_UID (insn),
+			   get_attr_length (insn));
 	      print_slot (final_sequence);
 	    }
 	  else
@@ -1720,6 +2211,12 @@
     }
 }
 
+/* Output a code sequence for INSN using TEMPLATE with OPERANDS; but before,
+   fill in operands 9 as a label to the successor insn.
+   We try to use jump threading where possible.
+   IF CODE matches the comparison in the IF_THEN_ELSE of a following jump,
+   we assume the jump is taken.  I.e. EQ means follow jmp and bf, NE means
+   follow jmp and bt, if the address is in range.  */
 const char *
 output_branchy_insn (enum rtx_code code, const char *template,
 		     rtx insn, rtx *operands)
@@ -2058,21 +2555,21 @@
        Using a multiply first and splitting it later if it's a loss
        doesn't work because of different sign / zero extension semantics
        of multiplies vs. shifts.  */
-    return TARGET_SMALLCODE ? 2 : 3;
+    return optimize_size ? 2 : 3;
 
   if (TARGET_SH2)
     {
       /* We have a mul insn, so we can never take more than the mul and the
 	 read of the mac reg, but count more because of the latency and extra
 	 reg usage.  */
-      if (TARGET_SMALLCODE)
+      if (optimize_size)
 	return 2;
       return 3;
     }
 
   /* If we're aiming at small code, then just count the number of
      insns in a multiply call sequence.  */
-  if (TARGET_SMALLCODE)
+  if (optimize_size)
     return 5;
 
   /* Otherwise count all the insns in the routine we'd be calling too.  */
@@ -2114,6 +2611,15 @@
       else if ((outer_code == AND || outer_code == IOR || outer_code == XOR)
 	       && CONST_OK_FOR_K08 (INTVAL (x)))
         *total = 1;
+      /* prepare_cmp_insn will force costly constants int registers before
+	 the cbrach[sd]i4 pattterns can see them, so preserve potentially
+	 interesting ones not covered by I08 above.  */
+      else if (outer_code == COMPARE
+	       && ((unsigned HOST_WIDE_INT) INTVAL (x)
+		    == (unsigned HOST_WIDE_INT) 0x7fffffff + 1
+		    || INTVAL (x) == 0x7fffffff
+		   || INTVAL (x) == 0x80 || INTVAL (x) == -0x81))
+        *total = 1;
       else
         *total = 8;
       return true;
@@ -2132,6 +2638,11 @@
     case CONST_DOUBLE:
       if (TARGET_SHMEDIA)
         *total = COSTS_N_INSNS (4);
+      /* prepare_cmp_insn will force costly constants int registers before
+	 the cbrachdi4 patttern can see them, so preserve potentially
+	 interesting ones.  */
+      else if (outer_code == COMPARE && GET_MODE (x) == DImode)
+        *total = 1;
       else
         *total = 10;
       return true;
@@ -3551,6 +4062,12 @@
   si_limit = 1018;
   hi_limit = 510;
 
+  if (TARGET_DBHWBUG)
+    {
+      hi_limit -= 2;
+      si_limit -= 2;
+    }
+
   while (from && count_si < si_limit && count_hi < hi_limit)
     {
       int inc = get_attr_length (from);
@@ -3590,7 +4107,7 @@
 
       if (GET_CODE (from) == BARRIER)
 	{
-
+	  rtx next;
 	  found_barrier = from;
 
 	  /* If we are at the end of the function, or in front of an alignment
@@ -3598,6 +4115,14 @@
 	     this kind of barrier.  */
 	  if (barrier_align (from) > 2)
 	    good_barrier = from;
+
+	  /* If we are at the end of a hot/cold block, dump the constants
+	     here.  */
+	  next = NEXT_INSN (from);
+	  if (next
+	      && NOTE_P (next)
+	      && NOTE_LINE_NUMBER (next) == NOTE_INSN_SWITCH_TEXT_SECTIONS)
+	    break;
 	}
 
       if (broken_move (from))
@@ -3696,9 +4221,36 @@
       /* For the SH1, we generate alignments even after jumps-around-jumps.  */
       else if (GET_CODE (from) == JUMP_INSN
 	       && ! TARGET_SH2
-	       && ! TARGET_SMALLCODE)
+	       && ! optimize_size)
+	new_align = 4;
+
+      /* long jumps will change the alignment for the .long label.  */
+      else if (GET_CODE (from) == JUMP_INSN
+	       && GET_CODE (PATTERN (from)) == SET
+	       && recog_memoized (from) == CODE_FOR_jump_compact
+	       && inc == 10)
 	new_align = 4;
 
+      /* There is a possibility that a bf is transformed into a bf/s by the
+	 delay slot scheduler, Conservatively handle this case.  */
+      if (GET_CODE (from) == JUMP_INSN
+	  && GET_CODE (PATTERN (from)) != ADDR_DIFF_VEC
+	  && GET_CODE (PATTERN (from)) != ADDR_VEC
+	  && get_attr_type (from) == TYPE_CBRANCH
+	  && GET_CODE (PATTERN (NEXT_INSN (PREV_INSN (from)))) != SEQUENCE)
+	inc += 2;
+
+      if (TARGET_DBHWBUG
+	  && ((GET_CODE (from) == INSN
+	       && GET_CODE (PATTERN (from)) != USE
+	       && GET_CODE (PATTERN (from)) != CLOBBER)
+	      || GET_CODE (from) == CALL_INSN
+	      || (GET_CODE (from) == JUMP_INSN
+		  && GET_CODE (PATTERN (from)) != ADDR_DIFF_VEC
+		  && GET_CODE (PATTERN (from)) != ADDR_VEC))
+	  && get_attr_needs_delay_slot (from) == NEEDS_DELAY_SLOT_YES)
+	inc += 2;
+
       if (found_si)
 	{
 	  count_si += inc;
@@ -3708,6 +4260,9 @@
 	      si_align = new_align;
 	    }
 	  count_si = (count_si + new_align - 1) & -new_align;
+
+	  if (new_align < si_align)
+	    si_align = new_align;	    
 	}
       if (found_hi)
 	{
@@ -3718,6 +4273,9 @@
 	      hi_align = new_align;
 	    }
 	  count_hi = (count_hi + new_align - 1) & -new_align;
+
+	  if (new_align < hi_align)
+	    hi_align = new_align;	    
 	}
       from = NEXT_INSN (from);
     }
@@ -4241,6 +4799,27 @@
     }
 }
 
+int sh_jump_align (rtx label)
+{
+  if (sh_align_small_blocks && TARGET_CACHE32)
+    {
+      rtx insn;
+      int size = 0;
+
+      for (insn = NEXT_INSN (label);
+	   insn && GET_CODE (insn) != BARRIER &&
+	     GET_CODE (insn) != CODE_LABEL;
+	   insn = NEXT_INSN (insn))
+	if (INSN_P (insn))
+	  size += get_attr_min_length (insn);
+
+      if (size <= sh_align_small_blocks)
+	return 0;
+    }
+
+  return align_jumps_log;
+}
+
 /* BARRIER_OR_LABEL is either a BARRIER or a CODE_LABEL immediately following
    a barrier.  Return the base 2 logarithm of the desired alignment.  */
 int
@@ -4248,6 +4827,7 @@
 {
   rtx next = next_real_insn (barrier_or_label), pat, prev;
   int slot, credit, jump_to_next = 0;
+  rtx label;
 
   if (! next)
     return 0;
@@ -4267,13 +4847,13 @@
       pat = PATTERN (prev);
       /* If this is a very small table, we want to keep the alignment after
 	 the table to the minimum for proper code alignment.  */
-      return ((TARGET_SMALLCODE
+      return ((optimize_size
 	       || ((unsigned) XVECLEN (pat, 1) * GET_MODE_SIZE (GET_MODE (pat))
 		   <= (unsigned) 1 << (CACHE_LOG - 2)))
 	      ? 1 << TARGET_SHMEDIA : align_jumps_log);
     }
 
-  if (TARGET_SMALLCODE)
+  if (optimize_size)
     return 0;
 
   if (! TARGET_SH2 || ! optimize)
@@ -4354,9 +4934,39 @@
 	}
     }
 
+  if (sh_align_small_blocks && TARGET_CACHE32)
+    {
+      label = next_nonnote_insn (barrier_or_label);
+      if (label && GET_CODE (label) == CODE_LABEL)
+	{
+	  rtx insn;
+	  int size = 0;
+
+	  for (insn = NEXT_INSN (label);
+	       insn && GET_CODE (insn) != BARRIER &&
+		 GET_CODE (insn) != CODE_LABEL;
+	       insn = NEXT_INSN (insn))
+	    if (INSN_P (insn))
+	      size += get_attr_min_length (insn);
+
+	  if (size <= sh_align_small_blocks)
+	    return 0;
+	}
+    }
+
   return align_jumps_log;
 }
 
+static bool in_between(rtx start, rtx end, rtx r)
+{
+  rtx scan;
+  for (scan = NEXT_INSN (start); scan && scan != end; scan = NEXT_INSN (scan))
+    if (scan == r)
+      return 1;
+
+  return 0;
+}
+
 /* If we are inside a phony loop, almost any kind of label can turn up as the
    first one in the loop.  Aligning a braf label causes incorrect switch
    destination addresses; we can detect braf labels because they are
@@ -4381,6 +4991,8 @@
   return align_loops_log;
 }
 
+static int fixup_addr;
+
 /* Do a final pass over the function, just before delayed branch
    scheduling.  */
 
@@ -4552,7 +5164,13 @@
                  instructions at the jump destination did not use REG.  */
 
 	      if (GET_CODE (scan) == JUMP_INSN)
+		{
+		  rtx next = next_active_insn (JUMP_LABEL (scan));
+
+		  if (next && in_between (link, scan, next))
+		    continue;
 		break;
+		}
 
 	      if (! reg_mentioned_p (reg, scan))
 		continue;
@@ -4838,7 +5456,7 @@
     PUT_MODE (insn, VOIDmode);
 
   mdep_reorg_phase = SH_SHORTEN_BRANCHES1;
-  INSN_ADDRESSES_FREE ();
+  init_insn_lengths ();
   split_branches (first);
 
   /* The INSN_REFERENCES_ARE_DELAYED in sh.h is problematic because it
@@ -4865,6 +5483,8 @@
     REG_USERVAR_P (get_fpscr_rtx ()) = 0;
 #endif
   mdep_reorg_phase = SH_AFTER_MDEP_REORG;
+
+   fixup_addr = 0;
 }
 
 int
@@ -5122,10 +5742,15 @@
    variable length.  This is because the second pass of shorten_branches
    does not bother to update them.  */
 
+static void sh_hw_workaround (rtx insn);
+
 void
 final_prescan_insn (rtx insn, rtx *opvec ATTRIBUTE_UNUSED,
 		    int noperands ATTRIBUTE_UNUSED)
 {
+  if (TARGET_DBHWBUG)
+    sh_hw_workaround (insn);
+
   if (TARGET_DUMPISIZE)
     fprintf (asm_out_file, "\n! at %04x\n", INSN_ADDRESSES (INSN_UID (insn)));
 
@@ -5452,13 +6077,13 @@
 static void
 push_regs (HARD_REG_SET *mask, int interrupt_handler)
 {
-  int i;
+  int i = interrupt_handler ? LAST_BANKED_REG + 1 : 0;
   int skip_fpscr = 0;
 
   /* Push PR last; this gives better latencies after the prologue, and
      candidates for the return delay slot when there are no general
      registers pushed.  */
-  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
+  for (; i < FIRST_PSEUDO_REGISTER; i++)
     {
       /* If this is an interrupt handler, and the SZ bit varies,
 	 and we have to push any floating point register, we need
@@ -5478,6 +6103,13 @@
 	  && TEST_HARD_REG_BIT (*mask, i))
 	push (i);
     }
+
+  /* Push banked registers last to improve delay slot opportunities.  */
+  if (interrupt_handler)
+    for (i = FIRST_BANKED_REG; i <= LAST_BANKED_REG; i++)
+      if (TEST_HARD_REG_BIT (*mask, i))
+	push (i);
+
   if (TEST_HARD_REG_BIT (*mask, PR_REG))
     push (PR_REG);
 }
@@ -5861,6 +6493,9 @@
   tree sp_switch_attr
     = lookup_attribute ("sp_switch", DECL_ATTRIBUTES (current_function_decl));
 
+  if (sh_cfun_naked_p ())
+    return;
+
   current_function_interrupt = sh_cfun_interrupt_handler_p ();
 
   /* We have pretend args if we had an object sent partially in registers
@@ -6246,6 +6881,9 @@
   int fpscr_deferred = 0;
   int e = sibcall_p ? -1 : 1;
 
+  if (sh_cfun_naked_p ())
+    return;
+
   d = calc_live_regs (&live_regs_mask);
 
   save_size = d;
@@ -6445,10 +7083,26 @@
     }
   else /* ! TARGET_SH5 */
     {
+      int last_reg;
+
       save_size = 0;
       if (TEST_HARD_REG_BIT (live_regs_mask, PR_REG))
 	pop (PR_REG);
-      for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
+
+      /* Banked registers are poped first to avoid being scheduled in the
+	 delay slot. RTE switches banks before the ds instruction.  */
+      if (current_function_interrupt)
+	{
+	  for (i = FIRST_BANKED_REG; i <= LAST_BANKED_REG; i++)
+	    if (TEST_HARD_REG_BIT (live_regs_mask, i))
+	      pop (LAST_BANKED_REG - i);
+
+	  last_reg = FIRST_PSEUDO_REGISTER - LAST_BANKED_REG - 1;
+	}
+      else
+	last_reg = FIRST_PSEUDO_REGISTER;
+
+      for (i = 0; i < last_reg; i++)
 	{
 	  int j = (FIRST_PSEUDO_REGISTER - 1) - i;
 
@@ -6458,9 +7112,9 @@
 	    fpscr_deferred = 1;
 	  else if (j != PR_REG && TEST_HARD_REG_BIT (live_regs_mask, j))
 	    pop (j);
+
 	  if (j == FIRST_FP_REG && fpscr_deferred)
 	    pop (FPSCR_REG);
-
 	}
     }
   if (target_flags != save_flags && ! current_function_interrupt)
@@ -7090,6 +7744,23 @@
   return result;
 }
 
+rtx
+sh_dwarf_register_span (rtx reg)
+{
+  unsigned regno = REGNO (reg);
+
+  if (WORDS_BIG_ENDIAN || GET_MODE (reg) != DFmode)
+    return NULL_RTX;
+
+  return
+    gen_rtx_PARALLEL (VOIDmode,
+		      gen_rtvec (2,
+				 gen_rtx_REG (SFmode,
+					      DBX_REGISTER_NUMBER (regno+1)),
+				 gen_rtx_REG (SFmode,
+					      DBX_REGISTER_NUMBER (regno))));
+}
+
 bool
 sh_promote_prototypes (tree type)
 {
@@ -7703,9 +8374,27 @@
   { "dllimport",         0, 0, true,  false, false, sh_symbian_handle_dll_attribute },
   { "dllexport",         0, 0, true,  false, false, sh_symbian_handle_dll_attribute },
 #endif
+  /* don't generate function prologue/epilogue and `ret' command.  */
+  { "naked",             0, 0, true,  false, false, sh_handle_fndecl_attribute },
   { NULL,                0, 0, false, false, false, NULL }
 };
 
+/* Handle an attribute requiring a FUNCTION_DECL;
+   arguments as in struct attribute_spec.handler.  */
+static tree
+sh_handle_fndecl_attribute (tree *node, tree name, tree args ATTRIBUTE_UNUSED,
+			     int flags ATTRIBUTE_UNUSED, bool *no_add_attrs)
+{
+  if (TREE_CODE (*node) != FUNCTION_DECL)
+    {
+      warning (OPT_Wattributes, "%qs attribute only applies to functions",
+	       IDENTIFIER_POINTER (name));
+      *no_add_attrs = true;
+    }
+
+  return NULL_TREE;
+}
+
 /* Handle an "interrupt_handler" attribute; arguments as in
    struct attribute_spec.handler.  */
 static tree
@@ -7819,6 +8508,15 @@
 	  != NULL_TREE);
 }
 
+/* Return nonzero if the current function has attribute naked .  */
+static int
+sh_cfun_naked_p (void)
+{
+  return (lookup_attribute ("naked",
+			    DECL_ATTRIBUTES (current_function_decl))
+	  != NULL_TREE);
+}
+
 /* Implement TARGET_CHECK_PCH_TARGET_FLAGS.  */
 
 static const char *
@@ -8113,6 +8811,69 @@
 			get_fpscr_rtx ()));
 }
 
+/* Expand an sfunc operation taking NARGS MODE arguments, using generator
+   function FUN, which needs symbol NAME loaded int a register first.
+   Add a REG_EQUAL note using EQUIV.  */
+static void
+expand_sfunc_op (int nargs, enum machine_mode mode, rtx (*fun) (rtx, rtx),
+		 const char *name, rtx equiv, rtx *operands)
+{
+  int i;
+  rtx addr, first = NULL_RTX, last, insn;
+  /* For now keep all variants ABI compatibilities.
+     Check for TARGET_OSFP: ARG_TO_R4 (see ieee-754-df.S) and _SH_FPU_ANY_.  */
+  int next_reg = FIRST_PARM_REG;
+
+  addr = gen_reg_rtx (Pmode);
+  function_symbol (addr, name, SFUNC_FREQUENT);
+
+  for (i = 1; i <= nargs; i++)
+    {
+      insn = emit_move_insn (gen_rtx_REG (mode, next_reg), operands[i]);
+      if (!first)
+	first = insn;
+      next_reg += GET_MODE_SIZE (mode) / UNITS_PER_WORD;
+    }
+  last = emit_insn ((*fun) (operands[0], addr));
+  REG_NOTES (last) = gen_rtx_EXPR_LIST (REG_EQUAL, equiv, REG_NOTES (last));
+
+  /* If flag_non_call_exceptions is in effect, it will stipulate BB boundaries
+     where we don't want them; we must not have a LIBCALL block spanning
+     multiple basic blocks.  */
+  if (flag_non_call_exceptions)
+    {
+      for (insn = first; insn != last; insn = NEXT_INSN (insn))
+	if (may_trap_p (insn))
+	  return;
+    }
+  /* Wrap the sequence in REG_LIBCALL / REG_RETVAL notes so that loop
+     invariant code motion can move it.  */
+  REG_NOTES (first) = gen_rtx_INSN_LIST (REG_LIBCALL, last, REG_NOTES (first));
+  REG_NOTES (last) = gen_rtx_INSN_LIST (REG_RETVAL, first, REG_NOTES (last));
+}
+
+/* Expand an sfunc unary operation taking an MODE argument, using generator
+   function FUN, which needs symbol NAME loaded int a register first.
+   Add a REG_EQUAL note using CODE.  */
+void
+expand_sfunc_unop (enum machine_mode mode, rtx (*fun) (rtx, rtx),
+		   const char *name, enum rtx_code code, rtx *operands)
+{
+  rtx equiv = gen_rtx_fmt_e (code, GET_MODE (operands[0]), operands[1]);
+  expand_sfunc_op (1, mode, fun, name, equiv, operands);
+}
+
+/* Expand an sfunc binary operation in MODE, using generator function FUN,
+   which needs symbol NAME loaded int a register first.
+   Add a REG_EQUAL note using CODE.  */
+void
+expand_sfunc_binop (enum machine_mode mode, rtx (*fun) (rtx, rtx),
+		    const char *name, enum rtx_code code, rtx *operands)
+{
+  rtx equiv = gen_rtx_fmt_ee (code, mode, operands[1], operands[2]);
+  expand_sfunc_op (2, mode, fun, name, equiv, operands);
+}
+
 /* ??? gcc does flow analysis strictly after common subexpression
    elimination.  As a result, common subexpression elimination fails
    when there are some intervening statements setting the same register.
@@ -8221,6 +8982,13 @@
   return gen_rtx_REG (Pmode, 7);
 }
 
+/* This function switches the fpscr.  */
+void
+emit_fpu_flip (void)
+{
+  emit_insn (gen_toggle_pr ());
+}
+
 /* This function will set the fpscr from memory.
    MODE is the mode we are setting it to.  */
 void
@@ -8238,9 +9006,236 @@
 #define IS_ASM_LOGICAL_LINE_SEPARATOR(C) ((C) == ';')
 #endif
 
+static int
+sh_forward_branch_p(rtx first, int n)
+{
+  int new_align;
+  rtx insn;
+  rtx lab = JUMP_LABEL (first);
+  int njumps = 0;
+
+  for (insn = NEXT_INSN (first); insn; insn = NEXT_INSN (insn))
+    {
+      if (((GET_CODE (insn) == CALL_INSN
+	    || (GET_CODE (insn) == JUMP_INSN
+		&& GET_CODE (PATTERN (insn)) != ADDR_DIFF_VEC
+		&& GET_CODE (PATTERN (insn)) != ADDR_VEC))
+	   && num_delay_slots (insn))
+	  || (GET_CODE (insn) == INSN && GET_CODE (PATTERN (insn)) == SEQUENCE))
+	njumps += get_attr_length (insn) + 2;
+
+      else if (GET_CODE (insn) == INSN
+	       && GET_CODE (PATTERN (insn)) == UNSPEC_VOLATILE
+	       && XINT (PATTERN (insn), 1) == UNSPECV_ALIGN)
+	{
+	  new_align = 1 << INTVAL (XVECEXP (PATTERN (insn), 0, 0));
+	  njumps += new_align;
+	}
+
+      else if (GET_CODE (insn) == CODE_LABEL)
+	{
+	  new_align = label_to_alignment (insn);
+	  if (new_align)
+	    new_align = 1 << new_align;
+	  njumps += new_align;
+	}
+
+      else
+	njumps += get_attr_length (insn);
+
+      if (insn == lab)
+	  return njumps > (n * 2);
+    }
+
+  njumps = 0;
+
+  for (insn = PREV_INSN (first); insn; insn = PREV_INSN (insn))
+    {
+      if (((GET_CODE (insn) == CALL_INSN
+	    || ((GET_CODE (insn) == JUMP_INSN)
+		&& GET_CODE (PATTERN (insn)) != ADDR_DIFF_VEC
+		&& GET_CODE (PATTERN (insn)) != ADDR_VEC))
+	   && num_delay_slots (insn))
+	  || (GET_CODE (insn) == INSN && GET_CODE (PATTERN (insn)) == SEQUENCE))
+	njumps += get_attr_length (insn) + 2;
+
+      else if (GET_CODE (insn) == INSN
+	       && GET_CODE (PATTERN (insn)) == UNSPEC_VOLATILE
+	       && XINT (PATTERN (insn), 1) == UNSPECV_ALIGN)
+	{
+	  new_align = 1 << INTVAL (XVECEXP (PATTERN (insn), 0, 0));
+	  njumps += new_align;
+	}
+
+      else if (GET_CODE (insn) == CODE_LABEL)
+	{
+	  new_align = label_to_alignment (insn);
+	  if (new_align)
+	    new_align = 1 << new_align;
+	  njumps += new_align;
+	}
+
+      else
+	njumps += get_attr_length (insn);
+
+      if (insn == lab)
+	return njumps > (n * 2);
+    }
+
+  return 0;
+}
+
 int
-sh_insn_length_adjustment (rtx insn)
+sh_insn_length_adjustment (rtx insn, const int cur_length, const int special)
 {
+  int addlen = 0;
+
+  if (special)
+    {
+      if ((GET_CODE (insn) == INSN) &&
+	  (recog_memoized (insn) == CODE_FOR_casesi_worker_1))
+	{
+	  rtx src = SET_SRC (XVECEXP (PATTERN (insn), 0, 0));
+	  rtx lab, diff_vec;
+
+	  if (MEM_P (src))
+	    src = XEXP (src, 0);
+
+	  lab = XEXP (XVECEXP (src, 0, 2), 0);
+	  diff_vec = PATTERN (next_real_insn (lab));
+
+	  if (GET_MODE (diff_vec) == QImode)
+	    {
+	      if (!ADDR_DIFF_VEC_FLAGS (diff_vec).offset_unsigned &&
+		  cur_length == 4)
+		return -2;
+	      if (ADDR_DIFF_VEC_FLAGS (diff_vec).offset_unsigned &&
+		  cur_length == 2)
+		return 2;
+	    }
+	}
+
+      return 0;
+    }
+
+  /* optimize DC introduced by reorg.  */
+  if (TARGET_DEAD_DELAY && mdep_reorg_phase == SH_AFTER_MDEP_REORG)
+    {
+      if (deleted_delay_slot_p (insn))
+	return 0;
+
+      if (NONJUMP_INSN_P (insn) && GET_CODE (PATTERN (insn)) == SEQUENCE
+	  && cur_length == 4 && INSN_ADDRESSES_SET_P ())
+	{
+	  rtx body = PATTERN (insn);
+	  rtx delay_insn = XVECEXP (body, 0, 1);
+	  rtx dpat = PATTERN (delay_insn);
+	  rtx nexti = next_real_insn (delay_insn);
+	  rtx label = NEXT_INSN (delay_insn);
+	  int log = 0;
+
+	  if (label && BARRIER_P (label))
+	    {
+	      for (label = label; label && ! INSN_P (label);
+		   label = NEXT_INSN (label))
+		if (LABEL_P (label))
+		  {
+		    log = LABEL_ALIGN_AFTER_BARRIER (label);
+		    break;
+		  }
+	    }
+
+	  if (nexti && GET_CODE (dpat) == SET)
+	    {
+	      rtx jump_insn = XVECEXP (body, 0, 0);
+	      enum attr_type jump_type = get_attr_type (jump_insn);
+
+	      if ((jump_type != TYPE_SFUNC && jump_type != TYPE_CALL)
+		  && GET_CODE (jump_insn) == JUMP_INSN
+		  && rtx_equal_p (PATTERN (nexti), dpat)
+		  && ! reg_overlap_mentioned_p (SET_DEST (dpat), dpat))
+		{
+		  rtx lab = JUMP_LABEL (jump_insn);
+		  rtx prev = prev_real_insn (lab);
+
+		  /* bug 50754 optimize 2 instructions :
+		     br .l
+		     mov r1,r3
+		     .l: mov r1,r3  */
+		  if (nexti == prev)
+		    {
+		      delete_insn (insn);
+		      return -4;
+		    }
+		  /* bug 58105: optimize 1 instruction :
+		     bf/s .l
+		     mov r1,r3
+		     mov r1,r3  */
+		  else if (!log)
+		    {
+		      rtx old_delay = delay_insn;
+		      delay_insn = make_insn_raw (gen_dup_db_insn ());
+
+		      NEXT_INSN (delay_insn) = NEXT_INSN (old_delay);
+		      PREV_INSN (delay_insn) = jump_insn;
+		      NEXT_INSN (jump_insn) = delay_insn;
+		      XVECEXP (body, 0, 1) = delay_insn;		
+
+		      INSN_ADDRESSES_NEW (delay_insn, -1);
+		      return -2;
+		    }
+		}
+	    }
+	}
+    }
+
+  if (TARGET_DBHWBUG
+      && mdep_reorg_phase == SH_AFTER_MDEP_REORG && INSN_ADDRESSES_SET_P ())
+    {
+      if (recog_memoized (insn) == CODE_FOR_tls_global_dynamic
+	  || recog_memoized (insn) == CODE_FOR_tls_local_dynamic
+	  || recog_memoized (insn) == CODE_FOR_tls_initial_exec)
+	return -2;
+
+      if (recog_memoized (insn) == CODE_FOR_jump_compact)
+	{
+	  if (cur_length == 2)
+	    {
+	      int long_b = sh_forward_branch_p (insn, 0x7ff);
+
+	      if (long_b)
+		{
+		  rtx prev;
+		  int xlen = 0;
+		
+		  /* will need a scratch register.  */
+		  if (GET_CODE ((prev = prev_nonnote_insn (insn))) != INSN
+		      || INSN_CODE (prev) != CODE_FOR_indirect_jump_scratch)
+		    xlen = 4;
+
+		  if (!xlen)
+		    {
+		      /* a nop will be inserted. count it.  */
+		      /* mov r0 lab; braf r0; nop.  */
+		      if (GET_CODE (PATTERN (NEXT_INSN (PREV_INSN (insn)))) != SEQUENCE)
+			xlen += 2;
+		    }
+		  addlen = 4 + xlen;
+		  return addlen;
+		}
+	    }
+	}
+
+      if ((recog_memoized (insn) == CODE_FOR_branch_true
+	   || recog_memoized (insn) == CODE_FOR_branch_false)
+	  && cur_length == 2)
+	{
+	  int long_b = sh_forward_branch_p (insn, 0x7f);
+	  if (long_b)
+	    return 4;
+	}
+    }
+
   /* Instructions with unfilled delay slots take up an extra two bytes for
      the nop in the delay slot.  */
   if (((GET_CODE (insn) == INSN
@@ -8252,7 +9247,79 @@
 	   && GET_CODE (PATTERN (insn)) != ADDR_VEC))
       && GET_CODE (PATTERN (NEXT_INSN (PREV_INSN (insn)))) != SEQUENCE
       && get_attr_needs_delay_slot (insn) == NEEDS_DELAY_SLOT_YES)
+    {
+      int xnop = 0;
+      if (TARGET_DBHWBUG
+	  && mdep_reorg_phase < SH_AFTER_MDEP_REORG && INSN_ADDRESSES_SET_P ())
+	xnop = 2;
+
+      /*
+	(2) mov.l	.L,r4		[length = 10]
+	(2) jmp	@r4
+	(2) nop
+	(4) .L: .long	.L5
+      */
+      if (cur_length == 10)
+	return xnop;
+	
+      /*
+	mov.l	r13,@-r15	[length = 16]
+	mov.w	.L4460,r13
+	braf	r13
+	mov.l	@r15+,r13
+	.word .L24-.L4460
+      */
+      if (cur_length == 14)
+	return -4 + xnop;
+
+      return 2 + xnop;
+    }
+
+  /* Filled delay slot. */
+  if (TARGET_DBHWBUG  && INSN_ADDRESSES_SET_P ())
+    {
+      if (mdep_reorg_phase == SH_AFTER_MDEP_REORG)
+	{
+	  if (recog_memoized (insn) == CODE_FOR_jump_compact)
+	    {
+	      rtx prev;
+	
+	      if (cur_length == 10)
+		{
+		  /*
+		    (0) delay slot	        [length = 2]
+		    (2) mov.l	r13,@-r15	[length = 10]
+		    (2) mov.w	.L,r13
+		    (2) braf	r13
+		    (2) mov.l	@r15+,r13
+		    (2) .L: .word
+		  */
+		  if (GET_CODE ((prev = prev_nonnote_insn (insn))) != INSN
+		      || INSN_CODE (prev) != CODE_FOR_indirect_jump_scratch)
+		    return 0;
+	
+		  /*
+		    (2) mov.l	.L,r4	 [length = 10]
+		    (2) jmp	@r4
+		    (0) delay slot       [length = 2]
+		    (4) .L: .long	
+		  */
+		  return -2;
+		}
+	    }
+	  return 0;
+	}
+      else if (mdep_reorg_phase < SH_AFTER_MDEP_REORG)
+	if (((GET_CODE (insn) == INSN
+	      && GET_CODE (PATTERN (insn)) != USE
+	      && GET_CODE (PATTERN (insn)) != CLOBBER)
+	     || GET_CODE (insn) == CALL_INSN
+	     || (GET_CODE (insn) == JUMP_INSN
+		 && GET_CODE (PATTERN (insn)) != ADDR_DIFF_VEC
+		 && GET_CODE (PATTERN (insn)) != ADDR_VEC))
+	    && get_attr_needs_delay_slot (insn) == NEEDS_DELAY_SLOT_YES)
     return 2;
+    }
 
   /* SH2e has a bug that prevents the use of annulled branches, so if
      the delay slot is not filled, we'll have to put a NOP in it.  */
@@ -8316,6 +9383,7 @@
       while (c);
       return sum;
     }
+
   return 0;
 }
 
@@ -8458,6 +9526,7 @@
   return lab;
 }
 
+
 /* Return true if it's possible to redirect BRANCH1 to the destination
    of an unconditional jump BRANCH2.  We only want to do this if the
    resulting branch will have a short displacement.  */
@@ -8470,7 +9539,7 @@
       rtx insn;
       int distance;
 
-      for (distance = 0, insn = NEXT_INSN (branch1);
+      for (distance = 0, insn = PREV_INSN (branch1);
 	   insn && distance < 256;
 	   insn = PREV_INSN (insn))
 	{
@@ -8489,6 +9558,7 @@
 	    distance += get_attr_length (insn);
 	}
     }
+
   return 0;
 }
 
@@ -8579,23 +9649,32 @@
     }
   else if (REG_NOTE_KIND (link) == 0)
     {
-      enum attr_type dep_type, type;
+      enum attr_type type;
+      rtx dep_set;
 
       if (recog_memoized (insn) < 0
 	  || recog_memoized (dep_insn) < 0)
 	return cost;
 
-      dep_type = get_attr_type (dep_insn);
-      if (dep_type == TYPE_FLOAD || dep_type == TYPE_PCFLOAD)
-	cost--;
-      if ((dep_type == TYPE_LOAD_SI || dep_type == TYPE_PCLOAD_SI)
-	  && (type = get_attr_type (insn)) != TYPE_CALL
-	  && type != TYPE_SFUNC)
-	cost--;
+      dep_set = single_set (dep_insn);
 
+      /* The latency that we specify in the scheduling description refers
+	 to the actual output, not to an auto-increment register; for that,
+	 the latency is one.  */
+      if (dep_set && MEM_P (SET_SRC (dep_set)) && cost > 1)
+	{
+	  rtx set = single_set (insn);
+
+	  if (set
+	      && !reg_mentioned_p (SET_DEST (dep_set), SET_SRC (set))
+	      && (!MEM_P (SET_DEST (set))
+		  || !reg_mentioned_p (SET_DEST (dep_set),
+				       XEXP (SET_DEST (set), 0))))
+	    cost = 1;
+	}
       /* The only input for a call that is timing-critical is the
 	 function's address.  */
-      if (GET_CODE(insn) == CALL_INSN)
+      if (GET_CODE (insn) == CALL_INSN)
 	{
 	  rtx call = PATTERN (insn);
 
@@ -8607,12 +9686,16 @@
 		  /* sibcalli_thunk uses a symbol_ref in an unspec.  */
 	      && (GET_CODE (XEXP (XEXP (call, 0), 0)) == UNSPEC
 		  || ! reg_set_p (XEXP (XEXP (call, 0), 0), dep_insn)))
-	    cost = 0;
+	    cost -= TARGET_SH4_300 ? 3 : 6;
 	}
       /* Likewise, the most timing critical input for an sfuncs call
 	 is the function address.  However, sfuncs typically start
 	 using their arguments pretty quickly.
-	 Assume a four cycle delay before they are needed.  */
+	 Assume a four cycle delay for SH4 before they are needed.
+	 Cached ST40-300 calls are quicker, so assume only a one
+	 cycle delay there.
+	 ??? Maybe we should encode the delays till input registers
+	 are needed by sfuncs into the sfunc call insn.  */
       /* All sfunc calls are parallels with at least four components.
 	 Exploit this to avoid unnecessary calls to sfunc_uses_reg.  */
       else if (GET_CODE (PATTERN (insn)) == PARALLEL
@@ -8620,13 +9703,22 @@
 	       && (reg = sfunc_uses_reg (insn)))
 	{
 	  if (! reg_set_p (reg, dep_insn))
-	    cost -= 4;
+	    cost -= TARGET_SH4_300 ? 1 : 4;
 	}
+      if (TARGET_HARD_SH4 && !TARGET_SH4_300)
+	{
+	  enum attr_type dep_type = get_attr_type (dep_insn);
+
+	  if (dep_type == TYPE_FLOAD || dep_type == TYPE_PCFLOAD)
+	    cost--;
+	  else if ((dep_type == TYPE_LOAD_SI || dep_type == TYPE_PCLOAD_SI)
+		   && (type = get_attr_type (insn)) != TYPE_CALL
+		   && type != TYPE_SFUNC)
+	    cost--;
       /* When the preceding instruction loads the shift amount of
 	 the following SHAD/SHLD, the latency of the load is increased
 	 by 1 cycle.  */
-      else if (TARGET_SH4
-	       && get_attr_type (insn) == TYPE_DYN_SHIFT
+	  if (get_attr_type (insn) == TYPE_DYN_SHIFT
 	       && get_attr_any_int_load (dep_insn) == ANY_INT_LOAD_YES
 	       && reg_overlap_mentioned_p (SET_DEST (single_set (dep_insn)),
 					   XEXP (SET_SRC (single_set (insn)),
@@ -8648,22 +9740,42 @@
 	       && ! regno_use_in (REGNO (SET_DEST (single_set (dep_insn))),
 				  SET_SRC (use_pat)))
 	cost -= 1;
-
-      if (get_attr_any_fp_comp (dep_insn) == ANY_FP_COMP_YES
-	  && get_attr_late_fp_use (insn) == LATE_FP_USE_YES)
-	cost -= 1;
+	}
+      else if (TARGET_SH4_300)
+	{
+	  /* Stores need their input register two cycles later.  */
+	  if (dep_set && cost >= 1
+	      && ((type = get_attr_type (insn)) == TYPE_STORE
+		  || type == TYPE_PSTORE
+		  || type == TYPE_FSTORE || type == TYPE_MAC_MEM))
+	    {
+	      rtx set = single_set (insn);
+
+	      if (!reg_mentioned_p (SET_SRC (set), XEXP (SET_DEST (set), 0))
+		  && rtx_equal_p (SET_SRC (set), SET_DEST (dep_set)))
+		{
+		  cost -= 2;
+		  /* But don't reduce the cost below 1 if the address depends
+		     on a side effect of dep_insn.  */
+		  if (cost < 1
+		      && modified_in_p (XEXP (SET_DEST (set), 0), dep_insn))
+		    cost = 1;
+		}
+	    }
+	}
     }
   /* An anti-dependence penalty of two applies if the first insn is a double
      precision fadd / fsub / fmul.  */
-  else if (REG_NOTE_KIND (link) == REG_DEP_ANTI
+  else if (!TARGET_SH4_300
+	   && REG_NOTE_KIND (link) == REG_DEP_ANTI
 	   && recog_memoized (dep_insn) >= 0
-	   && get_attr_type (dep_insn) == TYPE_DFP_ARITH
+	   && (get_attr_type (dep_insn) == TYPE_DFP_ARITH
+	       || get_attr_type (dep_insn) == TYPE_DFP_MUL)
 	   /* A lot of alleged anti-flow dependences are fake,
 	      so check this one is real.  */
 	   && flow_dependent_p (dep_insn, insn))
     cost = 2;
 
-
   return cost;
 }
 
@@ -9246,7 +10358,8 @@
   emit_move_insn (adjust_address (tramp_mem, SImode, 12), fnaddr);
   if (TARGET_HARVARD)
     {
-      if (TARGET_USERMODE)
+      if (!TARGET_INLINE_IC_INVALIDATE
+	  || (!(TARGET_SH4A_ARCH || TARGET_SH4_300) && TARGET_USERMODE))
 	emit_library_call (function_symbol (NULL, "__ic_invalidate",
 					    FUNCTION_ORDINARY),
 			   0, VOIDmode, 1, tramp, SImode);
@@ -10010,10 +11123,9 @@
 {
   rtx sym;
 
-  /* If this is not an ordinary function, the name usually comes from a
-     string literal or an sprintf buffer.  Make sure we use the same
+  /* The name usually comes from a string literal or an sprintf buffer.
+     Make sure we use the same
      string consistently, so that cse will be able to unify address loads.  */
-  if (kind != FUNCTION_ORDINARY)
     name = IDENTIFIER_POINTER (get_identifier (name));
   sym = gen_rtx_SYMBOL_REF (Pmode, name);
   SYMBOL_REF_FLAGS (sym) = SYMBOL_FLAG_FUNCTION;
@@ -10022,6 +11134,10 @@
       {
       case FUNCTION_ORDINARY:
 	break;
+      case SFUNC_FREQUENT:
+	if (!optimize || optimize_size)
+	  break;
+	/* Fall through.  */
       case SFUNC_GOT:
 	{
 	  rtx reg = target ? target : gen_reg_rtx (Pmode);
@@ -10124,6 +11240,141 @@
   return 1;
 }
 
+void
+sh_expand_float_cbranch (rtx operands[4])
+{
+  static rtx (*branches[]) (rtx) = { gen_branch_true, gen_branch_false };
+
+  sh_expand_float_condop (operands, branches);
+}
+
+void
+sh_expand_float_scc (rtx operands[4])
+{
+  static rtx (*movts[]) (rtx) = { gen_movt, gen_movnegt };
+
+  operands[3] = NULL_RTX;
+  sh_expand_float_condop (operands, movts);
+}
+
+/* The first element of USER is for positive logic, the second one for
+   negative logic.  */
+static void
+sh_expand_float_condop (rtx operands[4], rtx (*user[2]) (rtx))
+{
+  enum machine_mode mode = GET_MODE (operands[1]);
+  enum rtx_code comparison = GET_CODE (operands[0]);
+  int swap_operands = 0;
+
+  if (TARGET_SH1_SOFTFP_MODE (mode))
+    {
+      switch (comparison)
+	{
+	case NE:
+	  comparison = EQ;
+	  user++;
+	  break;
+	case LT:
+	  swap_operands = 1;	/* Fall through.  */
+	case GT:
+	  comparison = UNLE;
+	  user++;
+	  break;
+	case UNGT:
+	  swap_operands = 1;	/* Fall through.  */
+	case UNLT:
+	  comparison = GE;
+	  user++;
+	  break;
+	case UNGE:
+	  swap_operands = 1;
+	  comparison = UNLE;
+	  break;
+	case LE:
+	  swap_operands = 1;
+	  comparison = GE;	/* Fall through.  */
+	case EQ:
+	case UNEQ:
+	case GE:
+	case UNLE:
+	case UNORDERED:
+	  break;
+	case LTGT:
+	  comparison = UNEQ;
+	  user++;
+	  break;
+	case ORDERED:
+	  comparison = UNORDERED;
+	  user++;
+	  break;
+	
+	default: gcc_unreachable ();
+	}
+    }
+  else /* SH2E .. SH4 Hardware floating point */
+    {
+      switch (comparison)
+	{
+	case NE:
+	  comparison = EQ;
+	  user++;
+	  break;
+	case LT:
+	  swap_operands = 1;	/* Fall through.  */
+	  comparison = GT;
+	case GT:
+	case EQ:
+	case LTGT:
+	case ORDERED:
+	  break;
+	case LE:
+	  if (flag_finite_math_only)
+	    {
+	      comparison = GT;
+	      user++;
+	      break;
+	    }
+	  swap_operands = 1;
+	  comparison = GE;	/* Fall through.  */
+	case GE:
+	  if (flag_finite_math_only)
+	    {
+	      swap_operands = 1;
+	      comparison = GT;
+	      user++;
+	      break;
+	    }
+	  break;
+	case UNGE:
+	  swap_operands = 1;	/* Fall through.  */
+	case UNLE:
+	  comparison = GT;
+	  user++;
+	  break;
+	case UNGT:
+	  swap_operands = 1;	/* Fall through.  */
+	case UNLT:
+	  comparison = GE;
+	  user++;
+	  break;
+	case UNEQ:
+	  comparison = LTGT;
+	  user++;
+	  break;
+	case UNORDERED:
+	  comparison = ORDERED;
+	  user++;
+	  break;
+
+	default: gcc_unreachable ();
+	}
+    }
+  sh_compare_op0 = operands[1+swap_operands];
+  sh_compare_op1 = operands[2-swap_operands];
+  from_compare (&operands[3], comparison);
+  emit_jump_insn ((*user) (operands[3]));
+}
+
 /* INSN is an sfunc; return the rtx that describes the address used.  */
 static rtx
 extract_sfunc_addr (rtx insn)
@@ -10793,6 +12044,20 @@
   return for_each_rtx (&PATTERN (insn), &sh_contains_memref_p_1, NULL);
 }
 
+/* Return nonzero iff INSN loads a banked register.  */
+int
+sh_loads_bankedreg_p (rtx insn)
+{
+  if (GET_CODE (PATTERN (insn)) == SET)
+    {
+      rtx op = SET_DEST (PATTERN(insn));
+      if (REG_P (op) && BANKED_REGISTER_P (REGNO (op)))
+	return 1;
+    }
+
+  return 0;
+}
+
 /* FNADDR is the MEM expression from a call expander.  Return an address
    to use in an SHmedia insn pattern.  */
 rtx
@@ -10941,6 +12206,398 @@
   return NO_REGS;
 }
 
+int
+sh_match_adjust (rtx x, int regno)
+{
+  /* On a WORDS_BIG_ENDIAN machine, point to the last register of a
+     multiple hard register group of scalar integer registers, so that
+     for example (reg:DI 0) and (reg:SI 1) will be considered the same
+     register.  */
+  if (WORDS_BIG_ENDIAN && GET_MODE_SIZE (GET_MODE (x)) > UNITS_PER_WORD
+      && regno < FIRST_PSEUDO_REGISTER)
+    regno += hard_regno_nregs[regno][GET_MODE (x)] - 1;
+  return regno;
+}
+
 enum sh_divide_strategy_e sh_div_strategy = SH_DIV_STRATEGY_DEFAULT;
 
+static int asm_size (char *s, int addr, int *seen_align)
+{
+  char *pt;
+
+  if (*s == ';' || *s == '\n' || *s == '\0')
+    return 0;
+
+  else if (strstr (s, ".long"))
+    return 2;
+
+  else if (strstr (s, ".short"))
+    return 1;
+
+  else if (insn_current_address != -1 && strstr (s, ".balign"))
+    {
+      long int align;
+      while (*s != '\t' && *s != ' ') s++;
+      align = strtol (s, NULL, 10);
+      if (errno == ERANGE || errno == EINVAL)
+	{
+	  warning (OPT_mdb_page_bug, "unsupported %s asm for w/a.", s);
+	  return 0;
+	}
+      if (seen_align) *seen_align = align ;
+      return (((addr + align - 1) & -align) - addr) / 2;
+    }
+
+  else if (insn_current_address != -1 && strstr (s, ".align"))
+    {
+      long int align;
+      while (*s != '\t' && *s != ' ') s++;
+      align = strtol (s, NULL, 10);
+      if (errno == ERANGE || errno == EINVAL)
+	{
+	  warning (OPT_mdb_page_bug, "unsupported %s asm for w/a.", s);
+	  return 0;
+	}
+
+      if (seen_align) *seen_align = align;
+      align = 1 << align;
+      return (((addr + align - 1) & -align) - addr) / 2;
+    }
+
+  else if (strstr (s, ".space") || strstr (s, ".skip"))
+    {
+      long int align;
+      errno = 0;
+      while (*s != '\t' && *s != ' ') s++;
+      align = strtol (s, NULL, 10);
+      if (errno == ERANGE || errno == EINVAL)
+	{
+	  warning (OPT_mdb_page_bug, "unsupported %s asm for w/a.", s);
+	  return 0;
+	}
+      return align / 2;
+    }
+
+  else if ((pt = strrchr (s, ':')) != NULL)
+    {
+      while (*pt == '\t' || *pt == ' ' || *pt == ':') pt++;
+      return asm_size (pt, addr, seen_align);
+    }
+
+  if (*s == '.' && mdep_reorg_phase == SH_AFTER_MDEP_REORG)
+    warning (OPT_mdb_page_bug, "unsupported %s asm for w/a.", s);
+
+  return 1;
+}
+
+int
+sh_asm_count (const char *template, int *seen_align)
+{
+  int count = 0;
+  char *s, *lt;
+  char delim[] = ";\n";
+  int in_sub = 0;
+  int addr = insn_current_address;
+
+  lt = alloca (strlen (template) + 1);
+  strcpy (lt, template);
+
+  s = strtok(lt, delim);
+
+  while (s != NULL)
+    {
+      while (*s == '\t' || *s == ' ') s++;
+
+      if (strstr (s, ".pushsection") || strstr (s, ".section"))
+	in_sub++;
+
+      if (! in_sub)
+	count += asm_size (s, addr + (count * 2), seen_align);
+
+      if (strstr (s, ".popsection") || strstr (s, ".previous"))
+	in_sub--;
+
+      s = strtok(NULL, delim);
+    }
+
+  return count;
+}
+
+static int align_next_insn;
+
+static void
+sh_hw_workaround (rtx insn)
+{
+  int uid_address = INSN_ADDRESSES (INSN_UID (insn));
+  int real_address = uid_address + fixup_addr;
+
+  if (GET_CODE (insn) == CODE_LABEL)
+    {
+      rtx barrier = prev_nonnote_insn (insn);
+      if (barrier && BARRIER_P (barrier))
+	{
+	  int log = label_to_alignment (insn);
+	  int align = 1 << log;
+
+	  int aligned_real_address;
+	  int last_unaligned_uid_address = INSN_ADDRESSES (INSN_UID (barrier));
+	
+	  real_address -= (uid_address - last_unaligned_uid_address);
+	  aligned_real_address = (real_address + align - 1) & -align;
+
+	  fixup_addr = aligned_real_address - uid_address;
+	  return;
+	}
+    }
+
+  if (INSN_P (insn))
+    {
+      rtx body = PATTERN (insn);
+
+      if (GET_CODE (body) == UNSPEC_VOLATILE && XINT (body, 1) == UNSPECV_ALIGN)
+	{
+	  int log = INTVAL (XVECEXP (body, 0, 0));
+	  int align = 1 << log;
+	  int aligned_real_address = (real_address + align - 1) & -align;
+	  int aligned_current_address = (uid_address + align - 1) & -align;
+
+	  fixup_addr = aligned_real_address - aligned_current_address;
+	  return;
+	}
+
+      if (align_next_insn)
+	{
+	  align_next_insn = 0;
+	  fixup_addr = -uid_address;
+	  fprintf (asm_out_file, ".align 5\t\t! for hw workaround \n");
+	}
+
+      if (GET_CODE (body) == ASM_INPUT || asm_noperands (body) >= 0)
+	{
+	  int align = 0;
+	  const char *template;
+
+	  if (GET_CODE (body) == ASM_INPUT)
+	    template = XSTR (body, 0);
+	  else
+	    template = decode_asm_operands (body, NULL, NULL, NULL, NULL);
+
+	  (void) sh_asm_count (template, &align);
+	  if (align)
+	    {
+	      align_next_insn = 1;
+	      return;
+	    }
+	}
+
+      gcc_assert (! (real_address % 2));
+
+      /* +2 because we check that the ds is not aligned on 32.  */
+      real_address += 2;
+
+      if (recog_memoized (insn) == CODE_FOR_cmpgtudi_t
+	  || recog_memoized (insn) == CODE_FOR_cmpgeudi_t
+	  || recog_memoized (insn) == CODE_FOR_cmpgtdi_t
+	  || recog_memoized (insn) == CODE_FOR_cmpgedi_t)
+	if (!((real_address + 2) % 32))
+	  {
+	    fprintf (asm_out_file,
+		     "\tnop\t\t! for hw workaround @%d\n", real_address);
+	    fixup_addr += 2;
+	    return;
+	  }
+
+      if (recog_memoized (insn) == CODE_FOR_tls_global_dynamic
+	  || recog_memoized (insn) == CODE_FOR_tls_local_dynamic)
+	{
+	  int aligned_real_address;
+	  int align = 4;
+
+	  if (!((real_address + 8) % 32) || !((real_address + 12) % 32))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address);
+	      fixup_addr += 2;
+	      real_address += get_attr_length (insn);
+	    }
+	  else
+	    real_address += get_attr_length (insn) - 2;
+
+	  aligned_real_address = (real_address + align - 1) & -align;
+	  fixup_addr += aligned_real_address - real_address;	
+	  return;
+	}
+
+      if (recog_memoized (insn) == CODE_FOR_tls_initial_exec)
+	{
+	  int aligned_real_address;
+	  int align = 4;
+
+	  if (!((real_address + 6) % 32))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address);
+	      fixup_addr += 2;
+	      real_address += get_attr_length (insn);
+	    }
+	  else
+	    real_address += get_attr_length (insn) - 2;
+
+	  aligned_real_address = (real_address + align - 1) & -align;
+	  fixup_addr += aligned_real_address - real_address;	
+	  return;
+	}
+
+      if (recog_memoized (insn) == CODE_FOR_jump_compact)
+	{
+	  int offset = branch_dest (insn) - INSN_ADDRESSES (INSN_UID (insn));
+	  int far;
+
+	  far = ! (offset >= -32764 && offset - get_attr_length (insn) <= 32766);
+
+	  /* length = 6
+	     mov.w	.L8588,r1
+	     braf	r1
+	     mov	#81,r4	         [length = 2]
+	     .L8588: .word .L12-.L8588
+
+	     length = 8
+	     mov.w	.L8588,r1	
+	     braf	r1
+	     nop
+	     .L8588: .word .L12-.L8588
+
+	     length = 10
+	     mov	#81,r4	         [length = 2]
+	     mov.l	r13,@-r15	
+	     mov.w	.L,r13
+	     braf	r13
+	     mov.l	@r15+,r13
+	     .L: .word
+	  */
+	  if (!far)
+	    {
+	      if (get_attr_length (insn) == 6 || get_attr_length (insn) == 8)
+		real_address += 2;
+	      if (get_attr_length (insn) == 10)
+		{
+		  if (dbr_sequence_length ())
+		    real_address += 6;
+		  else
+		    real_address += 4;
+		}
+	    }
+
+	  /* length = 8
+	     mov.l	.L8586,r4	
+	     jmp	@r4
+	     mov	#81,r4         [length = 2]
+	     .align	2
+	     .L8586:  .long	.L5
+
+	     length = 10
+	     mov.l	.L8589,r3
+	     jmp	@r3
+	     nop
+	     .align	2
+	     .L8589:  .long	.L2242
+	  */
+	  else if (far
+		   && (get_attr_length (insn) == 8 || get_attr_length (insn) == 10))
+	    {
+	      int aligned_real_address;
+	      int next_uid_address = uid_address + 10;
+	      int align = 4;
+
+	      if (!((real_address + 2) % 32))
+		{
+		  fprintf (asm_out_file,
+			   "\tnop\t\t! for hw workaround @%d\n", real_address);
+		  real_address += 10;
+		  aligned_real_address = (real_address + align - 1) & -align;
+		  fixup_addr = aligned_real_address - next_uid_address;
+		}
+	      else
+		{
+		  real_address += 8;
+		  aligned_real_address = (real_address + align - 1) & -align;
+		  fixup_addr = aligned_real_address - next_uid_address;
+		}
+	      return;
+	    }
+
+	  if (!(real_address % 32))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround 1 @%d\n", real_address);
+	      fixup_addr += 2;
+	      return;
+	    }
+	}
+
+      /* we have bt; (aligned)bra; nop; avoid nop ; (aligned)bt; bra; nop.  */
+      /* or bt; delayed instruction; (aligned)bra.  */
+      if ((recog_memoized (insn) == CODE_FOR_branch_true
+	   || recog_memoized (insn) == CODE_FOR_branch_false)
+	  && get_attr_length (insn) == 6)
+	{
+	  if (!((real_address + 2) % 32))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address-2);
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address);
+	      fixup_addr += 4;
+	    }
+	  else if (!(real_address % 32))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address);
+	      fixup_addr += 2;
+	    }
+	  else if (!((real_address + 4) % 32)
+		   && (dbr_sequence_length ()
+		       && ! INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0))
+		       && get_attr_length (XVECEXP (final_sequence, 0, 1))))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address);
+	      fixup_addr += 2;
+	    }
+	}
+
+      if ((GET_CODE (insn) == JUMP_INSN
+	   && GET_CODE (PATTERN (insn)) != ADDR_DIFF_VEC
+	   && GET_CODE (PATTERN (insn)) != ADDR_VEC))
+	{
+	  if (!(real_address % 32)
+	      && (dbr_sequence_length ()
+		  && ! INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0))
+		  && get_attr_length (XVECEXP (final_sequence, 0, 1))))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address);
+	      fixup_addr += 2;
+	    }
+	  else if (GET_CODE (insn) == CALL_INSN
+		   && !(real_address % 32))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address);
+	      fixup_addr += 2;
+	    }
+	}
+
+	if ((GET_CODE (insn) == CALL_INSN || INSN_P (insn))
+	    && !(real_address % 32)
+	    && get_attr_needs_delay_slot (insn) == NEEDS_DELAY_SLOT_YES)
+	{
+	  fprintf (asm_out_file,
+		   "\tnop\t\t! for hw workaround @%d\n", real_address);
+	  fixup_addr += 2;
+	}
+    }
+}
+
 #include "gt-sh.h"
+
Index: gcc/config/sh/sh.h
===================================================================
--- gcc/config/sh/sh.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/sh.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -3,6 +3,7 @@
    2003, 2004, 2005, 2006, 2007 Free Software Foundation, Inc.
    Contributed by Steve Chamberlain (sac@cygnus.com).
    Improved by Jim Wilson (wilson@cygnus.com).
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -33,6 +34,8 @@
 
 #define TARGET_CPU_CPP_BUILTINS() \
 do { \
+  if (TARGET_DBHWBUG) \
+      builtin_define_with_value ("DB_ST40300_BUG_WORKAROUND", "32", 0); \
   builtin_define ("__sh__"); \
   builtin_assert ("cpu=sh"); \
   builtin_assert ("machine=sh"); \
@@ -94,7 +97,7 @@
 } while (0)
 
 /* We can not debug without a frame pointer.  */
-/* #define CAN_DEBUG_WITHOUT_FP */
+#define CAN_DEBUG_WITHOUT_FP
 
 #define CONDITIONAL_REGISTER_USAGE do					\
 {									\
@@ -103,6 +106,10 @@
     if (! VALID_REGISTER_P (regno))					\
       fixed_regs[regno] = call_used_regs[regno] = 1;			\
   /* R8 and R9 are call-clobbered on SH5, but not on earlier SH ABIs.  */ \
+  if (TARGET_SH4A_FP || TARGET_SH4_300)					\
+    {                                                                   \
+      global_regs[FPSCR_REG] = 1;                                       \
+    }									\
   if (TARGET_SH5)							\
     {									\
       call_used_regs[FIRST_GENERAL_REG + 8]				\
@@ -116,6 +123,10 @@
       CLEAR_HARD_REG_SET (reg_class_contents[FP0_REGS]);		\
       regno_reg_class[FIRST_FP_REG] = FP_REGS;				\
     }									\
+  if (TARGET_R0R3_TO_REG_MUL < 2)					\
+    regno_reg_class[R1_REG] = regno_reg_class[R2_REG]			\
+      = regno_reg_class[R3_REG] = GENERAL_REGS;				\
+    /* The peephole2s needs reg_class_contents[R0R3_REGS].  */		\
   if (flag_pic)								\
     {									\
       fixed_regs[PIC_OFFSET_TABLE_REGNUM] = 1;				\
@@ -171,6 +182,11 @@
 #define TARGET_FPU_DOUBLE \
   ((target_flags & MASK_SH4) != 0 || TARGET_SH2A_DOUBLE)
 
+#define TARGET_SH1_SOFTFP (TARGET_SH1 && !TARGET_FPU_DOUBLE)
+
+#define TARGET_SH1_SOFTFP_MODE(MODE) \
+  (TARGET_SH1_SOFTFP && (!TARGET_SH2E || (MODE) == DFmode))
+
 /* Nonzero if an FPU is available.  */
 #define TARGET_FPU_ANY (TARGET_SH2E || TARGET_FPU_DOUBLE)
 
@@ -211,7 +227,7 @@
    && ! (TARGET_HITACHI || sh_attr_renesas_p (FUN_DECL)))
 
 #ifndef TARGET_CPU_DEFAULT
-#define TARGET_CPU_DEFAULT SELECT_SH1
+#define TARGET_CPU_DEFAULT SELECT_SH4
 #define SUPPORT_SH1 1
 #define SUPPORT_SH2E 1
 #define SUPPORT_SH4 1
@@ -268,11 +284,48 @@
 #define SELECT_SH5_COMPACT       (MASK_SH5 | MASK_SH4 | SELECT_SH3E)
 #define SELECT_SH5_COMPACT_NOFPU (MASK_SH5 | SELECT_SH3)
 
+/* Check if we have support for optimized software floating point using
+   dynamic shifts - then some function calls clobber fewer registers.  */
+#ifdef SUPPORT_SH3
+#define SUPPORT_SH3_OSFP 1
+#else
+#define SUPPORT_SH3_OSFP 0
+#endif
+
+#ifdef SUPPORT_SH3E
+#define SUPPORT_SH3E_OSFP 1
+#else
+#define SUPPORT_SH3E_OSFP 0
+#endif
+
+#if defined(SUPPORT_SH4_NOFPU) || defined(SUPPORT_SH3_OSFP)
+#define SUPPORT_SH4_NOFPU_OSFP 1
+#else
+#define SUPPORT_SH4_NOFPU_OSFP 0
+#endif
+
+#if defined(SUPPORT_SH4_SINGLE_ONLY) || defined (SUPPORT_SH3E_OSFP)
+#define SUPPORT_SH4_SINGLE_ONLY_OSFP 1
+#else
+#define SUPPORT_SH4_SINGLE_ONLY_OSFP 0
+#endif
+
+#ifdef notyet
+#define TARGET_OSFP (0 \
+ || (TARGET_SH3 && !TARGET_SH2E && SUPPORT_SH3_OSFP) \
+ || (TARGET_SH3E && SUPPORT_SH3E_OSFP) \
+ || (TARGET_HARD_SH4 && !TARGET_SH2E && SUPPORT_SH4_NOFPU_OSFP) \
+ || (TARGET_HARD_SH4 && TARGET_SH2E && SUPPORT_SH4_SINGLE_ONLY_OSFP))
+#else
+#define TARGET_OSFP (0)
+#endif
+
 #if SUPPORT_SH1
 #define SUPPORT_SH2 1
 #endif
 #if SUPPORT_SH2
 #define SUPPORT_SH3 1
+#define SUPPORT_SH2A_NOFPU 1
 #endif
 #if SUPPORT_SH3
 #define SUPPORT_SH4_NOFPU 1
@@ -280,16 +333,17 @@
 #if SUPPORT_SH4_NOFPU
 #define SUPPORT_SH4A_NOFPU 1
 #define SUPPORT_SH4AL 1
-#define SUPPORT_SH2A_NOFPU 1
 #endif
 
 #if SUPPORT_SH2E
 #define SUPPORT_SH3E 1
+#define SUPPORT_SH2A_SINGLE_ONLY 1
 #endif
 #if SUPPORT_SH3E
 #define SUPPORT_SH4_SINGLE_ONLY 1
+#endif
+#if SUPPORT_SH4_SINGLE_ONLY
 #define SUPPORT_SH4A_SINGLE_ONLY 1
-#define SUPPORT_SH2A_SINGLE_ONLY 1
 #endif
 
 #if SUPPORT_SH4
@@ -359,20 +413,13 @@
   { "subtarget_link_emul_suffix", SUBTARGET_LINK_EMUL_SUFFIX },	\
   { "subtarget_link_spec", SUBTARGET_LINK_SPEC },		\
   { "subtarget_asm_endian_spec", SUBTARGET_ASM_ENDIAN_SPEC },	\
-  { "subtarget_asm_relax_spec", SUBTARGET_ASM_RELAX_SPEC },	\
   { "subtarget_asm_isa_spec", SUBTARGET_ASM_ISA_SPEC },		\
   { "subtarget_asm_spec", SUBTARGET_ASM_SPEC },			\
   SUBTARGET_EXTRA_SPECS
 
-#if TARGET_CPU_DEFAULT & MASK_HARD_SH4
-#define SUBTARGET_ASM_RELAX_SPEC "%{!m1:%{!m2:%{!m3*:%{!m5*:-isa=sh4-up}}}}"
-#else
-#define SUBTARGET_ASM_RELAX_SPEC "%{m4*:-isa=sh4-up}"
-#endif
-
 #define SH_ASM_SPEC \
- "%(subtarget_asm_endian_spec) %{mrelax:-relax %(subtarget_asm_relax_spec)}\
-%(subtarget_asm_isa_spec) %(subtarget_asm_spec)\
+"%(subtarget_asm_endian_spec) %{mrelax:-relax} \
+%(subtarget_asm_isa_spec) %(subtarget_asm_spec) \
 %{m2a:--isa=sh2a} \
 %{m2a-single:--isa=sh2a} \
 %{m2a-single-only:--isa=sh2a} \
@@ -403,7 +450,7 @@
  "%{m4-nofpu:-isa=sh4-nofpu} " ASM_ISA_DEFAULT_SPEC
 #endif
 #else /* ! STRICT_NOFPU */
-#define SUBTARGET_ASM_ISA_SPEC ASM_ISA_DEFAULT_SPEC
+#define SUBTARGET_ASM_ISA_SPEC "%{m4:-isa=sh4} %{m4-300:-isa=st40-300} %{m4-nofpu:-isa=sh4-nofpu} %{m4-300-nofpu:-isa=st40-300-nofpu}" ASM_ISA_DEFAULT_SPEC
 #endif
 
 #ifndef SUBTARGET_ASM_SPEC
@@ -431,8 +478,14 @@
 #define ASM_ISA_DEFAULT_SPEC \
 " %{!m1:%{!m2*:%{!m3*:%{!m4*:%{!m5*:" ASM_ISA_SPEC_DEFAULT "}}}}}"
 #else /* !MASK_SH5 */
-#define LINK_DEFAULT_CPU_EMUL ""
+#if TARGET_CPU_DEFAULT & MASK_SH4
+#define ASM_ISA_SPEC_DEFAULT "--isa=sh4-up"
+#define ASM_ISA_DEFAULT_SPEC \
+" %{!m1:%{!m2*:%{!m3*:%{!m4*:%{!m5*:" ASM_ISA_SPEC_DEFAULT "}}}}}"
+#else /* !MASK_SH4 */
 #define ASM_ISA_DEFAULT_SPEC ""
+#endif
+#define LINK_DEFAULT_CPU_EMUL ""
 #endif /* MASK_SH5 */
 
 #define SUBTARGET_LINK_EMUL_SUFFIX ""
@@ -448,6 +501,8 @@
 %{m5-64media*:64}\
 %{!m1:%{!m2:%{!m3*:%{!m4*:%{!m5*:%(link_default_cpu_emul)}}}}}\
 %(subtarget_link_emul_suffix) \
+%{shared:-shared} \
+%{mdb-page-bug:--db-page-bug} \
 %{mrelax:-relax} %(subtarget_link_spec)"
 
 #ifndef SH_DIV_STR_FOR_SIZE
@@ -457,17 +512,19 @@
 #define DRIVER_SELF_SPECS "%{m2a:%{ml:%eSH2a does not support little-endian}}"
 #define OPTIMIZATION_OPTIONS(LEVEL,SIZE)				\
 do {									\
+  TARGET_CBRANCHDI4 = 1;						\
+  TARGET_EXPAND_CBRANCHDI4 = 1;						\
+                                                                        \
+  /* temporary until the exceptions are fixed.  */ 		        \
+  flag_omit_frame_pointer = -1;						\
+                                                                        \
   if (LEVEL)								\
     {									\
-      flag_omit_frame_pointer = -1;					\
       if (! SIZE)							\
 	sh_div_str = "inv:minlat";					\
     }									\
   if (SIZE)								\
-    {									\
-      target_flags |= MASK_SMALLCODE;					\
       sh_div_str = SH_DIV_STR_FOR_SIZE ;				\
-    }									\
   /* We can't meaningfully test TARGET_SHMEDIA here, because -m options	\
      haven't been parsed yet, hence we'd read only the default.	\
      sh_target_reg_class will return NO_REGS if this is not SHMEDIA, so	\
@@ -487,6 +544,8 @@
      the user explicitly requested this to be on or off.  */		\
   if (flag_schedule_insns > 0)						\
     flag_schedule_insns = 2;						\
+  if (flag_trapping_math > 0) /* Likewise for flag_trapping_math.  */	\
+    flag_trapping_math = 2;						\
 } while (0)
 
 #define ASSEMBLER_DIALECT assembler_dialect
@@ -527,9 +586,14 @@
   if (flag_finite_math_only == 2)					\
     flag_finite_math_only						\
       = !flag_signaling_nans && TARGET_SH2E && ! TARGET_IEEE;		\
+  if (flag_trapping_math == 2)						\
+    flag_trapping_math = (!TARGET_SH2E && TARGET_OSFP);			\
   if (TARGET_SH2E && !flag_finite_math_only)				\
     target_flags |= MASK_IEEE;						\
   sh_cpu = CPU_SH1;							\
+  if (TARGET_SH1)							\
+    assembler_dialect = 1;						\
+  else                                                                  \
   assembler_dialect = 0;						\
   if (TARGET_SH2)							\
     sh_cpu = CPU_SH2;							\
@@ -607,6 +671,7 @@
 	      else							\
 		sh_div_strategy = SH_DIV_INV;				\
 	    }								\
+	  TARGET_CBRANCHDI4 = 0;					\
 	}								\
       /* -fprofile-arcs needs a working libgcov .  In unified tree	\
 	 configurations with newlib, this requires to configure with	\
@@ -621,6 +686,8 @@
        targetm.asm_out.aligned_op.di = NULL;				\
        targetm.asm_out.unaligned_op.di = NULL;				\
     }									\
+  if (!TARGET_SH1)							\
+    TARGET_PRETEND_CMOVE = 0;						\
   if (TARGET_SH1)							\
     {									\
       if (! strcmp (sh_div_str, "call-div1"))				\
@@ -650,13 +717,9 @@
 	  sh_div_strategy = SH_DIV_CALL_FP;				\
         /* SH1 .. SH3 cores often go into small-footprint systems, so	\
 	   default to the smallest implementation available.  */	\
-	else if (TARGET_SH2)	/* ??? EXPERIMENTAL */			\
-	  sh_div_strategy = SH_DIV_CALL_TABLE;				\
 	else								\
 	  sh_div_strategy = SH_DIV_CALL_DIV1;				\
     }									\
-  if (!TARGET_SH1)							\
-    TARGET_PRETEND_CMOVE = 0;						\
   if (sh_divsi3_libfunc[0])						\
     ; /* User supplied - leave it alone.  */				\
   else if (TARGET_DIVIDE_CALL_FP)					\
@@ -667,9 +730,23 @@
     sh_divsi3_libfunc = "__sdivsi3_1";					\
   else									\
     sh_divsi3_libfunc = "__sdivsi3";					\
+  if (sh_branch_cost == -1)						\
+    sh_branch_cost							\
+      = TARGET_SH5 ? 1 : ! TARGET_SH2 || TARGET_HARD_SH4 ? 2 : 1;	\
+  if (TARGET_R0R3_TO_REG_MUL == -1)					\
+    TARGET_R0R3_TO_REG_MUL = /* TARGET_SH4_300 ? 2 : */ 0;		\
   if (TARGET_FMOVD)							\
     reg_class_from_letter['e' - 'a'] = NO_REGS;				\
 									\
+  if (flag_unwind_tables || flag_exceptions)			        \
+      flag_omit_frame_pointer = 0;					\
+                                                                        \
+  if (flag_omit_frame_pointer < 0)					\
+   {									\
+      flag_omit_frame_pointer						\
+        = (PREFERRED_DEBUGGING_TYPE == DWARF2_DEBUG);			\
+   }									\
+                                                                        \
   for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)		\
     if (! VALID_REGISTER_P (regno))					\
       sh_register_names[regno][0] = '\0';				\
@@ -678,17 +755,6 @@
     if (! VALID_REGISTER_P (ADDREGNAMES_REGNO (regno)))			\
       sh_additional_register_names[regno][0] = '\0';			\
 									\
-  if (flag_omit_frame_pointer < 0)					\
-   {									\
-     /* The debugging information is sufficient,			\
-        but gdb doesn't implement this yet */				\
-     if (0)								\
-      flag_omit_frame_pointer						\
-        = (PREFERRED_DEBUGGING_TYPE == DWARF2_DEBUG);			\
-     else								\
-      flag_omit_frame_pointer = 0;					\
-   }									\
-									\
   if ((flag_pic && ! TARGET_PREFERGOT)					\
       || (TARGET_SHMEDIA && !TARGET_PT_FIXED))				\
     flag_no_function_cse = 1;						\
@@ -716,6 +782,9 @@
 	}								\
     }									\
 									\
+  if (TARGET_DBHWBUG)                                                   \
+       align_functions = 32;						\
+									\
   if (align_loops == 0)							\
     align_loops =  1 << (TARGET_SH5 ? 3 : 2);				\
   if (align_jumps == 0)							\
@@ -729,21 +798,11 @@
      SH2 .. SH5 : align to cache line start.  */			\
   if (align_functions == 0)						\
     align_functions							\
-      = TARGET_SMALLCODE ? FUNCTION_BOUNDARY/8 : (1 << CACHE_LOG);	\
-  /* The linker relaxation code breaks when a function contains		\
-     alignments that are larger than that at the start of a		\
-     compilation unit.  */						\
-  if (TARGET_RELAX)							\
-    {									\
-      int min_align							\
-	= align_loops > align_jumps ? align_loops : align_jumps;	\
-									\
-      /* Also take possible .long constants / mova tables int account.	*/\
-      if (min_align < 4)						\
-	min_align = 4;							\
-      if (align_functions < min_align)					\
-	align_functions = min_align;					\
-    }									\
+      = optimize_size ? FUNCTION_BOUNDARY/8 : (1 << CACHE_LOG);	        \
+  if (!TARGET_SH2)							\
+      TARGET_EXPAND_CBRANCHDI4 = 1;					\
+  if (TARGET_DBHWBUG)                                                   \
+    flag_no_peephole = 1;                                               \
 } while (0)
 
 /* Target machine storage layout.  */
@@ -843,7 +902,7 @@
   ((GET_MODE_CLASS (TYPE_MODE (TYPE)) == MODE_COMPLEX_INT \
     || GET_MODE_CLASS (TYPE_MODE (TYPE)) == MODE_COMPLEX_FLOAT) \
    ? (unsigned) MIN (BIGGEST_ALIGNMENT, GET_MODE_BITSIZE (TYPE_MODE (TYPE))) \
-   : (unsigned) ALIGN)
+   : (unsigned) DATA_ALIGNMENT(TYPE, ALIGN))
 
 /* Make arrays of chars word-aligned for the same reasons.  */
 #define DATA_ALIGNMENT(TYPE, ALIGN)		\
@@ -856,6 +915,18 @@
    multiple of this.  */
 #define STRUCTURE_SIZE_BOUNDARY (TARGET_PADSTRUCT ? 32 : 8)
 
+/* Define this macro as an expression for the alignment of a structure
+   (given by STRUCT as a tree node) if the alignment computed in the
+   usual way is COMPUTED and the alignment explicitly specified was
+   SPECIFIED.
+*/
+#define ROUND_TYPE_ALIGN(STRUCT, COMPUTED, SPECIFIED)	\
+    ((TARGET_FMOVD &&						       \
+      TREE_CODE (STRUCT) == RECORD_TYPE && TYPE_FIELDS (STRUCT) != 0 && \
+      TREE_INT_CST_LOW (TYPE_SIZE (STRUCT)) > 64)	\
+     ? MAX (MAX ((COMPUTED), (SPECIFIED)), 64)		\
+     : MAX ((COMPUTED), (SPECIFIED)))
+
 /* Set this nonzero if move instructions will actually fail to work
    when given unaligned data.  */
 #define STRICT_ALIGNMENT 1
@@ -864,8 +935,11 @@
 #define LABEL_ALIGN_AFTER_BARRIER(LABEL_AFTER_BARRIER) \
   barrier_align (LABEL_AFTER_BARRIER)
 
+#define JUMP_ALIGN(LABEL) \
+   sh_jump_align (LABEL)
+
 #define LOOP_ALIGN(A_LABEL) \
-  ((! optimize || TARGET_HARD_SH4 || TARGET_SMALLCODE) \
+  ((! optimize || TARGET_HARD_SH4 || optimize_size) \
    ? 0 : sh_loop_align (A_LABEL))
 
 #define LABEL_ALIGN(A_LABEL) \
@@ -882,12 +956,20 @@
 #define ADDR_VEC_ALIGN(ADDR_VEC) 2
 
 /* The base two logarithm of the known minimum alignment of an insn length.  */
+/* After a addr_diff_vec:HI the log align is 1.  Update it so the  next
+   insn_current_address can correctly be computed in final.  */
 #define INSN_LENGTH_ALIGNMENT(A_INSN)					\
   (GET_CODE (A_INSN) == INSN						\
    ? 1 << TARGET_SHMEDIA						\
+   : GET_CODE (A_INSN) == JUMP_INSN                                     \
+     && GET_CODE (PATTERN (A_INSN)) == ADDR_DIFF_VEC                    \
+     && GET_MODE (PATTERN (A_INSN)) == HImode                           \
+   ? 0                                                                  \
+   : GET_CODE (A_INSN) == BARRIER                                       \
+   ? 0                                                                  \
    : GET_CODE (A_INSN) == JUMP_INSN || GET_CODE (A_INSN) == CALL_INSN	\
    ? 1 << TARGET_SHMEDIA						\
-   : CACHE_LOG)
+   : 1)
 
 /* Standard register usage.  */
 
@@ -1016,6 +1098,16 @@
 #define FIRST_TARGET_REG TR0_REG
 #define LAST_TARGET_REG  (FIRST_TARGET_REG + (TARGET_SHMEDIA ? 7 : -1))
 
+/* Registers that can be accessed through bank0 or bank1 depending on sr.md.  */
+
+#define FIRST_BANKED_REG R0_REG
+#define LAST_BANKED_REG R7_REG
+
+#define BANKED_REGISTER_P(REGNO)                       \
+  IN_RANGE ((REGNO),                                   \
+	    (unsigned HOST_WIDE_INT) FIRST_BANKED_REG, \
+	    (unsigned HOST_WIDE_INT) LAST_BANKED_REG)
+
 #define GENERAL_REGISTER_P(REGNO) \
   IN_RANGE ((REGNO), \
 	    (unsigned HOST_WIDE_INT) FIRST_GENERAL_REG, \
@@ -1382,6 +1474,7 @@
 {
   NO_REGS,
   R0_REGS,
+  R0R3_REGS,
   PR_REGS,
   T_REGS,
   MAC_REGS,
@@ -1407,6 +1500,7 @@
 {			\
   "NO_REGS",		\
   "R0_REGS",		\
+  "R0R3_REGS",		\
   "PR_REGS",		\
   "T_REGS",		\
   "MAC_REGS",		\
@@ -1434,6 +1528,8 @@
   { 0x00000000, 0x00000000, 0x00000000, 0x00000000, 0x00000000 },	\
 /* R0_REGS:  */								\
   { 0x00000001, 0x00000000, 0x00000000, 0x00000000, 0x00000000 },	\
+/* R0R3_REGS:  */							\
+  { 0x0000000f, 0x00000000, 0x00000000, 0x00000000, 0x00000000 },	\
 /* PR_REGS:  */								\
   { 0x00000000, 0x00000000, 0x00000000, 0x00000000, 0x00040000 },	\
 /* T_REGS:  */								\
@@ -1524,9 +1620,11 @@
    description.  */
 extern enum reg_class reg_class_from_letter[];
 
-/* We might use 'Rxx' constraints in the future for exotic reg classes.*/
+/* Use 'Rxx' constraints for exotic reg classes.  */
 #define REG_CLASS_FROM_CONSTRAINT(C, STR) \
-  (ISLOWER (C) ? reg_class_from_letter[(C)-'a'] : NO_REGS )
+  (ISLOWER (C) ? reg_class_from_letter[(C)-'a'] \
+   : (C) == 'R' && (STR)[1] == '0' && (STR)[2] == '3' ? R0R3_REGS \
+   : NO_REGS )
 
 /* Overview of uppercase letter constraints:
    A: Addresses (constraint len == 3)
@@ -2205,7 +2303,7 @@
 
 /* Alignment required for a trampoline in bits .  */
 #define TRAMPOLINE_ALIGNMENT \
-  ((CACHE_LOG < 3 || (TARGET_SMALLCODE && ! TARGET_HARVARD)) ? 32 \
+  ((CACHE_LOG < 3 || (optimize_size && ! TARGET_HARVARD)) ? 32 \
    : TARGET_SHMEDIA ? 256 : 64)
 
 /* Emit RTL insns to initialize the variable parts of a trampoline.
@@ -2253,12 +2351,13 @@
                                            ? 0 : TARGET_SH1)
 
 #define MOVE_BY_PIECES_P(SIZE, ALIGN) \
-  (move_by_pieces_ninsns (SIZE, ALIGN, MOVE_MAX_PIECES + 1) \
-   < (TARGET_SMALLCODE ? 2 : ((ALIGN >= 32) ? 16 : 2)))
+  ((TARGET_FMOVD) ? ((SIZE)*8 <= 64 || ALIGN != 64) \
+   : (move_by_pieces_ninsns ((SIZE), ALIGN, MOVE_MAX_PIECES + 1) \
+      < (optimize_size ? 2 : ((ALIGN >= 32) ? 16 : 2))))
 
 #define STORE_BY_PIECES_P(SIZE, ALIGN) \
   (move_by_pieces_ninsns (SIZE, ALIGN, STORE_MAX_PIECES + 1) \
-   < (TARGET_SMALLCODE ? 2 : ((ALIGN >= 32) ? 16 : 2)))
+   < (optimize_size ? 2 : ((ALIGN >= 32) ? 16 : 2)))
 
 /* Macros to check register numbers against specific register classes.  */
 
@@ -2287,6 +2386,7 @@
 #define CONSTANT_ADDRESS_P(X)	(GET_CODE (X) == LABEL_REF)
 
 /* Nonzero if the constant value X is a legitimate general operand.  */
+/* can_store_by_pieces constructs VOIDmode CONST_DOUBLEs.  */
 
 #define LEGITIMATE_CONSTANT_P(X) \
   (TARGET_SHMEDIA							\
@@ -2297,7 +2397,7 @@
       || TARGET_SHMEDIA64)						\
    : (GET_CODE (X) != CONST_DOUBLE					\
       || GET_MODE (X) == DFmode || GET_MODE (X) == SFmode		\
-      || (TARGET_SH2E && (fp_zero_operand (X) || fp_one_operand (X)))))
+      || GET_MODE (X) == DImode || GET_MODE (X) == VOIDmode))
 
 /* The macros REG_OK_FOR..._P assume that the arg is a REG rtx
    and check its validity for a certain class.
@@ -2800,7 +2900,8 @@
 #define CASE_VECTOR_MODE ((! optimize || TARGET_BIGTABLE) ? SImode : HImode)
 
 #define CASE_VECTOR_SHORTEN_MODE(MIN_OFFSET, MAX_OFFSET, BODY) \
-((MIN_OFFSET) >= 0 && (MAX_OFFSET) <= 127 \
+(TARGET_DBHWBUG ? SImode                                    \
+ : (MIN_OFFSET) >= 0 && (MAX_OFFSET) <= 127		    \
  ? (ADDR_DIFF_VEC_FLAGS (BODY).offset_unsigned = 0, QImode) \
  : (MIN_OFFSET) >= 0 && (MAX_OFFSET) <= 255 \
  ? (ADDR_DIFF_VEC_FLAGS (BODY).offset_unsigned = 1, QImode) \
@@ -2850,7 +2951,7 @@
 
 /* Max number of bytes we want move_by_pieces to be able to copy
    efficiently.  */
-#define MOVE_MAX_PIECES (TARGET_SH4 || TARGET_SHMEDIA ? 8 : 4)
+#define MOVE_MAX_PIECES (TARGET_SH1 || TARGET_SH5 ? 8 : 4)
 
 /* Define if operations between registers always perform the operation
    on the full register even if a narrower mode is specified.  */
@@ -3111,16 +3212,18 @@
    ? (TARGET_SH5 ? 18 : 17) \
    : (REGNO) == PR_MEDIA_REG \
    ? (TARGET_SH5 ? 18 : (unsigned) -1) \
-   : (REGNO) == T_REG \
-   ? (TARGET_SH5 ? 242 : 18) \
    : (REGNO) == GBR_REG \
-   ? (TARGET_SH5 ? 238 : 19) \
+   ? (TARGET_SH5 ? 238 : 18) \
    : (REGNO) == MACH_REG \
    ? (TARGET_SH5 ? 239 : 20) \
    : (REGNO) == MACL_REG \
    ? (TARGET_SH5 ? 240 : 21) \
+   : (REGNO) == T_REG \
+   ? (TARGET_SH5 ? 242 : 22) \
    : (REGNO) == FPUL_REG \
    ? (TARGET_SH5 ? 244 : 23) \
+   : (REGNO) == FPSCR_REG \
+   ? (TARGET_SH5 ? 243 : 24) \
    : (unsigned) -1)
 
 /* This is how to output a reference to a symbol_ref.  On SH5,
@@ -3145,6 +3248,8 @@
 /* Globalizing directive for a label.  */
 #define GLOBAL_ASM_OP "\t.global\t"
 
+#define TARGET_ASM_COUNT(TEMP, ALIGNP) sh_asm_count (TEMP, ALIGNP)
+
 /* #define ASM_OUTPUT_CASE_END(STREAM,NUM,TABLE)	    */
 
 /* Output a relative address table.  */
@@ -3337,8 +3442,9 @@
    extra two bytes for the nop in the delay slot.
    sh-dsp parallel processing insns are four bytes long.  */
 
-#define ADJUST_INSN_LENGTH(X, LENGTH)				\
-  (LENGTH) += sh_insn_length_adjustment (X);
+#define ADJUST_INSN_LENGTH(X, LENGTH, SPECIAL)				\
+  (LENGTH) += sh_insn_length_adjustment (X, LENGTH, SPECIAL);
+
 
 /* Define this macro if it is advisable to hold scalars in registers
    in a wider mode than that declared by the program.  In such cases,
@@ -3367,7 +3473,7 @@
    compatibility problems.  */
 
 #define SH_DYNAMIC_SHIFT_COST \
-  (TARGET_HARD_SH4 ? 1 : TARGET_SH3 ? (TARGET_SMALLCODE ? 1 : 2) : 20)
+  (TARGET_HARD_SH4 ? 1 : TARGET_SH3 ? (optimize_size ? 1 : 2) : 20)
 
 
 #define NUM_MODES_FOR_MODE_SWITCHING { FP_MODE_NONE }
@@ -3405,11 +3511,16 @@
 #define MODE_PRIORITY_TO_MODE(ENTITY, N) \
   ((TARGET_FPU_SINGLE != 0) ^ (N) ? FP_MODE_SINGLE : FP_MODE_DOUBLE)
 
-#define EMIT_MODE_SET(ENTITY, MODE, HARD_REGS_LIVE) \
-  fpscr_set_from_mem ((MODE), (HARD_REGS_LIVE))
+#define EMIT_MODE_SET(ENTITY, MODE, FLIP, HARD_REGS_LIVE) \
+  ((TARGET_SH4A_FP || TARGET_SH4_300)                     \
+   && (FLIP) ? emit_fpu_flip ()                           \
+   : fpscr_set_from_mem ((MODE), (HARD_REGS_LIVE)))
 
+/* Too conservative, if distances are not computed get_attr_length is too
+   much conservative. better let it go and split_branches afterwards.
 #define MD_CAN_REDIRECT_BRANCH(INSN, SEQ) \
   sh_can_redirect_branch ((INSN), (SEQ))
+ */
 
 #define DWARF_FRAME_RETURN_COLUMN \
   (TARGET_SH5 ? DWARF_FRAME_REGNUM (PR_MEDIA_REG) : DWARF_FRAME_REGNUM (PR_REG))
@@ -3459,4 +3570,7 @@
 /* FIXME: middle-end support for highpart optimizations is missing.  */
 #define high_life_started reload_in_progress
 
+#define TARGET_USES_LEB128 \
+  (! TARGET_RELAX || (!flag_unwind_tables && !flag_exceptions))
+
 #endif /* ! GCC_SH_H */
Index: gcc/config/sh/trap-handler.c
===================================================================
--- gcc/config/sh/trap-handler.c	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/trap-handler.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,10 @@
+extern void exit(int) __attribute__ ((noreturn));
+
+void
+_superh_trap_handler (unsigned int trap_reason)
+{
+  exit(*(int*)0xff000024);  /* return EXPEVT */
+
+  /* in case exit returns ... */
+  while(1);
+}
Index: gcc/config/sh/t-superh
===================================================================
--- gcc/config/sh/t-superh	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/t-superh	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,6 +1,7 @@
 EXTRA_MULTILIB_PARTS= crt1.o crti.o crtn.o \
 	crtbegin.o crtend.o crtbeginS.o crtendS.o \
-	crt1-mmu.o gcrt1-mmu.o gcrt1.o $(IC_EXTRA_PARTS) $(OPT_EXTRA_PARTS)
+	crt1-mmu.o gcrt1-mmu.o gcrt1.o $(IC_EXTRA_PARTS) $(OPT_EXTRA_PARTS) \
+	trap-handler.o
 
 # Compile crt1-mmu.o as crt1.o with -DMMU_SUPPORT
 $(T)crt1-mmu.o: $(srcdir)/config/sh/crt1.asm $(GCC_PASSES)
@@ -13,3 +14,4 @@
 # For sh4-400: Compile gcrt1.o as crt1.o with -DPROFILE
 $(T)gcrt1.o: $(srcdir)/config/sh/crt1.asm $(GCC_PASSES)
 	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $(T)gcrt1.o -DPROFILE -x assembler-with-cpp $(srcdir)/config/sh/crt1.asm
+
Index: gcc/config/sh/sh-modes.def
===================================================================
--- gcc/config/sh/sh-modes.def	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/sh-modes.def	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -22,6 +22,11 @@
 /* PDI mode is used to represent a function address in a target register.  */
 PARTIAL_INT_MODE (DI);
 
+/* For software floating point comparisons.  */
+CC_MODE (CC_FP_NE);
+CC_MODE (CC_FP_GT);
+CC_MODE (CC_FP_UNLT);
+
 /* Vector modes.  */
 VECTOR_MODE  (INT, QI, 2);    /*                 V2QI */
 VECTOR_MODES (INT, 4);        /*            V4QI V2HI */
Index: gcc/config/sh/lib1funcs.h
===================================================================
--- gcc/config/sh/lib1funcs.h	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/lib1funcs.h	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,6 +1,7 @@
 /* Copyright (C) 1994, 1995, 1997, 1998, 1999, 2000, 2001, 2002, 2003,
    2004, 2005, 2006
    Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is free software; you can redistribute it and/or modify it
 under the terms of the GNU General Public License as published by the
@@ -69,13 +70,152 @@
 #endif /* !__LITTLE_ENDIAN__ */
 
 #ifdef __sh1__
+/* branch with two-argument delay slot insn */
 #define SL(branch, dest, in_slot, in_slot_arg2) \
 	in_slot, in_slot_arg2; branch dest
+/* branch with one-argument delay slot insn */
 #define SL1(branch, dest, in_slot) \
 	in_slot; branch dest
+/* branch with comparison in delay slot */
+#define SLC(branch, dest, in_slot, in_slot_arg2) \
+        branch dest; in_slot, in_slot_arg2
+/* comparison in a delay slot, at branch destination */
+#define SLI(in_slot, in_slot_arg2) in_slot, in_slot_arg2
+#define SLCMP(branch, cmp1, cmp1arg2, cmp2, cmp2arg2) \
+	branch .+6; bra .+6; cmp2, cmp2arg2; cmp1, cmp1arg2
+#define DMULU_SAVE \
+ mov.l r10,@-r15; \
+ mov.l r11,@-r15; \
+ mov.l r12,@-r15; \
+ mov.l r13,@-r15
+#define DMULUL(m1, m2, rl) \
+ swap.w m1,r12; \
+ mulu.w r12,m2; \
+ swap.w m2,r13; \
+ sts macl,r10; \
+ mulu.w r13,m1; \
+ clrt; \
+ sts macl,r11; \
+ mulu.w r12,r13; \
+ addc r11,r10; \
+ sts macl,r12; \
+ mulu.w m1,m2; \
+ movt r11; \
+ sts macl,rl; \
+ mov r10,r13; \
+ shll16 r13; \
+ addc r13,rl; \
+ xtrct r11,r10; \
+ addc r10,r12 \
+/* N.B. the carry is cleared here.  */
+#define DMULUH(rh) mov r12,rh
+#define DMULU_RESTORE \
+ mov.l @r15+,r13; \
+ mov.l @r15+,r12; \
+ mov.l @r15+,r11; \
+ mov.l @r15+,r10
 #else /* ! __sh1__ */
+/* branch with two-argument delay slot insn */
 #define SL(branch, dest, in_slot, in_slot_arg2) \
-	branch##.s dest; in_slot, in_slot_arg2
+	branch##/s dest; in_slot, in_slot_arg2
+/* branch with one-argument delay slot insn */
 #define SL1(branch, dest, in_slot) \
 	branch##/s dest; in_slot
+/* branch with comparison in delay slot */
+#define SLC(branch, dest, in_slot, in_slot_arg2) \
+        branch##/s dest; in_slot, in_slot_arg2
+/* comparison in a delay slot, at branch destination */
+#define SLI(in_slot, in_slot_arg)
+#define SLCMP(branch, cmp1, cmp1arg2, cmp2, cmp2arg2) \
+	branch##/s .+6; cmp1, cmp1arg2; cmp2, cmp2arg2
+#define DMULU_SAVE
+#define DMULUL(m1, m2, rl) dmulu.l m1,m2; sts macl,rl
+#define DMULUH(rh) sts mach,rh
+#define DMULU_RESTORE
 #endif /* !__sh1__ */
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+/* don't #define DYN_SHIFT */
+  #define SHLL4(REG)	\
+	shll2	REG;	\
+	shll2	REG
+
+  #define SHLR4(REG)	\
+	shlr2	REG;	\
+	shlr2	REG
+
+  #define SHLL6(REG)	\
+	shll2	REG;	\
+	shll2	REG;	\
+	shll2	REG
+
+  #define SHLR6(REG)	\
+	shlr2	REG;	\
+	shlr2	REG;	\
+	shlr2	REG
+
+  #define SHLL12(REG)	\
+	shll8	REG;	\
+	SHLL4 (REG)
+
+  #define SHLR12(REG)	\
+	shlr8	REG;	\
+	SHLR4 (REG)
+
+  #define SHLR19(REG)	\
+	shlr16	REG;	\
+	shlr2	REG;	\
+	shlr	REG
+
+  #define SHLL23(REG)	\
+	shll16	REG;	\
+	shlr	REG;	\
+	shll8	REG
+
+  #define SHLR24(REG)	\
+	shlr16	REG;	\
+	shlr8	REG
+
+  #define SHLR21(REG)	\
+	shlr16	REG;	\
+	shll2	REG;	\
+	add	REG,REG;\
+	shlr8	REG
+
+  #define SHLL21(REG)	\
+	shll16	REG;	\
+	SHLL4 (REG);	\
+	add	REG,REG
+
+  #define SHLR11(REG)	\
+	shlr8	REG;	\
+	shlr2	REG;	\
+	shlr	REG
+
+  #define SHLR22(REG)	\
+	shlr16	REG;	\
+	shll2	REG;	\
+	shlr8	REG
+
+  #define SHLR23(REG)	\
+	shlr16	REG;	\
+	add	REG,REG;\
+	shlr8	REG
+
+  #define SHLR20(REG)	\
+	shlr16	REG;	\
+	SHLR4 (REG)
+
+  #define SHLL20(REG)	\
+	shll16	REG;	\
+	SHLL4 (REG)
+#define SHLD_COUNT(N,COUNT)
+#define SHLRN(N,COUNT,REG) SHLR##N(REG)
+#define SHLLN(N,COUNT,REG) SHLL##N(REG)
+#else
+#define SHLD_COUNT(N,COUNT) mov #N,COUNT
+#define SHLRN(N,COUNT,REG) shld COUNT,REG
+#define SHLLN(N,COUNT,REG) shld COUNT,REG
+#define DYN_SHIFT 1
+#endif
+
Index: gcc/config/sh/lib1funcs-Os-4-200.asm
===================================================================
--- gcc/config/sh/lib1funcs-Os-4-200.asm	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/lib1funcs-Os-4-200.asm	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,4 +1,5 @@
 /* Copyright (C) 2006 Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is free software; you can redistribute it and/or modify it
 under the terms of the GNU General Public License as published by the
@@ -30,7 +31,6 @@
 
 #include "lib1funcs.h"
 
-#if !__SHMEDIA__
 #ifdef L_udivsi3_i4i
 
 /* 88 bytes; sh4-200 cycle counts:
@@ -44,7 +44,7 @@
 	.global GLOBAL(udivsi3_i4i)
 	FUNC(GLOBAL(udivsi3_i4i))
 GLOBAL(udivsi3_i4i):
-	mova L1,r0
+	mova LOCAL(L1),r0
 	cmp/pz r5
 	sts fpscr,r1
 	lds.l @r0+,fpscr
@@ -111,7 +111,7 @@
 	movt r0
 
 	.p2align 2
-L1:
+LOCAL(L1):
 #ifndef FMOVD_WORKS
 	.long 0x80000
 #else
@@ -149,7 +149,7 @@
 	mov.l @r15+,r2
 #endif /* 0 */
 
-/* Size: 186 bytes jointly for udivsi3_i4i and sdivsi3_i4i
+/* Size: 188 bytes jointly for udivsi3_i4i and sdivsi3_i4i
    sh4-200 run times:
    udiv small divisor: 55 cycles
    udiv large divisor: 52 cycles
@@ -277,7 +277,7 @@
 GLOBAL(sdivsi3_i4i):
 	sts.l fpscr,@-r15
 	sts fpul,r1
-	mova L1,r0
+	mova LOCAL(L1),r0
 	lds.l @r0+,fpscr
 	lds r4,fpul
 #ifdef FMOVD_WORKS
@@ -314,7 +314,7 @@
 	lds r1,fpul
 
 	.p2align 2
-L1:
+LOCAL(L1):
 #ifndef FMOVD_WORKS
 	.long 0x80000
 #else
@@ -324,4 +324,3 @@
 	ENDFUNC(GLOBAL(sdivsi3_i4i))
 #endif /* __SH_FPU_DOUBLE__ */
 #endif /* L_sdivsi3_i4i */
-#endif /* !__SHMEDIA__ */
Index: gcc/config/sh/IEEE-754/divsf3.S
===================================================================
--- gcc/config/sh/IEEE-754/divsf3.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/divsf3.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,404 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!divides two single precision floating point 
+
+! Author: Aanchal Khanna
+
+! Arguments: Dividend is in r4, divisor in r5
+! Result: r0
+
+! r4 and r5 are referred as op1 and op2 resp.
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align	5
+	.global	GLOBAL (divsf3)
+	FUNC (GLOBAL (divsf3))
+
+GLOBAL (divsf3):
+	mov.l	.L_mask_sign,r1
+	mov	r4,r3
+
+	xor	r5,r3
+	shll	r4
+
+	shlr	r4
+	mov.l	.L_inf,r2
+
+	and	r3,r1		!r1=resultant sign
+	mov	r4,r6
+
+	shll	r5
+	mov	#0,r0		
+
+	shlr	r5
+	and	r2,r6
+
+	cmp/eq	r2,r6
+	mov	r5,r7
+
+	and     r2,r7
+	bt	.L_op1_inv
+
+	cmp/eq	r2,r7
+	mov	#-23,r3
+
+	bt	.L_op2_inv
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLR23 (r6)
+	SHLR23 (r7)
+#else
+	shld	r3,r6
+	shld	r3,r7
+#endif
+
+	cmp/eq	r0,r4
+
+	bt	.L_op1_zero		!dividend=0
+	cmp/eq	r0,r6
+
+	mov.l   .L_imp_bit,r3
+	bt	.L_norm_op1		!normalize dividend
+.L_chk_op2:
+	cmp/eq	r0,r5
+	bt	.L_op2_zero		!divisor=0
+
+	cmp/eq	r0,r7
+	bt	.L_norm_op2		!normalize divisor
+
+.L_div1:
+	sub	r7,r6
+	add	#127,r6			!r6=resultant exponent
+
+	mov     r3,r7
+	mov.l	.L_mask_mant,r3
+
+	and	r3,r4
+	!chk exponent for overflow
+        mov.l   .L_255,r2
+
+	and     r3,r5
+	or	r7,r4
+
+	cmp/ge  r2,r6
+	or	r7,r5
+
+	bt	.L_return_inf
+	mov	r0,r2
+
+	cmp/eq  r4,r5
+	bf      .L_den_one
+
+	cmp/ge	r6,r0
+	!numerator=denominator, quotient=1, remainder=0
+	mov	r7,r2			
+
+	mov     r0,r4
+	!chk exponent for underflow
+	bt	.L_underflow
+        bra     .L_pack
+        nop
+
+.L_den_one:
+	!denominator=1, result=numerator
+
+	cmp/eq  r7,r5
+        bf      .L_divide
+
+	!chk exponent for underflow
+	cmp/ge  r6,r0
+        mov    r4,r2           
+
+        SL(bt,    .L_underflow,
+	 mov	r0,r4)
+	bra     .L_pack
+	nop
+
+.L_divide:
+	!dividing the mantissas r4<-dividend, r5<-divisor
+
+	cmp/hi	r4,r5
+	bf	.L_loop
+
+	shll	r4		! if mantissa(op1)< mantissa(op2)
+	add     #-1,r6		! shift left the numerator and decrease the exponent.
+
+.L_loop:
+	!division loop
+
+	cmp/ge	r5,r4
+	bf	.L_skip
+
+	or	r7,r2
+	sub	r5,r4
+
+.L_skip:
+	shlr	r7
+	shll	r4
+
+	cmp/eq	r0,r7
+	bf	.L_loop
+
+	!chk the exponent for underflow
+	cmp/ge  r6,r0
+	bt      .L_underflow
+	
+	!apply rounding
+	cmp/gt	r5,r4
+	bt	.L_round1
+
+	cmp/eq	r4,r5
+	bt	.L_round2
+
+.L_pack:
+	!pack the result, r1=sign, r2=quotient, r6=exponent
+
+	mov    #23,r4
+	and     r3,r2
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r6)
+#else
+	shld	r4,r6
+#endif
+	or	r2,r1
+
+	or	r6,r1
+	mov	r1,r0	
+	
+	rts
+	nop
+
+.L_round1:
+	!Apply proper rounding
+
+        bra     .L_pack
+        add     #1,r2
+
+.L_round2:
+	!Apply proper rounding
+
+        mov.l   .L_comp_1,r5
+        bra     .L_pack
+        and     r5,r2
+
+.L_op1_inv:
+	!chk if op1 is Inf or NaN
+
+	mov.l	.L_mask_mant,r3
+	mov	r4,r6
+
+	and	r3,r6
+	cmp/hi	r0,r6
+
+	bt	.L_ret_op1
+	cmp/eq	r2,r7
+
+	SL(bf,	.L_ret_op1,
+	 mov	r1,r0)
+
+	rts
+	mov	#-1,r0	! 0/0, return NaN
+	
+.L_op2_inv:
+	!chk if op2 is Inf or NaN
+
+	mov.l	.L_mask_mant,r3
+	mov	r5,r7
+	
+	and	r3,r7
+	cmp/hi	r0,r7
+
+	bt	.L_ret_op2
+	mov	r1,r0
+	
+	rts
+	nop
+
+.L_op1_zero:
+	!op1 is zero. If op2 is zero, return NaN, else return zero
+
+	cmp/eq	r0,r5
+
+	bf	.L_ret_op1	
+
+	rts
+	mov	#-1,r0
+
+.L_op2_zero:
+	!B is zero,return Inf
+
+	rts
+	or	r2,r0
+
+.L_return_inf:
+	mov.l	.L_inf,r0
+	
+	rts
+	or	r1,r0
+
+.L_norm_op1:
+	!normalize dividend
+
+	shll	r4
+	tst	r2,r4
+	
+	add     #-1,r6
+	bt	.L_norm_op1
+
+	bra	.L_chk_op2
+	add	#1,r6
+
+.L_norm_op2:
+	!normalize divisor
+
+	shll	r5
+	tst	r2,r5
+	
+	add	#-1,r7
+	bt	.L_norm_op2
+
+	bra	.L_div1
+	add	#1,r7
+
+.L_underflow:
+	!denormalize the result
+
+	add	#1,r6
+	mov	#-24,r7
+
+	cmp/gt	r6,r7
+	mov	r2,r5
+
+	bt	.L_return_zero
+	add     #-1,r6
+
+	mov	#32,r3
+	neg	r6,r7
+
+	add	#1,r7
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r6,r2
+#else
+	cmp/ge	r0,r6
+	bf	.L_mov_right
+
+.L_mov_left:
+	cmp/eq	r0,r6
+	bt	.L_out
+
+	shll	r2
+	bra	.L_mov_left
+	add	#-1,r6
+
+.L_mov_right:
+	cmp/eq	r0,r6
+	bt	.L_out
+
+	add	#1,r6
+	bra	.L_mov_right
+	shlr	r2
+	
+.L_out:
+#endif
+	sub	r7,r3
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r3,r5
+#else
+	cmp/ge	r0,r3
+	bf	.L_mov_right_1
+
+.L_mov_left_1:
+	shll	r5
+	add	#-1,r3
+
+	cmp/eq	r0,r3
+	bf	.L_mov_left_1
+
+	bt	.L_out_1
+
+.L_mov_right_1:
+	cmp/eq	r0,r3
+	bt	.L_out_1
+
+	add	#1,r3
+	bra	.L_mov_right_1
+	shlr	r5
+
+.L_out_1:
+#endif
+	shlr	r2
+	addc	r0,r2
+
+	cmp/eq	r4,r0		!r4 contains the remainder
+	mov      r2,r0
+
+	mov.l	.L_mask_sign,r7
+	bf	.L_return
+
+	mov.l   .L_comp_1,r2
+	cmp/eq	r7,r5
+
+	bf	.L_return
+	and	r2,r0
+
+.L_return:
+	rts
+	or     r1,r0
+	
+.L_ret_op1:
+	rts
+	or	r4,r0
+
+.L_ret_op2:
+	rts
+	or	r5,r0
+
+.L_return_zero:
+	rts
+	or	r1,r0
+
+
+
+	.align	2
+.L_inf:
+	.long	0x7f800000
+.L_mask_sign:
+	.long	0x80000000
+.L_mask_mant:
+	.long	0x007fffff
+.L_imp_bit:
+	.long	0x00800000
+.L_comp_1:
+	.long	0xfffffffe
+.L_255:
+	.long	255
+
+ENDFUNC (GLOBAL (divsf3))
Index: gcc/config/sh/IEEE-754/m3/divsf3.S
===================================================================
--- gcc/config/sh/IEEE-754/m3/divsf3.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/m3/divsf3.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,375 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! divsf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+! long 0th..3rd significant byte
+#ifdef __LITTLE_ENDIAN__
+#define L0SB	3
+#define L1SB	2
+#define L2SB	1
+#define L3SB	0
+#else
+#define L0SB	0
+#define L1SB	1
+#define L2SB	2
+#define L3SB	3
+#endif
+
+! clobbered: r0,r1,r2,r3,r6,r7,T (and for sh.md's purposes PR)
+!
+! Note: When the divisor is larger than the divident, we have to adjust the
+! exponent down by one.  We do this automatically when subtracting the entire
+! exponent/fraction bitstring as an integer, by means of the borrow from
+! bit 23 to bit 24.
+! Note: non-denormal rounding of a division result cannot cause fraction
+! overflow / exponent change. (r4 > r5 : fraction must stay in (2..1] interval;
+! r4 < r5: having an extra bit of precision available, even the smallest
+! possible difference of the result from one is rounded in all rounding modes
+! to a fraction smaller than one.)
+! sh4-200: 59 cycles
+! sh4-300: 44 cycles
+! tab indent: exponent / sign computations
+! tab+space indent: fraction computation
+FUNC(GLOBAL(divsf3))
+	.global GLOBAL(divsf3)
+	.balign	4
+GLOBAL(divsf3):
+	mov.l	LOCAL(x7f800000),r3
+	mov	#1,r2
+	mov	r4,r6
+	 shll8	 r6
+	mov	r5,r7
+	 shll8	 r7
+	rotr	r2
+	tst	r3,r4
+	or	r2,r6
+	bt/s	LOCAL(denorm_arg0)
+	or	r2,r7
+	tst	r3,r5
+	bt	LOCAL(denorm_arg1)
+	mov.l	LOCAL(x3f000000),r3	! bias minus explict leading 1
+	 div0u
+LOCAL(denorm_done):
+!	mov.l	LOCAL(x3f000000),r3	! bias minus explict leading 1
+	cmp/hs	r7,r6
+	mov.l	r8,@-r15
+	mov.l	LOCAL(xff800000),r8
+	bt		LOCAL(no_norm)
+	add  	r8,r3
+LOCAL(no_norm):
+	shlr	 r6
+	 div1	 r7,r6
+	 bt	 0f
+	 div1	r7,r6
+0:	mov.l	r9,@-r15
+	 div1	 r7,r6
+	mov	r4,r1
+	and	r8,r1
+	add	r1,r3
+	 div1	 r7,r6
+	and	r5,r8
+	sub	r8,r3	! result sign/exponent minus 1 if no overflow/underflow
+	 div1	 r7,r6
+	or	r3,r2
+	 div1	 r7,r6
+	mov.w	LOCAL(xff00),r9
+	 div1	 r7,r6
+	mov.l	r2,@-r15 ! L0SB is 0xff iff denorm / infinity exp is computed
+	 div1	 r7,r6
+	mov.w	LOCAL(m23),r2
+	 div1	 r7,r6
+	mov	r4,r0
+	 div1	 r7,r6
+	 extu.b	 r6,r1
+	 and	 r9,r6
+	 swap.w	 r1,r1	! first 8 bits of result fraction in bit 23..16
+	 div1	 r7,r6
+	shld	r2,r0
+	 div1	 r7,r6
+	mov.b	r0,@(L3SB,r15)	! 0xff iff divident was infinity / nan
+	 div1	 r7,r6
+	mov	r5,r0
+	 div1	 r7,r6
+	shld	r2,r0
+	 div1	 r7,r6
+	mov.b	r0,@(L2SB,r15)	! 0xff iff divisor was infinity / nan
+	 div1	 r7,r6
+	mov	r4,r0
+	 div1	 r7,r6
+	mov.w	LOCAL(m31),r2
+	 div1	 r7,r6
+	 extu.b	 r6,r8	! second 8 bits of result fraction in bit 7..0
+	 and	 r9,r6
+	mov.l	LOCAL(xff800000),r9
+	 div1	 r7,r6
+	xor	r5,r0	! msb := correct result sign
+	 div1	 r7,r6
+	xor	r3,r0	! xor with sign of result sign/exponent word
+	 div1	 r7,r6
+	shad	r2,r0
+	 div1	 r7,r6
+	mov.b	r0,@(L1SB,r15)	! 0xff	iff exponent over/underflows
+	and	r9,r3	! isolate sign / exponent
+	 div1	 r7,r6
+	 swap.b	r8,r0	! second 8 bits of result fraction in bit 15..8
+	 div1	 r7,r6
+	 or	r1,r0	! first 16 bits of result fraction in bit 23..8
+	 div1	 r7,r6
+	mov.w	LOCAL(m1),r9
+	 div1	 r7,r6
+	mov.l	@r15+,r8 ! load encoding of unusal exponent conditions
+	 extu.b	 r6,r1
+	 or	 r1,r0	! 24 bit result fraction with explicit leading 1
+	addc	r3,r0	! add in exponent / sign
+	cmp/str	r9,r8
+	! (no stall *here* for SH4-100 / SH4-200)
+	bt/s	LOCAL(inf_nan_denorm_zero)
+	mov.l	@r15+,r9
+	rts
+	mov.l	@r15+,r8
+
+/* The exponennt adjustment for denormal numbers is done by leaving an
+   adjusted value in r3; r4/r5 are not changed.  */
+	.balign	4
+LOCAL(denorm_arg0):
+	mov.w	LOCAL(xff00),r1
+	sub	r2,r6	! 0x800000000 : remove implict 1
+	tst	r6,r6
+	bt	LOCAL(div_zero)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	mov	r6,r0
+	shld	r0,r6
+	tst	r3,r5
+	mov.l	LOCAL(x3f800000),r3	! bias - 1 + 1
+	mov	#23,r1
+	shld	r1,r0
+	bt/s	LOCAL(denorm_arg1_2)
+	sub	r0,r3
+	bra	LOCAL(denorm_done)
+	 div0u
+
+LOCAL(denorm_arg1):
+	mov.l	LOCAL(x3f000000),r3	! bias - 1
+LOCAL(denorm_arg1_2):
+	sub	r2,r7	! 0x800000000 : remove implict 1
+	mov.w	LOCAL(xff00),r1
+	tst	r7,r7
+	bt	LOCAL(div_by_zero)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	mov	r7,r0
+	shld	r0,r7
+	add	#-1,r0
+	mov	#23,r1
+	shld	r1,r0
+	add	r0,r3
+	bra	LOCAL(denorm_done)
+	 div0u
+
+	.balign	4
+LOCAL(inf_nan_denorm_zero):
+! r0 has the rounded result, r6 has the non-rounded lowest bits & rest.
+! the bit just below the LSB of r6 is available as ~Q
+
+! Alternative way to get at ~Q:
+! if rounding took place, ~Q must be set.
+! if the rest appears to be zero, ~Q must be set.
+! if the rest appears to be nonzero, but rounding didn't take place,
+! ~Q must be clear;  the apparent rest will then require adjusting to test if 
+! the actual rest is nonzero.
+	mov	r0,r2
+	not	r8,r0
+	tst	#0xff,r0
+	shlr8	r0
+	mov.l	@r15+,r8
+	bt/s	LOCAL(div_inf_or_nan)
+	tst	#0xff,r0
+	mov	r4,r0
+	bt	LOCAL(div_by_inf_or_nan)
+	add	r0,r0
+	mov	r5,r1
+	add	r1,r1
+	cmp/hi	r1,r0
+	mov	r6,r0
+	bt	LOCAL(overflow)
+	sub	r2,r0
+	exts.b	r0,r0	! -1 if rounding took place
+	shlr8	r6	! isolate div1-mangled rest
+	addc	r2,r0	! generate carry if rounding took place
+	shlr8	r7
+	mov.l	LOCAL(xffffff),r1
+	sub	r3,r0	! pre-rounding fraction
+	bt	0f ! going directly to denorm_sticky would cause mispredicts
+	tst	r6,r6	! rest can only be zero if lost bit was set
+0:	add	r7,r6	! (T ? corrupt : reconstruct) actual rest
+	bt	0f
+	and r1,r6
+	cmp/pl	r6
+0:	mov.w	LOCAL(m24),r1
+	addc	r0,r0	! put in sticky bit
+	add	#-1,r3
+	mov.l	LOCAL(x80000000),r6
+	add	r3,r3
+	mov	r0,r2
+	shad	r1,r3	! exponent ; s32.0
+	!
+	cmp/pl	r3
+	bt/s	LOCAL(zero_nan)	! return zero
+	clrt
+	shld	r3,r0
+	add	#31,r3
+	cmp/pl	r3
+	shld	r3,r2
+	bf	LOCAL(zero_nan)	! return zero
+	rotl	r2
+	cmp/hi	r6,r2
+	mov	#0,r7
+	addc	r7,r0
+	shll	r0
+	div0s	r4,r5
+	rts
+	rotcr	r0
+	
+! ????
+! undo normal rounding (lowest bits still in r6). then do denormal rounding.
+	
+LOCAL(overflow):
+	mov.l	LOCAL(xff000000),r0
+	div0s	r4,r5
+	rts
+	rotcr	r0
+	
+LOCAL(div_inf_or_nan):
+	mov	r4,r0
+	bra	LOCAL(nan_if_t)
+	add	r0,r0
+	
+LOCAL(div_by_inf_or_nan):
+	mov.l	LOCAL(xff000000),r1
+	mov	#0,r0
+	mov	r5,r2
+	add	r2,r2
+	bra	LOCAL(nan_if_t)
+	cmp/hi	r1,r2
+
+
+
+! still need to check for divide by zero or divide by nan
+! r3: 0x7f800000
+	.balign	4
+LOCAL(div_zero):
+	mov	r5,r1
+	add	r1,r1
+	tst	r1,r1	! 0 / 0 -> nan
+	bt	LOCAL(nan)
+	add	r3,r3
+	cmp/hi	r3,r1	! 0 / nan -> nan (but 0 / inf -> 0)
+LOCAL(zero_nan):
+	mov	#0,r0
+LOCAL(nan_if_t):
+	bf	0f:
+LOCAL(nan):
+	mov	#-1,r0
+0:	div0s	r4,r5	! compute sign
+	rts
+	rotcr	r0	! insert sign
+
+LOCAL(div_by_zero):
+	mov.l	LOCAL(xff000000),r0
+	mov	r4,r2
+	add	r2,r2
+	bra	LOCAL(nan_if_t)
+	cmp/hi	r0,r2
+	
+	.balign	4
+LOCAL(clz):
+	mov.l	r8,@-r15
+	extu.w	r0,r8
+	mov.l	r9,@-r15
+	cmp/eq	r0,r8
+	bt/s	0f
+	mov	#32,r9
+	shlr16	r0
+	extu.w	r0,r8
+	add	#-16,r9
+0:	tst	r1,r8	! 0xff00
+	mov.l	LOCAL(c_clz_tab),r0
+	bt	0f
+	shlr8	r8
+0:	bt	0f
+	add	#-8,r9
+0:
+#ifdef	__PIC__
+	add	r0,r8
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r8),r8
+	mov	r9,r0
+	mov.l	@r15+,r9
+	!
+	!
+	!
+	sub	r8,r0
+	mov.l	@r15+,r8
+	rts
+	lds.l	@r15+,pr
+
+!	We encode even some words as pc-relative that would fit as immediate
+!	in the instruction in order to avoid some pipeline stalls on
+!	SH4-100 / SH4-200.
+LOCAL(m23):	.word -23
+LOCAL(m24):	.word -24
+LOCAL(m31):	.word -31
+LOCAL(xff01):	.word 0xff01
+	.balign	4
+LOCAL(xff000000): .long 0xff000000
+#ifdef __LITTLE_ENDIAN__
+LOCAL(xff00):	.word 0xff00
+LOCAL(m1):	.word -1
+#else
+LOCAL(m1):	.word -1
+LOCAL(xff00):	.word 0xff00
+#endif
+LOCAL(xffffff): .long 0xffffff
+LOCAL(x7f800000): .long 0x7f800000
+LOCAL(x3f000000): .long 0x3f000000
+LOCAL(x3f800000): .long 0x3f800000
+LOCAL(xff800000): .long 0xff800000
+LOCAL(x40000000): .long 0x40000000
+LOCAL(x80000000): .long 0x80000000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(divsf3))
Index: gcc/config/sh/IEEE-754/m3/divdf3.S
===================================================================
--- gcc/config/sh/IEEE-754/m3/divdf3.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/m3/divdf3.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,668 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! divdf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke joern.rennecke@st.com
+
+/* y = 1/x  ; x (- [1,2)
+   y0 = 1.5 - x/2 - tab[(1-x)*64] = y + d ; abs(d)/y <= 0x1.0c/256
+
+   y1 = y0 - ((y0) * x - 1) * y0  =  y-x*d^2
+   y2 = y1 - ((y1) * x - 1) * y1 =~= y-x^3*d^4
+
+   z0 = y2*a ;  a1 = a - z0*x /# 32 * 64 -> 64 bit #/
+   z1 = y2*a1 (round to nearest odd 0.5 ulp);
+   a2 = a1 - z1*x /# 32 * 64 -> 64 bit #/
+
+   z = a/x = z0 + z1 - 0.5 ulp + (a2 > 0) * ulp
+
+   Unless stated otherwise, multiplies can be done in 32 * 32 bit or less
+   with suitable scaling and/or top truncation.
+   We use a slightly modified algorithm here that checks if the lower
+   bits in z1 are sufficient to determine the outcome of rounding - in that
+   case a2 is not computed.
+   -z1 is computed in units of 1/128 ulp, with an error in the range
+   -0x3.e/128 .. +0 ulp.
+   Thus, after adding three, the result can be safely rounded for normal
+   numbers if any of the bits 5..2 is set, or if the highest guard bit
+   (bit 6 if y <1, otherwise bit 7) is set.
+   (Because of the way truncation works, we would be fine for an open
+    error interval of (-4/128..+1/128) ulp )
+   For denormal numbers, the rounding point lies higher, but it would be
+   quite cumbersome to calculate where exactly; it is sufficient if any
+   of the bits 7..3 is set.
+   x truncated to 20 bits is sufficient to calculate y0 or even y1.
+   Table entries are adjusted by about +128 to use full signed byte range.
+   This adjustment has been perturbed slightly to allow cse with the
+   shift count constant -26.
+   The threshold point for the shift adjust before rounding is found by
+   comparing the fractions, which is exact, unlike the top bit of y2.
+   Therefore, the top bit of y2 becomes slightly random after the adjustment
+   shift, but that's OK because this can happen only at the boundaries of
+   the interval, and the biasing of the error means that it can in fact happen
+   only at the bottom end.  And there, the carry propagation will make sure
+   that in the end we will have in effect an implicit 1 (or two whem rounding
+   up...)  */
+/* If an exact result exists, it can have no more bits than the divident.
+   Hence, we don't need to bother with the round-to-even tie breaker
+   unless the result is denormalized.  */
+/* 64 cycles through main path for sh4-300 (about 93.7% of normalized numbers),
+   82 for the path for rounding tie-breaking for normalized numbers
+   (including one branch mispredict).
+   Some cycles might be saved by more careful register allocation.  */
+
+#define x_h r12
+#define yn  r3
+
+FUNC(GLOBAL(divdf3))
+ .global GLOBAL(divdf3)
+
+/* Adjust arg0 now, too.  We still have to come back to denorm_arg1_done,
+   since we heven't done any of the work yet that we do till the denorm_arg0
+   entry point.  We know that neither of the arguments is inf/nan, but
+   arg0 might be zero.  Check for that first to avoid having to establish an
+   rts return address.  */
+LOCAL(both_denorm):
+	mov.l	r9,@-r15
+	mov	DBL0H,r1
+	mov.l	r0,@-r15
+	shll2	r1
+	mov.w LOCAL(both_denorm_cleanup_off),r9
+	or	DBL0L,r1
+	tst	r1,r1
+	mov	DBL0H,r0
+	bf/s	LOCAL(zero_denorm_arg0_1)
+	shll2	r0
+	mov.l	@(4,r15),r9
+	add	#8,r15
+	bra	LOCAL(ret_inf_nan_0)
+	mov	r1,DBLRH
+
+LOCAL(both_denorm_cleanup):
+	mov.l	@r15+,r0
+	!
+	mov.l	@r15+,r9
+ mov	#12,r3
+ mov.l	LOCAL(xfffe2006),r1	! yn := (-1. << 17) + (0x80 << 6) ; shift #-26
+	bra	LOCAL(denorm_arg1_done)
+	!
+	add	r0,DBL0H
+
+/* Denorm handling leaves the incoming denorm argument with an exponent of +1
+   (implicit 1).  To leave the result exponent unaltered, the other
+   argument's exponent is adjusted by the the shift count.  */
+
+	.balign 4
+LOCAL(arg0_tiny):
+	bsr	LOCAL(clz)
+	mov	DBL0L,r0
+	shll	DBL0H
+	add	#1,r0
+	mov	DBL0L,DBL0H
+	shld	r0,DBL0H
+	rotcr	DBL0H
+	
+	cmp/pl r0
+	bf/s	LOCAL(a0t_dpt_neg)
+	tst	DBL0L,DBL0L	/* Check for divide of zero.  */
+	add	#-32,r0
+	shld	r0,DBL0L
+	bf/s	LOCAL(adjust_arg1_exp)
+	add	#63,r0
+	bra 	LOCAL(return_0)
+	nop
+
+LOCAL(a0t_dpt_neg):
+	add 	#31,r0
+	bf/s	LOCAL(adjust_arg1_exp)
+	shld	r0,DBL0L
+		  
+LOCAL(return_0): /* Return 0 with appropriate sign.  */
+	mov.l	@r15+,r10
+	mov	#0,DBLRH
+	mov.l	@r15+,r9
+	bra	LOCAL(ret_inf_nan_0)
+	mov.l	@r15+,r8
+
+	.balign 4
+LOCAL(arg1_tiny):
+	bsr	LOCAL(clz)
+	mov	DBL1L,r0
+	shll	DBL1H
+	add	#1,r0
+	mov	DBL1L,DBL1H
+	shld	r0,DBL1H
+	rotcr	DBL1H
+
+	cmp/pl r0
+	bf/s	LOCAL(a1t_dpt_neg)
+	tst	DBL1L,DBL1L	/* Check for divide by zero.  */
+	add	#-32,r0
+	shld	r0,DBL1L
+	bf/s	LOCAL(adjust_arg0_exp)
+	add	#63,r0
+	bra 	LOCAL(a1t_end)
+	nop
+
+LOCAL(a1t_dpt_neg):
+	add 	#31,r0
+	bf/s	LOCAL(adjust_arg0_exp)
+	shld	r0,DBL1L
+	
+LOCAL(a1t_end):		  		  
+	mov	DBL0H,r0
+	add	r0,r0
+	tst	r0,r0	! 0 / 0 ?
+	mov	#-1,DBLRH
+	bf	LOCAL(return_inf)
+	!
+	tst   DBL0L,DBL0L
+	bf	LOCAL(return_inf)
+	bt	LOCAL(ret_inf_nan_0)
+	!
+
+	.balign 4
+LOCAL(zero_denorm_arg1):
+	not	DBL0H,r3
+	mov	DBL1H,r0
+	tst	r2,r3
+	shll2	r0
+	bt	LOCAL(early_inf_nan_arg0)
+	tst	r0,r0
+	mov.w	LOCAL(xff00),r12
+	bt/s	LOCAL(arg1_tiny)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	shlr2	r0
+	!
+	shll	DBL1H
+	mov	DBL1L,r3
+	shld	r0,DBL1H
+	shld	r0,DBL1L
+	rotcr	DBL1H
+	add	#-32,r0
+	shld	r0,r3
+	add	#32,r0
+	or	r3,DBL1H
+LOCAL(adjust_arg0_exp):
+	tst	r2,DBL0H
+	mov	#20,r3
+	shld	r3,r0
+	bt	LOCAL(both_denorm)
+	add	DBL0H,r0
+	div0s	r0,DBL0H	! Check for obvious overflow.  */
+	not	r0,r3		! Check for more subtle overflow - lest
+	bt	LOCAL(return_inf)
+	mov	r0,DBL0H
+	tst	r2,r3		! we mistake it for NaN later
+	mov	#12,r3
+	bf	LOCAL(denorm_arg1_done)
+LOCAL(return_inf): /* Return infinity with appropriate sign.  */
+	mov	#20,r3
+	mov	#-2,DBLRH
+	bra	LOCAL(ret_inf_nan_0)
+	shad	r3,DBLRH
+
+/* inf/n -> inf; inf/0 -> inf; inf/inf -> nan; inf/nan->nan  nan/x -> nan */
+LOCAL(inf_nan_arg0):
+	mov.l	@r15+,r10
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+LOCAL(early_inf_nan_arg0):
+	not	DBL1H,r3
+	mov	DBL0H,DBLRH
+	tst	r2,r3	! both inf/nan?
+	add	DBLRH,DBLRH
+	bf	LOCAL(ret_inf_nan_0)
+	mov	#-1,DBLRH
+LOCAL(ret_inf_nan_0):
+	mov	#0,DBLRL
+	mov.l	@r15+,r12
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	
+/* Already handled: inf/x, nan/x .  Thus: x/inf -> 0; x/nan -> nan */
+	.balign	4
+LOCAL(inf_nan_arg1):
+	mov	DBL1H,r2
+	mov	#12,r1
+	shld	r1,r2
+	mov.l	@r15+,r10
+	mov	#0,DBLRL
+	mov.l	@r15+,r9
+	or	DBL1L,r2
+	mov.l	@r15+,r8
+	cmp/hi	DBLRL,r2
+	mov.l	@r15+,r12
+	subc	DBLRH,DBLRH
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	
+	.balign 4
+LOCAL(zero_denorm_arg0):
+	mov.w	LOCAL(denorm_arg0_done_off),r9
+	not	DBL1H,r1
+	mov	DBL0H,r0
+	tst	r2,r1
+	shll2	r0
+	bt	LOCAL(inf_nan_arg1)
+LOCAL(zero_denorm_arg0_1):
+	tst	r0,r0
+	mov.w	LOCAL(xff00),r12
+	bt/s	LOCAL(arg0_tiny)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	shlr2	r0
+	shll	DBL0H
+	mov	DBL0L,r12
+	shld	r0,DBL0H
+	shld	r0,DBL0L
+	rotcr	DBL0H
+	add	#-32,r0
+	shld	r0,r12
+	add	#32,r0
+	or	r12,DBL0H
+LOCAL(adjust_arg1_exp):
+	mov	#20,r12
+	shld	r12,r0
+	add	DBL1H,r0
+	div0s	r0,DBL1H	! Check for obvious underflow.  */
+	not	r0,r12		! Check for more subtle underflow - lest
+	bt	LOCAL(return_0)
+	mov	r0,DBL1H
+	tst	r2,r12		! we mistake it for NaN later
+	bt	LOCAL(return_0)
+	!
+	braf	r9
+	mov	#13,r0
+LOCAL(zero_denorm_arg1_dispatch):
+
+LOCAL(xff00):	.word 0xff00
+LOCAL(denorm_arg0_done_off):
+	.word LOCAL(denorm_arg0_done)-LOCAL(zero_denorm_arg1_dispatch)
+LOCAL(both_denorm_cleanup_off):
+	.word LOCAL(both_denorm_cleanup)-LOCAL(zero_denorm_arg1_dispatch)
+
+ .balign	8
+GLOBAL(divdf3):
+ mov.l	LOCAL(x7ff00000),r2
+ mov	#12,r3
+ mov.l	LOCAL(xfffe2006),r1	! yn := (-1. << 17) + (0x80 << 6) ; shift #-26
+ tst	r2,DBL1H
+ mov.l	r12,@-r15
+ bt	LOCAL(zero_denorm_arg1)
+
+LOCAL(denorm_arg1_done):
+ mov	DBL1H,x_h	! x_h live in r12
+ shld	r3,x_h	! x - 1 ; u0.20
+ mov	x_h,yn
+ mova	LOCAL(ytab),r0
+ mov.l	r8,@-r15
+ shld	r1,yn	! x-1 ; u26.6
+ mov.b	@(r0,yn),yn
+ mov	#6,r0
+ mov.l	r9,@-r15
+ mov	x_h,r8
+ mov.l	r10,@-r15
+ shlr16	x_h	! x - 1; u16.16	! x/2 - 0.5 ; u15.17
+ add	x_h,r1	! SH4-200 single-issues this insn
+ shld	r0,yn
+ sub	r1,yn	! yn := y0 ; u15.17
+ mov	DBL1L,r1
+ mov	#-20,r10
+ mul.l	yn,x_h	! r12 dead
+ swap.w	yn,r9
+ shld	r10,r1
+ sts	macl,r0	! y0 * (x-1) - n ; u-1.32
+ add	r9,r0	! y0 * x - 1     ; s-1.32
+ tst	r2,DBL0H
+ dmuls.l r0,yn
+ mov.w	LOCAL(d13),r0
+ or	r1,r8	! x  - 1; u0.32
+ add	yn,yn	! yn = y0 ; u14.18
+ bt	LOCAL(zero_denorm_arg0)
+
+LOCAL(denorm_arg0_done):
+ sts	mach,r1	!      d0 ; s14.18
+ sub	r1,yn	! yn = y1 ; u14.18 ; <= 0x3fffc
+ mov	DBL0L,r12
+ shld	r0,yn	! yn = y1 ; u1.31 ; <= 0x7fff8000
+ mov.w	LOCAL(d12),r9
+ dmulu.l yn,r8
+ shld	r10,r12
+ mov	yn,r0
+ mov	DBL0H,r8
+ add	yn,yn	! yn = y1 ; u0.32 ; <= 0xffff0000
+ sts	mach,r1	! y1 * (x-1); u1.31
+ add	r0,r1	! y1 * x    ; u1.31
+ dmulu.l yn,r1
+ not	DBL0H,r10
+ shld	r9,r8
+ tst	r2,r10
+ or	r8,r12	! a - 1; u0.32
+ bt	LOCAL(inf_nan_arg0)
+ sts	mach,r1	! d1+yn; u1.31
+ sett		! adjust y2 so that it can be interpreted as s1.31
+ not	DBL1H,r10
+ subc	r1,yn	! yn := y2 ; u1.31 ; can be 0x7fffffff
+ mov.l	LOCAL(x001fffff),r9
+ dmulu.l yn,r12
+ tst	r2,r10
+ or	DBL1H,r2
+ bt	LOCAL(inf_nan_arg1)
+ mov.l	r11,@-r15
+ sts	mach,r12	! y2*(a-1) ; u1.31
+ add	yn,r12		! z0       ; u1.31
+ dmulu.l r12,DBL1L
+ mov.l	LOCAL(x40000000),DBLRH ! bias + 1
+ and	r9,r2		! x ; u12.20
+ cmp/hi	DBL0L,DBL1L
+ sts	macl,r8
+ mov	#-24,r11
+ sts	mach,r9 	! r9:r8 := z0 * DBL1L; u-19.64
+ 
+ subc	DBL1H,DBLRH
+ mul.l	r12,r2  	! (r9+macl):r8 == z0*x; u-19.64
+ shll	r8
+ add	DBL0H,DBLRH	! result sign/exponent + 1
+ mov	r8,r10
+ sts	macl,DBLRL
+ add	DBLRL,r9
+ rotcl	r9		! r9:r8 := z*x; u-20.63
+ shld	r11,r10
+
+! mov.l	LOCAL(xfff00000),DBLRL
+! mov DBL1H,r11
+! and DBLRL,r11
+! subc r11,DBLRH
+! add DBL0H,DBLRH
+		  
+ mov.l	LOCAL(x7fe00000),DBLRL
+ sub	DBL0L,r9	! r9:r8 := -a ; u-20.63
+ cmp/pz	r9		! In corner cases this shift can loose ..
+ shll8	r9		!  .. the sign, so check it first.
+ mov.l	LOCAL(x00200000),r11
+ !mov.l	LOCAL(x00100000),r11
+ or	r10,r9	! -a1 ; s-28.32
+ mov.l	LOCAL(x00100000),r10
+ dmulu.l r9,yn	! sign for r9 is in T
+ xor	DBL0H,DBL1H	! calculate expected sign & bit20
+ mov.w	LOCAL(d120),DBL0H ! to test bits 6..4
+ xor	DBLRH,DBL1H
+ !
+ sts	mach,DBL0L	! -z1 ; s-27.32
+ bt 0f
+ sub	yn,DBL0L	! multiply adjust for -a1 negative; r3 dies here
+0:tst	r10,DBL1H		! set T if a >= x
+ mov.l LOCAL(xfff00000),r3
+ bt	0f
+ add	DBL0L,DBL0L	! z1 ; s-27.32 / s-28.32
+0:bt 0f
+ add	r12,r12	! z0 ; u1.31 / u0.31
+0:add	#6-64,DBL0L
+ and	r3,DBLRH	! isolate sign / exponent
+ tst	DBL0H,DBL0L
+ bf/s	LOCAL(exact)	! make the hot path taken for best branch prediction
+ cmp/pz	DBL1H
+
+! Unless we follow the next branch, we need to test which way the rounding
+! should go.
+! For normal numbers, we know that the result is not exact, so the sign
+! of the rest will be conclusive.
+! We generate a number that looks safely rounded so that denorm handling
+! can safely test the number twice.
+! r10:r8 == 0 will indicate if the number was exact, which can happen
+! when we come here for denormals to check a number that is close or
+! equal to a result in whole ulps.
+ bf	LOCAL(ret_denorm_inf)	! denorm or infinity, DBLRH has inverted sign
+ add	#64,DBL0L
+LOCAL(find_adjust): tst	r10,DBL1H ! set T if a >= x
+ mov	#-2,r10
+ addc	r10,r10
+ mov	DBL0L,DBLRL	! z1 ; s-27.32 / s-28.32 ; lower 4 bits unsafe.
+ shad	r10,DBLRL	! tentatively rounded z1 ; s-24.32
+ shll8	r8		! r9:r8 := -a1 ; s-28.64
+ clrt
+ dmuls.l DBLRL,DBL1L	! DBLRL signed, DBL1L unsigned
+ mov	r8,r10
+ shll16	r8		! r8  := lowpart  of -a1 ; s-44.48
+ xtrct	r9,r10		! r10 := highpart of -a1 ; s-44.48
+ !
+ sts	macl,r3
+ subc	r3,r8
+ sts	mach,r3
+ subc	r3,r10
+ cmp/pz	DBL1L
+ mul.l	DBLRL,r2
+ bt	0f
+ sub	DBLRL,r10	! adjust for signed/unsigned multiply
+0: mov.l	LOCAL(x7fe00000),DBLRL
+ mov	#-26,r2
+ sts	macl,r9
+ sub	r9,r10		! r10:r8 := -a2
+ add	#-64+16,DBL0L	! the denorm code negates this adj. for exact results
+ shld	r2,r10		! convert sign into adjustment in the range 32..63
+ sub	r10,DBL0L
+ cmp/pz	DBL1H
+
+ .balign 4
+LOCAL(exact):
+ bf	LOCAL(ret_denorm_inf)	! denorm or infinity, DBLRH has inverted sign
+ tst	DBLRL,DBLRH
+ bt	LOCAL(ret_denorm_inf)	! denorm, DBLRH has correct sign
+ mov	#-7,DBL1H
+ cmp/pz	DBL0L		! T is sign extension of z1
+ not	DBL0L,DBLRL
+ subc	r11,DBLRH	! calculate sign / exponent minus implicit 1 minus T
+ mov.l	@r15+,r11
+ mov.l	@r15+,r10
+ shad	DBL1H,DBLRL
+ mov.l	@r15+,r9
+ mov	#-11,DBL1H
+ mov	r12,r8		! z0 contributes to DBLRH and DBLRL
+ shld	DBL1H,r12
+ mov	#21,DBL1H
+ clrt
+ shld	DBL1H,r8
+ addc	r8,DBLRL
+ mov.l	@r15+,r8
+ addc	r12,DBLRH
+ rts
+ mov.l	@r15+,r12
+
+!	sign in DBLRH ^ DBL1H
+! If the last 7 bits are in the range 64..64+7, we might have an exact
+! value in the preceding bits - or we might not. For denorms, we need to
+! find out.
+! if r10:r8 is zero, we just have found out that there is an exact value.
+	.balign	4
+LOCAL(ret_denorm_inf):
+	mov	DBLRH,r3
+	add	r3,r3
+	div0s	DBL1H,r3
+!	mov	#248,DBLRL
+	mov	#120,DBLRL
+	bt	LOCAL(ret_inf_late)
+	add	#64,DBL0L
+	tst	DBLRL,DBL0L
+	mov	#-21,DBLRL
+	bt	LOCAL(find_adjust)
+	or	r10,r8
+!	add	#-64,DBL0L
+	tst	r8,r8		! check if find_adjust found an exact value.
+	shad	DBLRL,r3
+	bf	0f
+	add	#-16,DBL0L	! if yes, cancel adjustment
+0:	mov	#-8,DBLRL	! remove the three lowest (inexact) bits
+	and	DBLRL,DBL0L
+	add	#-2-11,r3	! shift count for denorm generation
+	neg 	DBL0L,DBL0L
+	mov	#-28,r2
+	mov	DBL0L,DBLRL
+	mov.l	@r15+,r11
+	mov.l	@r15+,r10
+	shll2	DBLRL
+	mov.l	@r15+,r9
+	shad	r2,DBL0L
+	mov.l	@r15+,r8
+	mov	#-31,r2
+	cmp/ge	r2,r3
+	shll2	DBLRL
+	bt/s	0f
+	add	DBL0L,r12	! fraction in r12:DBLRL ; u1.63
+	mov	#0,r2
+	cmp/hi r2,DBLRL
+	mov	#-33,r2
+	add	#31,r3
+	mov	r12,DBLRL
+	rotcl	DBLRL		! put in sticky bit
+	movt	r12
+	cmp/ge	r3,r2
+	bt	LOCAL(test1)
+0:	div0s	DBL1H,DBLRH	! calculate sign
+	mov	r12,DBLRH
+	shld	r3,DBLRH
+	mov	DBLRL,r2
+	shld	r3,DBLRL
+	add	#32,r3
+	add	DBLRH,DBLRH
+	mov.l	LOCAL(x80000000),DBL1H
+	shld	r3,r12
+	rotcr	DBLRH		! combine sign with highpart
+	add	#-1,r3
+	shld	r3,r2
+	mov	#0,r3
+	rotl	r2
+	cmp/hi	DBL1H,r2
+	addc	r12,DBLRL
+	mov.l	@r15+,r12
+	rts
+	addc	r3,DBLRH
+
+LOCAL(test1):
+	cmp/ge	r2,r3
+	bf/s	LOCAL(return_0_late)
+	div0s	DBL1H,DBLRH
+	mov #0,DBLRH
+	mov	DBLRL,r2
+	mov #0,DBLRL
+	rotcr	DBLRH		! combine sign with highpart
+	mov	#0,r3
+	rotl	r2
+	cmp/hi	r3,r2
+	addc	r3,DBLRL
+	mov.l	@r15+,r12
+	rts
+	addc	r3,DBLRH
+		  
+		  
+LOCAL(ret_inf_late):
+	mov.l	@r15+,r11
+	mov.l	@r15+,r10
+	mov	DBLRH,DBL0H
+	mov.l	@r15+,r9
+	bra	LOCAL(return_inf)
+	mov.l	@r15+,r8
+
+LOCAL(return_0_late):
+	div0s	DBLRH,DBL1H
+	mov.l	@r15+,r12
+	mov	#0,DBLRH
+	mov	#0,DBLRL
+	rts
+	rotcr	DBLRH
+
+	
+	
+	.balign	4
+LOCAL(clz):
+	mov.l	r8,@-r15
+	extu.w	r0,r8
+	mov.l	r9,@-r15
+	cmp/eq	r0,r8
+	bt/s	0f
+	mov	#21,r9
+	shlr16	r0
+	extu.w	r0,r8
+	add	#-16,r9
+0:	tst	r12,r8	! 0xff00
+	mov.l	LOCAL(c_clz_tab),r0
+	bt	0f
+	shlr8	r8
+0:	bt	0f
+	add	#-8,r9
+0:
+#ifdef	__PIC__
+	add	r0,r8
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r8),r8
+	mov	r9,r0
+	mov.l	@r15+,r9
+	!
+	!
+	!
+	sub	r8,r0
+	mov.l	@r15+,r8
+	rts
+	lds.l	@r15+,pr
+
+!	We encode even some words as pc-relative that would fit as immediate
+!	in the instruction in order to avoid some pipeline stalls on
+!	SH4-100 / SH4-200.
+LOCAL(d1):	.word 1
+LOCAL(d12):	.word 12
+LOCAL(d13):	.word 13
+LOCAL(d120):	.word 120
+
+	.balign 4
+LOCAL(x7ff00000): .long 0x7ff00000
+LOCAL(xfffe2006): .long 0xfffe2006
+LOCAL(x001fffff): .long 0x001fffff
+LOCAL(x40000000): .long 0x40000000
+LOCAL(x7fe00000): .long 0x7fe00000
+LOCAL(x00100000): .long 0x00100000
+LOCAL(x00200000): .long 0x00200000
+LOCAL(xfff00000): .long 0xfff00000
+LOCAL(x80000000): .long 0x80000000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+LOCAL(ytab):
+        .byte   120, 105,  91,  78,  66,  54,  43,  33
+        .byte    24,  15,   8,   0,  -5, -12, -17, -22
+        .byte   -27, -31, -34, -37, -40, -42, -44, -45
+        .byte   -46, -46, -47, -46, -46, -45, -44, -42
+        .byte   -41, -39, -36, -34, -31, -28, -24, -20
+        .byte   -17, -12,  -8,  -4,   0,   5,  10,  16
+        .byte    21,  27,  33,  39,  45,  52,  58,  65
+        .byte    72,  79,  86,  93, 101, 109, 116, 124
+ENDFUNC(GLOBAL(divdf3))
Index: gcc/config/sh/IEEE-754/m3/floatunssisf.S
===================================================================
--- gcc/config/sh/IEEE-754/m3/floatunssisf.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/m3/floatunssisf.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,94 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! floatsisf for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+FUNC(GLOBAL(floatunsisf))
+	.global GLOBAL(floatunsisf)
+	.balign	4
+GLOBAL(floatunsisf):
+	mov.l	LOCAL(c_clz_tab),r0
+	extu.w	r4,r1
+	mov.w	LOCAL(xff00),r3
+	cmp/eq	r4,r1
+	mov	#24,r2
+	bt	0f
+	mov	r4,r1
+	shlr16	r1
+	add	#-16,r2
+0:	tst	r3,r1	! 0xff00
+	bt	0f
+	shlr8	r1
+0:	bt	0f
+	add	#-8,r2
+0:
+#ifdef	__PIC__
+	add	r0,r1
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r1),r1
+	mov	r4,r0
+	mov.l	LOCAL(x4a800000),r3	! bias + 23 - implicit 1
+	tst	r4,r4
+	bt	LOCAL(ret0)
+	!
+	sub	r1,r2
+	mov.l	LOCAL(x80000000),r1
+	shld	r2,r0
+	cmp/pz	r2
+	add	r3,r0
+	bt	LOCAL(noround)
+	add	#31,r2
+	shld	r2,r4
+	rotl	r4
+	add	#-31,r2
+	cmp/hi	r1,r4
+	mov	#0,r3
+	addc	r3,r0
+LOCAL(noround):
+	mov	#23,r1
+	shld	r1,r2
+	rts
+	sub	r2,r0
+LOCAL(ret0):
+	rts
+	nop
+
+LOCAL(xff00):	.word 0xff00
+	.balign	4
+LOCAL(x4a800000): .long 0x4a800000
+LOCAL(x80000000): .long 0x80000000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(floatunsisf))
Index: gcc/config/sh/IEEE-754/m3/floatunssidf.S
===================================================================
--- gcc/config/sh/IEEE-754/m3/floatunssidf.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/m3/floatunssidf.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,96 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! floatunssidf for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+FUNC(GLOBAL(floatunsidf))
+	.global GLOBAL(floatunsidf)
+	.balign	4
+GLOBAL(floatunsidf):
+	mov.l	LOCAL(c_clz_tab),r0
+	extu.w	r4,r1
+	mov.w	LOCAL(0xff00),r3
+	cmp/eq	r4,r1
+	mov	#21,r2
+	bt	0f
+	mov	r4,r1
+	shlr16	r1
+	add	#-16,r2
+0:	tst	r3,r1	! 0xff00
+	bt	0f
+	shlr8	r1
+0:	bt	0f
+	add	#-8,r2
+0:
+#ifdef	__PIC__
+	add	r0,r1
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r1),r5
+	mov	r4,DBLRL
+	mov.l	LOCAL(x41200000),r3	! bias + 20 - implicit 1
+	tst	r4,r4
+	mov	r4,DBLRH
+	bt	LOCAL(ret0)
+	sub	r5,r2
+	mov	r2,r5
+	shld	r2,DBLRH
+	cmp/pz	r2
+	add	r3,DBLRH
+	add	#32,r2
+	shld	r2,DBLRL
+	bf	0f
+	mov.w	LOCAL(d0),DBLRL
+0:	mov	#20,r2
+	shld	r2,r5
+	rts
+	sub	r5,DBLRH
+LOCAL(ret0):
+	mov	r4,DBLRL
+	rts
+	mov	r4,DBLRH
+
+LOCAL(0xff00):	.word  0xff00
+	.balign	4
+LOCAL(x41200000):
+#ifdef __LITTLE_ENDIAN__
+LOCAL(d0):	  .word 0
+		  .word 0x4120
+#else
+		  .word 0x4120
+LOCAL(d0):	  .word 0
+#endif
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(floatunsidf))
Index: gcc/config/sh/IEEE-754/m3/divdf3-rt.S
===================================================================
--- gcc/config/sh/IEEE-754/m3/divdf3-rt.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/m3/divdf3-rt.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,519 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! divdf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke joern.rennecke@st.com
+
+/* This version is not quite finshed, since I've found that I can
+   get better average performance with a slightly altered algorithm.
+   Still, if you want a version for hard real time, this version here might
+   be a good starting point, since it has effectively no conditional
+   branches in the path that deals with normal numbers
+   (branches with zero offset are effectively conditional execution),
+   and thus it has a uniform execution time in this path.  */
+
+/* y = 1/x  ; x (- [1,2)
+   y0 = 1.5 - x/2 - tab[(1-x)*64] = y + d ; abs(d)/y <= 0x1.0c/256
+
+   y1 = y0 - ((y0) * x - 1) * y0  =  y-x*d^2
+   y2 = y1 - ((y1) * x - 1) * y1 =~= y-x^3*d^4
+
+   z0 = y2*a ;  a1 = a - z0*x /# 32 * 64 -> 64 bit #/
+   z1 = y2*a1 (round to nearest odd 0.5 ulp);
+   a2 = a1 - z1*x /# 32 * 64 -> 64 bit #/
+
+   z = a/x = z0 + z1 - 0.5 ulp + (a2 > 0) * ulp
+
+   Unless stated otherwise, multiplies can be done in 32 * 32 bit or less
+   with suitable scaling and/or top truncation.
+   x truncated to 20 bits is sufficient to calculate y0 or even y1.
+   Table entries are adjusted by about +128 to use full signed byte range.
+   This adjustment has been perturbed slightly to allow cse with the
+   shift count constant -26.
+   The threshold point for the shift adjust before rounding is found by
+   comparing the fractions, which is exact, unlike the top bit of y2.
+   Therefore, the top bit of y2 becomes slightly random after the adjustment
+   shift, but that's OK because this can happen only at the boundaries of
+   the interval, and the baising of the error means that it can in fact happen
+   only at the bottom end.  And there, the carry propagation will make sure
+   that in the end we will have in effect an implicit 1 (or two whem rounding
+   up...)  */
+/* If an exact result exists, it can have no more bits than the divident.
+   Hence, we don't need to bother with the round-to-even tie breaker
+   unless the result is denormalized.  */
+/* 70 cycles through main path for sh4-300 .  Some cycles might be
+   saved by more careful register allocation.
+   122 cycles for sh4-200.  If execution time for sh4-200 is of concern,
+   a specially scheduled version makes sense.  */
+
+#define x_h r12
+#define yn  r3
+
+FUNC(GLOBAL(divdf3))
+ .global GLOBAL(divdf3)
+
+/* Adjust arg0 now, too.  We still have to come back to denorm_arg1_done,
+   since we heven't done any of the work yet that we do till the denorm_arg0
+   entry point.  We know that neither of the arguments is inf/nan, but
+   arg0 might be zero.  Check for that first to avoid having to establish an
+   rts return address.  */
+LOCAL(both_denorm):
+	mov.l	r9,@-r15
+	mov	DBL0H,r1
+	mov.l	r0,@-r15
+	shll2	r1
+	mov.w LOCAL(both_denorm_cleanup_off),r9
+	or	DBL0L,r1
+	tst	r1,r1
+	mov	DBL0H,r0
+	bf/s	LOCAL(zero_denorm_arg0_1)
+	shll2	r0
+	mov.l	@(4,r15),r9
+	add	#8,r15
+	bra	LOCAL(ret_inf_nan_0)
+	mov	r1,DBLRH
+
+LOCAL(both_denorm_cleanup):
+	mov.l	@r15+,r0
+	!
+	mov.l	@r15+,r9
+ mov	#12,r3
+ mov.l	LOCAL(xfffe2006),r1	! yn := (-1. << 17) + (0x80 << 6) ; shift #-26
+	bra	LOCAL(denorm_arg1_done)
+	!
+	add	r0,DBL0H
+
+/* Denorm handling leaves the incoming denorm argument with an exponent of +1
+   (implicit 1).  To leave the result exponent unaltered, the other
+   argument's exponent is adjusted by the the shift count.  */
+
+	.balign 4
+LOCAL(arg0_tiny):
+	bsr	LOCAL(clz)
+	mov	DBL0L,r0
+	shll	DBL0H
+	add	#1,r0
+	mov	DBL0L,DBL0H
+	shld	r0,DBL0H
+	rotcr	DBL0H
+	tst	DBL0L,DBL0L	/* Check for divide of zero.  */
+	add	#-33,r0
+	shld	r0,DBL0L
+	bf/s	LOCAL(adjust_arg1_exp)
+	add	#64,r0
+LOCAL(return_0): /* Return 0 with appropriate sign.  */
+	mov.l	@r15+,r10
+	mov	#0,DBLRH
+	mov.l	@r15+,r9
+	bra	LOCAL(ret_inf_nan_0)
+	mov.l	@r15+,r8
+
+	.balign 4
+LOCAL(arg1_tiny):
+	bsr	LOCAL(clz)
+	mov	DBL1L,r0
+	shll	DBL1H
+	add	#1,r0
+	mov	DBL1L,DBL1H
+	shld	r0,DBL1H
+	rotcr	DBL1H
+	tst	DBL1L,DBL1L	/* Check for divide by zero.  */
+	add	#-33,r0
+	shld	r0,DBL1L
+	bf/s	LOCAL(adjust_arg0_exp)
+	add	#64,r0
+	mov	DBL0H,r0
+	add	r0,r0
+	tst	r0,r0	! 0 / 0 ?
+	mov	#-1,DBLRH
+	bf	LOCAL(return_inf)
+	!
+	bt	LOCAL(ret_inf_nan_0)
+	!
+
+	.balign 4
+LOCAL(zero_denorm_arg1):
+	not	DBL0H,r3
+	mov	DBL1H,r0
+	tst	r2,r3
+	shll2	r0
+	bt	LOCAL(early_inf_nan_arg0)
+	tst	r0,r0
+	mov.w	LOCAL(xff00),r12
+	bt/s	LOCAL(arg1_tiny)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	shlr2	r0
+	!
+	shll	DBL1H
+	mov	DBL1L,r3
+	shld	r0,DBL1H
+	shld	r0,DBL1L
+	rotcr	DBL1H
+	add	#-32,r0
+	shld	r0,r3
+	add	#32,r0
+	or	r3,DBL1H
+LOCAL(adjust_arg0_exp):
+	tst	r2,DBL0H
+	mov	#20,r3
+	shld	r3,r0
+	bt	LOCAL(both_denorm)
+	add	DBL0H,r0
+	div0s	r0,DBL0H	! Check for obvious overflow.  */
+	not	r0,r3		! Check for more subtle overflow - lest
+	bt	LOCAL(return_inf)
+	mov	r0,DBL0H
+	tst	r2,r3		! we mistake it for NaN later
+	mov	#12,r3
+	bf	LOCAL(denorm_arg1_done)
+LOCAL(return_inf): /* Return infinity with appropriate sign.  */
+	mov	#20,r3
+	mov	#-2,DBLRH
+	bra	LOCAL(ret_inf_nan_0)
+	shad	r3,DBLRH
+
+/* inf/n -> inf; inf/0 -> inf; inf/inf -> nan; inf/nan->nan  nan/x -> nan */
+LOCAL(inf_nan_arg0):
+	mov.l	@r15+,r10
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+LOCAL(early_inf_nan_arg0):
+	not	DBL1H,r3
+	mov	DBL0H,DBLRH
+	tst	r2,r3	! both inf/nan?
+	add	DBLRH,DBLRH
+	bf	LOCAL(ret_inf_nan_0)
+	mov	#-1,DBLRH
+LOCAL(ret_inf_nan_0):
+	mov	#0,DBLRL
+	mov.l	@r15+,r12
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	
+/* Already handled: inf/x, nan/x .  Thus: x/inf -> 0; x/nan -> nan */
+	.balign	4
+LOCAL(inf_nan_arg1):
+	mov	DBL1H,r2
+	mov	#12,r1
+	shld	r1,r2
+	mov.l	@r15+,r10
+	mov	#0,DBLRL
+	mov.l	@r15+,r9
+	or	DBL1L,r2
+	mov.l	@r15+,r8
+	cmp/hi	DBLRL,r2
+	mov.l	@r15+,r12
+	subc	DBLRH,DBLRH
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	
+	.balign 4
+LOCAL(zero_denorm_arg0):
+	mov.w	LOCAL(denorm_arg0_done_off),r9
+	not	DBL1H,r1
+	mov	DBL0H,r0
+	tst	r2,r1
+	shll2	r0
+	bt	LOCAL(inf_nan_arg1)
+LOCAL(zero_denorm_arg0_1):
+	tst	r0,r0
+	mov.w	LOCAL(xff00),r12
+	bt/s	LOCAL(arg0_tiny)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	shlr2	r0
+	shll	DBL0H
+	mov	DBL0L,r12
+	shld	r0,DBL0H
+	shld	r0,DBL0L
+	rotcr	DBL0H
+	add	#-32,r0
+	shld	r0,r12
+	add	#32,r0
+	or	r12,DBL0H
+LOCAL(adjust_arg1_exp):
+	mov	#20,r12
+	shld	r12,r0
+	add	DBL1H,r0
+	div0s	r0,DBL1H	! Check for obvious underflow.  */
+	not	r0,r12		! Check for more subtle underflow - lest
+	bt	LOCAL(return_0)
+	mov	r0,DBL1H
+	tst	r2,r12		! we mistake it for NaN later
+	bt	LOCAL(return_0)
+	!
+	braf	r9
+	mov	#13,r0
+LOCAL(zero_denorm_arg1_dispatch):
+
+LOCAL(xff00):	.word 0xff00
+LOCAL(denorm_arg0_done_off):
+	.word LOCAL(denorm_arg0_done)-LOCAL(zero_denorm_arg1_dispatch)
+LOCAL(both_denorm_cleanup_off):
+	.word LOCAL(both_denorm_cleanup)-LOCAL(zero_denorm_arg1_dispatch)
+
+ .balign	8
+GLOBAL(divdf3):
+ mov.l	LOCAL(x7ff00000),r2
+ mov	#12,r3
+ mov.l	LOCAL(xfffe2006),r1	! yn := (-1. << 17) + (0x80 << 6) ; shift #-26
+ tst	r2,DBL1H
+ mov.l	r12,@-r15
+ bt	LOCAL(zero_denorm_arg1)
+
+LOCAL(denorm_arg1_done):
+ mov	DBL1H,x_h	! x_h live in r12
+ shld	r3,x_h	! x - 1 ; u0.20
+ mov	x_h,yn
+ mova	LOCAL(ytab),r0
+ mov.l	r8,@-r15
+ shld	r1,yn	! x-1 ; u26.6
+ mov.b	@(r0,yn),yn
+ mov	#6,r0
+ mov.l	r9,@-r15
+ mov	x_h,r8
+ mov.l	r10,@-r15
+ shlr16	x_h	! x - 1; u16.16	! x/2 - 0.5 ; u15.17
+ add	x_h,r1	! SH4-200 single-issues this insn
+ shld	r0,yn
+ sub	r1,yn	! yn := y0 ; u15.17
+ mov	DBL1L,r1
+ mov	#-20,r10
+ mul.l	yn,x_h	! r12 dead
+ swap.w	yn,r9
+ shld	r10,r1
+ sts	macl,r0	! y0 * (x-1) - n ; u-1.32
+ add	r9,r0	! y0 * x - 1     ; s-1.32
+ tst	r2,DBL0H
+ dmuls.l r0,yn
+ mov.w	LOCAL(d13),r0
+ or	r1,r8	! x  - 1; u0.32
+ add	yn,yn	! yn = y0 ; u14.18
+ bt	LOCAL(zero_denorm_arg0)
+
+LOCAL(denorm_arg0_done):	! This label must stay aligned.
+ sts	mach,r1	!      d0 ; s14.18
+ sub	r1,yn	! yn = y1 ; u14.18 ; <= 0x3fffc
+ mov	DBL0L,r12
+ shld	r0,yn	! yn = y1 ; u1.31 ; <= 0x7fff8000
+ mov.w	LOCAL(d12),r9
+ dmulu.l yn,r8
+ shld	r10,r12
+ mov	yn,r0
+ mov	DBL0H,r8
+ add	yn,yn	! yn = y1 ; u0.32 ; <= 0xffff0000
+ sts	mach,r1	! y1 * (x-1); u1.31
+ add	r0,r1	! y1 * x    ; u1.31
+ dmulu.l yn,r1
+ not	DBL0H,r10
+ shld	r9,r8
+ tst	r2,r10
+ or	r8,r12	! a - 1; u0.32
+ bt	LOCAL(inf_nan_arg0)
+ sts	mach,r1	! d1+yn; u1.31
+ sett		! adjust y2 so that it can be interpreted as s1.31
+ not	DBL1H,r10
+ subc	r1,yn	! yn := y2 ; u1.31 ; can be 0x7fffffff
+ mov.l	LOCAL(x001fffff),r9
+ dmulu.l yn,r12
+ tst	r2,r10
+ or	DBL1H,r2
+ bt	LOCAL(inf_nan_arg1)
+ mov.l	r11,@-r15
+ sts	mach,r11	! y2*(a-1) ; u1.31
+ add	yn,r11		! z0       ; u1.31
+ dmulu.l r11,DBL1L
+ mov.l	LOCAL(x40000000),DBLRH	! bias + 1
+ and	r9,r2		! x ; u12.20
+ cmp/hi	DBL0L,DBL1L
+ sts	macl,r8
+ mov	#-24,r12
+ sts	mach,r9 	! r9:r8 := z0 * DBL1L; u-19.64
+ subc	DBL1H,DBLRH
+ mul.l	r11,r2  	! (r9+macl):r8 == z0*x; u-19.64
+ shll	r8
+ add	DBL0H,DBLRH	! result sign/exponent + 1
+ mov	r8,r10
+ sts	macl,DBLRL
+ add	DBLRL,r9
+ rotcl	r9		! r9:r8 := z*x; u-20.63
+ shld	r12,r10
+ mov.l	LOCAL(x7fe00000),DBLRL
+ sub	DBL0L,r9	! r9:r8 := -a ; u-20.63
+ mov.l	LOCAL(x00200000),r12
+FIXME: the following  shift might loose the sign.
+ shll8	r9
+ or	r10,r9	! -a1 ; s-28.32
+ mov.l	LOCAL(x00100000),r10
+ dmuls.l r9,yn	! r3 dead
+ mov	DBL1H,r3
+ mov.l LOCAL(xfff00000),DBL0L
+ xor	DBL0H,r3	! calculate expected sign & bit20
+ div0s	r3,DBLRH
+ xor	DBLRH,r3
+ bt	LOCAL(ret_denorm_inf)
+ tst	DBLRL,DBLRH
+ bt	LOCAL(ret_denorm)
+ sub	r12,DBLRH ! calculate sign / exponent minus implicit 1
+ tst	r10,r3	! set T if a >= x
+ sts	mach,r12! -z1 ; s-27.32
+ bt	0f
+ add	r11,r11	! z0 ; u1.31 / u0.31
+0: mov	#6,r3
+ negc	r3,r10 ! shift count := a >= x ? -7 : -6; T := 1
+ shll8	r8	! r9:r8 := -a1 ; s-28.64
+ shad	r10,r12	! -z1 ; truncate to s-20.32 / s-21.32
+ rotcl	r12	! -z1 ; s-21.32 / s-22.32 / round to odd 0.5 ulp ; T := sign
+ add	#20,r10
+ dmulu.l r12,DBL1L ! r12 signed, DBL1L unsigned
+ and	DBL0L,DBLRH	! isolate sign / exponent
+ shld	r10,r9
+ mov	r8,r3
+ shld	r10,r8
+ sts	macl,DBL0L
+ sts	mach,DBLRL
+ add	#-32,r10
+ shld	r10,r3
+ mul.l r12,r2
+ bf	0f	! adjustment for signed/unsigned multiply
+ sub	DBL1L,DBLRL	! DBL1L dead
+0: shar	r12	! -z1 ; truncate to s-20.32 / s-21.32
+ sts	macl,DBL1L
+ or	r3,r9	! r9:r8 := -a1 ;             s-41.64/s-42.64
+ !
+ cmp/hi	r8,DBL0L
+ add	DBLRL,DBL1L ! DBL1L:DBL0L := -z1*x ; s-41.64/s-42.64
+ subc	DBL1L,r9
+ not	r12,DBLRL ! z1, truncated to s-20.32 / s-21.32
+ shll	r9	! T :=  a2 > 0
+ mov	r11,r2
+ mov	#21,r7
+ shld	r7,r11
+ addc	r11,DBLRL
+ mov.l	@r15+,r11
+ mov.l	@r15+,r10
+ mov	#-11,r7
+ mov.l	@r15+,r9
+ shld	r7,r2
+ mov.l	@r15+,r8
+ addc	r2,DBLRH
+ rts
+ mov.l	@r15+,r12
+
+LOCAL(ret_denorm):
+	tst	r10,DBLRH
+	bra	LOCAL(denorm_have_count)
+	movt	DBLRH	! calculate shift count (off by 2)
+
+LOCAL(ret_denorm_inf):
+	mov	DBLRH,r12
+	add	r12,r12
+	cmp/pz	r12
+	mov	#-21,DBLRL
+	bt	LOCAL(ret_inf_late)
+	shld	DBLRL,DBLRH
+LOCAL(denorm_have_count):
+	add	#-2,DBLRH
+/* FIXME */
+	bra	LOCAL(return_0)
+	mov.l	@r15+,r11
+
+LOCAL(ret_inf_late):
+	mov.l	@r15+,r11
+	!
+	mov.l	@r15+,r10
+	!
+	mov.l	@r15+,r9
+	bra	LOCAL(return_inf)
+	mov.l	@r15+,r8
+
+	.balign	4
+LOCAL(clz):
+	mov.l	r8,@-r15
+	extu.w	r0,r8
+	mov.l	r9,@-r15
+	cmp/eq	r0,r8
+	bt/s	0f
+	mov	#8-11,r9
+	xtrct	r0,r8
+	add	#16,r9
+0:	tst	r12,r8	! 0xff00
+	mov.l	LOCAL(c_clz_tab),r0
+	bt	0f
+	shlr8	r8
+0:	bt	0f
+	add	#8,r9
+0:
+#ifdef	__PIC__
+	add	r0,r8
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r8),r8
+	mov	r9,r0
+	mov.l	@r15+,r9
+	!
+	!
+	!
+	sub	r8,r0
+	mov.l	@r15+,r8
+	rts
+	lds.l	@r15+,pr
+
+!	We encode even some words as pc-relative that would fit as immediate
+!	in the instruction in order to avoid some pipeline stalls on
+!	SH4-100 / SH4-200.
+LOCAL(d1):	.word 1
+LOCAL(d12):	.word 12
+LOCAL(d13):	.word 13
+
+	.balign 4
+LOCAL(x7ff00000): .long 0x7ff00000
+LOCAL(xfffe2006): .long 0xfffe2006
+LOCAL(x001fffff): .long 0x001fffff
+LOCAL(x40000000): .long 0x40000000
+LOCAL(x7fe00000): .long 0x7fe00000
+LOCAL(x00100000): .long 0x00100000
+LOCAL(x00200000): .long 0x00200000
+LOCAL(xfff00000): .long 0xfff00000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+LOCAL(ytab):
+        .byte   120, 105,  91,  78,  66,  54,  43,  33
+        .byte    24,  15,   8,   0,  -5, -12, -17, -22
+        .byte   -27, -31, -34, -37, -40, -42, -44, -45
+        .byte   -46, -46, -47, -46, -46, -45, -44, -42
+        .byte   -41, -39, -36, -34, -31, -28, -24, -20
+        .byte   -17, -12,  -8,  -4,   0,   5,  10,  16
+        .byte    21,  27,  33,  39,  45,  52,  58,  65
+        .byte    72,  79,  86,  93, 101, 109, 116, 124
+ENDFUNC(GLOBAL(divdf3))
Index: gcc/config/sh/IEEE-754/m3/fixunsdfsi.S
===================================================================
--- gcc/config/sh/IEEE-754/m3/fixunsdfsi.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/m3/fixunsdfsi.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,81 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!! fixunsdfsi for Renesas SH / STMicroelectronics ST40 CPUs
+!! Contributed by J"orn Rennecke joern.rennecke@st.com
+
+	! What is a bit unusal about this implementation is that the
+	! sign bit influences the result for NANs: for cleared sign bit, you
+	! get INT_MAX, for set sign bit, you get INT_MIN.
+	! However, since the result for NANs is undefined, this should be no
+	! problem.
+	! N.B. This is scheduled both for SH4-200 and SH4-300
+	.balign 4
+	.global GLOBAL(fixunsdfsi)
+	FUNC(GLOBAL(fixunsdfsi))
+	.balign	4
+GLOBAL(fixunsdfsi):
+	mov.w	LOCAL(x413),r1	! bias + 20
+	mov	DBL0H,r0
+	shll	DBL0H
+	mov.l	LOCAL(mask),r3
+	mov	#-21,r2
+	shld	r2,DBL0H	! SH4-200 will start this insn in a new cycle
+	bt/s	LOCAL(ret0)
+	sub	r1,DBL0H
+	cmp/pl	DBL0H		! SH4-200 will start this insn in a new cycle
+	and	r3,r0
+	bf/s	LOCAL(ignore_low)
+	addc	r3,r0	! uses T == 1; sets implict 1
+	mov	#11,r2
+	shld	DBL0H,r0	! SH4-200 will start this insn in a new cycle
+	cmp/gt	r2,DBL0H
+	add	#-32,DBL0H
+	bt	LOCAL(retmax)
+	shld	DBL0H,DBL0L
+	rts
+	or	DBL0L,r0
+
+	.balign	8
+LOCAL(ignore_low):
+	mov	#-21,r2
+	cmp/gt	DBL0H,r2	! SH4-200 will start this insn in a new cycle
+	add	#1,r0
+	bf	0f
+LOCAL(ret0): mov #0,r0		! results in 0 return
+0:	rts
+	shld	DBL0H,r0
+
+LOCAL(retmax):
+	rts
+	mov	#-1,r0
+
+LOCAL(x413): .word 0x413
+
+	.balign 4
+LOCAL(mask): .long 0x000fffff
+	ENDFUNC(GLOBAL(fixunsdfsi))
+
Index: gcc/config/sh/IEEE-754/m3/addsf3.S
===================================================================
--- gcc/config/sh/IEEE-754/m3/addsf3.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/m3/addsf3.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,290 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! addsf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+
+	.balign 4
+	.global GLOBAL(subsf3)
+	FUNC(GLOBAL(subsf3))
+	.global GLOBAL(addsf3)
+	FUNC(GLOBAL(addsf3))
+GLOBAL(subsf3):
+	cmp/pz	r5
+	add	r5,r5
+	rotcr	r5
+	.balign 4
+GLOBAL(addsf3):
+	mov.l	LOCAL(x7f800000),r3
+	mov	r4,r6
+	add	r6,r6
+	mov	r5,r7
+	add	r7,r7
+	mov	r4,r0
+	or	r3,r0
+	cmp/hi	r6,r7
+	mov	r5,r1
+	bf/s	LOCAL(r4_hs)
+	 or	r3,r1
+	cmp/eq	r5,r1
+	bt	LOCAL(ret_r5) /* sole Inf or NaN, return unchanged.  */
+	shll8	r0	! r4 fraction
+	shll8	r1	! r5 fraction
+	mov	r6,r3
+	mov	#-24,r2
+	mov	r7,r6
+	shld	r2,r6	! r5 exp
+	mov	r0,r7
+	shld	r2,r3	! r4 exp
+	tst	r3,r3
+	sub	r6,r3	! exp difference (negative or 0)
+	bt	LOCAL(denorm_r4)
+LOCAL(denorm_r4_done): ! r1: u1.31
+	shld	r3,r0	! Get 31 upper bits, including 8 guard bits
+	mov.l	LOCAL(xff000000),r2
+	add	#31,r3
+	mov.l	r5,@-r15 ! push result sign.
+	cmp/pl	r3	! r0 has no more than one bit set -> return arg 1
+	shld	r3,r7	! copy of lowest guard bit in r0 and lower guard bits
+	bf	LOCAL(ret_stack)
+	div0s	r4,r5
+	bf/s	LOCAL(add)
+	 cmp/pl	r7	/* Is LSB in r0 clear, but any lower guards bit set?  */
+	subc	r0,r1
+	mov.l	LOCAL(c__clz_tab),r7
+	tst	r2,r1
+	mov	#-24,r3
+	bf/s LOCAL(norm_r0)
+	 mov	r1,r0
+	extu.w	r1,r1
+	bra	LOCAL(norm_check2)
+	 cmp/eq	r0,r1
+LOCAL(ret_r5):
+	rts
+	mov	r5,r0
+LOCAL(ret_stack):
+	rts
+	mov.l	@r15+,r0
+
+/* We leave the numbers denormalized, but we change the bit position to be
+   consistent with normalized numbers.  This also removes the spurious
+   leading one that was inserted before.  */
+LOCAL(denorm_r4):
+	tst	r6,r6
+	add	r0,r0
+	bf	LOCAL(denorm_r4_done)
+	bra	LOCAL(denorm_r4_done)
+	add	r1,r1
+LOCAL(denorm_r5):
+	tst	r6,r6
+	add	r1,r1
+	bf	LOCAL(denorm_r5_done)
+	clrt
+	bra	LOCAL(denorm_r5_done)
+	add	r0,r0
+
+/* If the exponent differs by two or more, normalization is minimal, and
+   few guard bits are needed for an exact final result, so sticky guard
+   bit compresion before subtraction (or add) works fine.
+   If the exponent differs by one, only one extra guard bit is generated,
+   and effectively no guard bit compression takes place.  */
+
+	.balign	4
+LOCAL(r4_hs):
+	cmp/eq	r4,r0
+	mov	#-24,r3
+	bt	LOCAL(inf_nan_arg0)
+	shld	r3,r7
+	shll8	r0
+	tst	r7,r7
+	shll8	r1
+	mov.l	LOCAL(xff000000),r2
+	bt/s	LOCAL(denorm_r5)
+	shld	r3,r6
+LOCAL(denorm_r5_done):
+	mov	r1,r3
+	subc	r6,r7
+	bf	LOCAL(same_exp)
+	shld	r7,r1	/* Get 31 upper bits.  */
+	add	#31,r7
+	mov.l	r4,@-r15 ! push result sign.
+	cmp/pl	r7
+	shld	r7,r3
+	bf	LOCAL(ret_stack)
+	div0s	r4,r5
+	bf/s	LOCAL(add)
+	 cmp/pl	r3	/* Is LSB in r1 clear, but any lower guard bit set?  */
+	subc	r1,r0
+	mov.l	LOCAL(c__clz_tab),r7
+LOCAL(norm_check):
+	tst	r2,r0
+	mov	#-24,r3
+	bf LOCAL(norm_r0)
+	extu.w	r0,r1
+	cmp/eq	r0,r1
+LOCAL(norm_check2):
+	mov	#-8,r3
+	bt LOCAL(norm_r0)
+	mov	#-16,r3
+LOCAL(norm_r0):
+	mov	r0,r1
+	shld	r3,r0
+#ifdef __pic__
+	add	r0,r7
+	mova  LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r7),r7
+	add	#25,r3
+	add	#-9+1,r6
+	mov	r1,r0
+	sub	r7,r3
+	mov.l	LOCAL(xbfffffff),r7
+	sub	r3,r6	/* generate exp-1  */
+	mov.w	LOCAL(d24),r2
+	cmp/pz	r6	/* check exp > 0  */
+	shld	r3,r0	/* Leading 1 becomes +1 exp adjustment.  */
+	bf	LOCAL(zero_denorm)
+LOCAL(denorm_done):
+	add	#30,r3
+	shld	r3,r1
+	mov.w   LOCAL(m1),r3
+	tst	r7,r1	! clear T if rounding up
+	shld	r2,r6
+	subc	r3,r0	! round - overflow will boost exp adjustment to 2.
+	mov.l	@r15+,r2
+	add	r6,r0	! overflow will generate inf
+	cmp/ge	r2,r3	! get sign into T
+	rts
+	rotcr	r0
+LOCAL(ret_r4):
+	rts
+	mov	r4,r0
+
+/* At worst, we are shifting the number back in place where an incoming
+   denormal was.  Thus, the shifts won't get out of range.  They still
+   might generate a zero fraction, but that's OK, that makes it 0.  */
+LOCAL(zero_denorm):
+	add	r6,r3
+	mov	r1,r0
+	mov	#0,r6	/* leading one will become free (except for rounding) */
+	bra	LOCAL(denorm_done)
+	shld	r3,r0
+
+/* Handle abs(r4) >= abs(r5), same exponents specially so we don't need
+   check for a zero fraction in the main path.  */
+LOCAL(same_exp):
+	div0s	r4,r5
+	mov.l	r4,@-r15
+	bf	LOCAL(add)
+	cmp/eq	r1,r0
+	mov.l	LOCAL(c__clz_tab),r7
+	bf/s	LOCAL(norm_check)
+	 sub	r1,r0
+	rts	! zero difference -> return +zero
+	mov.l	@r15+,r1
+
+/* r2: 0xff000000 */
+LOCAL(add):
+	addc	r1,r0
+	mov.w	LOCAL(x2ff),r7
+	shll8	r6
+	bf/s	LOCAL(no_carry)
+	shll16	r6
+	tst	r7,r0		
+	shlr8	r0
+	mov.l	@r15+,r3	! discard saved sign
+	subc	r2,r0
+	sett
+	addc	r6,r0
+	cmp/hs	r2,r0
+	bt/s	LOCAL(inf)
+	div0s	r7,r4 /* Copy sign.  */
+	rts
+	rotcr	r0
+LOCAL(inf):
+	mov	r2,r0
+	rts
+	rotcr	r0
+	
+LOCAL(no_carry):
+	mov.w	LOCAL(m1),r3
+	tst	r6,r6
+	bt	LOCAL(denorm_add)
+	add	r0,r0
+	tst	r7,r0		! check if lower guard bit set or round to even
+	shlr8	r0
+	mov.l	@r15+,r1	! discard saved sign
+	subc	r3,r0	! round ; overflow -> exp++
+	cmp/ge	r4,r3	/* Copy sign.  */
+	add	r6,r0	! overflow -> inf
+	rts
+	rotcr	r0
+
+LOCAL(denorm_add):
+	cmp/ge	r4,r3	/* Copy sign.  */
+	shlr8	r0
+	mov.l	@r15+,r1	! discard saved sign
+	rts
+	rotcr	r0
+
+LOCAL(inf_nan_arg0):
+	cmp/eq	r5,r1
+	bf	LOCAL(ret_r4)
+	div0s	r4,r5		/* Both are inf or NaN, check signs.  */
+	bt	LOCAL(ret_nan)	/* inf - inf, or NaN.  */
+	mov	r4,r0		! same sign; return NaN if either is NaN.
+	rts
+	or	r5,r0
+LOCAL(ret_nan):
+	rts
+	mov	#-1,r0
+
+LOCAL(d24):
+	.word	24
+LOCAL(x2ff):
+	.word	0x2ff
+LOCAL(m1):
+	.word	-1
+	.balign	4
+LOCAL(x7f800000):
+	.long	0x7f800000
+LOCAL(xbfffffff):
+	.long	0xbfffffff
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(xfe000000):
+	.long	0xfe000000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+
+	ENDFUNC(GLOBAL(addsf3))
+	ENDFUNC(GLOBAL(subsf3))
+
Index: gcc/config/sh/IEEE-754/m3/adddf3.S
===================================================================
--- gcc/config/sh/IEEE-754/m3/adddf3.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/m3/adddf3.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,614 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! adddf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4-200 without FPU, but can also be used for SH3.
+! Numbers with same sign are added in typically 37 cycles, worst case is
+! 43 cycles, unless there is an overflow, in which case the addition can
+! take up to takes 47 cycles.
+! Normal numbers with different sign are added in 56 (57 for PIC) cycles
+! or less on SH4.
+! If one of the inputs is a denormal, the worst case is 59 (60 for PIC)
+! cycles. (Two denormal inputs are faster than normal inputs, and
+! denormal outputs don't slow down computation).
+! Subtraction takes two cycles to negate the second input and then drops
+! through to addition.
+
+/* If the input exponents of a difference of two normalized numbers
+   differ by more than one, the output does not need to be adjusted
+   by more than one bit position.  Hence, it makes sense to ensure that
+   the shifts by 0 & 1 are handled quickly to reduce average and worst
+   case times.  */
+FUNC(GLOBAL(adddf3))
+FUNC(GLOBAL(subdf3))
+	.global	GLOBAL(adddf3)
+	.global	GLOBAL(subdf3)
+LOCAL(denorm_arg1):
+	bt LOCAL(inf_nan_arg0)
+	tst	r0,r2
+	bt/s	LOCAL(denorm_both)
+	shlr	r1
+	mov.l	LOCAL(x00100000),r3
+	bra	LOCAL(denorm_arg1_done)
+	 sub	r2,r3
+
+! Handle denorm addition here because otherwise the ordinary addition would
+! have to check for denormal results.
+! Denormal subtraction could also be done faster, but the denorm subtraction
+! path here is still one cycles faster than the one for normalized input
+! numbers, and 16 instructions shorter than the fastest version.
+! Here we also generate +0.0 + +0.0 -> +0.0 ; -0.0 + -0.0 -> -0.0
+LOCAL(denorm_both):
+	div0s	r8,DBL1H
+	mov.l	LOCAL(x800fffff),r9
+	bt/s	LOCAL(denorm_sub)
+	and	r1,DBL1H
+	and	r9,r8
+	mov.l	@r15+,r9
+	mov	DBL0L,DBLRL
+	mov	r8,DBLRH
+	addc	DBL1L,DBLRL
+	mov.l	@r15+,r8
+	rts
+	 addc	DBL1H,DBLRH
+
+! N.B., since subtraction also generates +0.0 for subtraction of numbers
+! with identical fractions, this also covers the +0.0 + -0.0 -> +0.0 /
+! -0.0 + +0.0 -> +0.0 cases.
+LOCAL(denorm_sub):
+	mov	r8,DBL0H	! tentative result sign
+	and	r1,DBL0H
+	bra	LOCAL(sub_same_exp)
+	 addc	r1,r2	! exponent++, clear T
+
+LOCAL(inf_nan_arg0):
+	cmp/hs 	r0,r3
+	bf		LOCAL(inf_nan_ret)
+	tst	DBL1L,DBL1L
+	bf		LOCAL(ret_nan)
+	shlr	r1
+	and 	DBL1H,r1
+	tst	r1,r1
+	bf		LOCAL(ret_nan)
+	div0s	r8,DBL1H
+	bf		LOCAL(inf_nan_ret)
+LOCAL(ret_nan):
+	mov 	#-1,DBLRH
+	bra	LOCAL(pop_r8_r9)
+		mov DBLRH,DBLRL
+  
+LOCAL(inf_nan_ret):
+	mov	DBL0L,DBLRL
+	bra	LOCAL(pop_r8_r9)
+		mov r8,DBLRH
+
+LOCAL(ret_arg0):
+	mov.l LOCAL(x800fffff),DBLRH
+	mov	DBL0L,DBLRL
+	mov	r2,r3
+LOCAL(ret_arg):
+	mov.l	@r15+,r9
+	and	r8,DBLRH
+	mov.l	@r15+,r8
+	rts
+	or r3,DBLRH
+
+LOCAL(no_carry):
+	shlr	r0
+	mov.l	LOCAL(x000fffff),DBLRH
+	addc	r3,r9
+	mov.w	LOCAL(d0),DBL1H
+	mov	DBL0L,DBLRL
+	and	DBL0H,DBLRH	! mask out implicit 1
+	mov.l	LOCAL(x7ff00000),r3
+	addc	DBL1H,DBLRL
+	addc	r2,DBLRH
+	mov.l	@r15+,r9
+	add	DBL1H,DBLRH	! fraction overflow -> exp increase
+	bra	LOCAL(add_done)
+	 cmp/hi	r3,DBLRH
+
+LOCAL(inf):
+	mov	#0,DBLRL
+	bra	LOCAL(or_sign)
+	mov	r3,DBLRH
+
+	.balign	4
+GLOBAL(subdf3):
+	cmp/pz DBL1H
+	add 	DBL1H,DBL1H
+	rotcr	DBL1H
+	nop
+
+GLOBAL(adddf3):
+	mov.l	LOCAL(x7ff00000),r0
+	mov	DBL0H,r2
+	mov.l	LOCAL(x001fffff),r1
+	mov	DBL1H,r3
+	mov.l	r8,@-r15
+	and	r0,r2		! r2 <- exp0
+	mov.l	r9,@-r15
+	and	r0,r3		! r3 <- exp1
+	cmp/hi r2,r3
+	or		r0,DBL0H
+	or		r0,DBL1H
+	bt		LOCAL(arg1_gt)
+	tst	r0,r3
+	mov	#-20,r9
+	mov	DBL0H,r8	! tentative result sign
+	and	r1,DBL0H	! arg0 fraction
+	bt/s	LOCAL(denorm_arg1)
+		cmp/hs r0,r2
+	bt		LOCAL(inf_nan_arg0)
+	sub	r2,r3
+LOCAL(denorm_arg1_done):	! r2 is tentative result exponent
+	shad	r9,r3
+	mov.w	LOCAL(m32),r9
+	mov	DBL1H,r0	! the 'other' sign
+	and	r1,DBL1H	! arg1 fraction
+	cmp/ge r9,r3
+	mov	DBL1H,r1
+	bf/s	LOCAL(large_shift_arg1)
+	 shld	r3,DBL1H
+LOCAL(small_shift_arg1):
+	mov	DBL1L,r9
+	shld	r3,DBL1L
+	tst	r3,r3
+	add	#32,r3
+	bt/s	LOCAL(same_exp)
+	 div0s r8,r0	! compare signs
+	shld	r3,r1
+
+	or		r1,DBL1L
+	bf/s	LOCAL(add)
+	shld	r3,r9
+	clrt
+	negc	r9,r9
+	mov.l	LOCAL(x001f0000),r3
+LOCAL(sub_high):
+	mov	DBL0L,DBLRL
+	subc	DBL1L,DBLRL
+	mov	DBL0H,DBLRH
+	bra	LOCAL(subtract_done)
+	 subc	DBL1H,DBLRH
+
+LOCAL(large_shift_arg1):
+	mov	DBL1L,r9
+	shld	r3,r9
+	add	#64,r3
+	cmp/pl r3
+	shld	r3,r1
+	shld	r3,DBL1L
+	bf		LOCAL(ret_arg0)
+	tst	DBL1L,DBL1L
+	bt		LOCAL(large_shift_arg1_end)
+	mov	#1,DBL1L
+	or 	DBL1L,r9
+LOCAL(large_shift_arg1_end):
+	mov	DBL1H,DBL1L
+	mov	#0,DBL1H
+	add	r1,r9
+	div0s	r8,r0	! compare signs
+	bf		LOCAL(add)
+	clrt
+	mov.l	LOCAL(x001f0000),r3
+	bra	LOCAL(sub_high)
+	 negc	r9,r9
+
+LOCAL(add_clr_r9):
+	mov	#0,r9
+LOCAL(add):
+	mov.l	LOCAL(x00200000),r3
+	addc	DBL1L,DBL0L
+	addc	DBL1H,DBL0H
+	mov.l	LOCAL(x80000000),r1
+	tst	r3,DBL0H
+	mov.l	LOCAL(x7fffffff),r3
+	mov	DBL0L,r0
+	bt/s	LOCAL(no_carry)
+	and	r1,r8
+	tst	r9,r9
+	bf		LOCAL(add_one)
+	tst	#2,r0
+LOCAL(add_one):
+	subc	r9,r9
+	sett
+	mov	r0,DBLRL 
+	addc	r9,DBLRL
+	mov	DBL0H,DBLRH
+	addc	r9,DBLRH
+	shlr	DBLRH
+	mov.l	LOCAL(x7ff00000),r3
+	add	r2,DBLRH
+	mov.l	@r15+,r9
+	rotcr	DBLRL
+	cmp/hs r3,DBLRH
+LOCAL(add_done):
+	bt		LOCAL(inf)
+LOCAL(or_sign):
+	or		r8,DBLRH
+	rts
+	 mov.l @r15+,r8
+
+LOCAL(same_exp):
+	bf	LOCAL(add_clr_r9)
+	clrt
+LOCAL(sub_same_exp):
+	subc	DBL1L,DBL0L
+	mov.l	LOCAL(x001f0000),r3
+	subc	DBL1H,DBL0H
+	mov.w	LOCAL(d0),r9
+	bf	LOCAL(pos_difference_0)
+	clrt
+	negc	DBL0L,DBLRL
+	mov.l	LOCAL(x80000000),DBL0L
+	negc	DBL0H,DBLRH
+	mov.l	LOCAL(x00100000),DBL0H
+	tst	r3,DBLRH
+	not	r8,r8
+	bt/s	LOCAL(long_norm)
+	and	DBL0L,r8
+	bra	LOCAL(norm_loop_2)
+	 not	DBL0L,r3
+
+LOCAL(large_shift_arg0):
+	mov	DBL0L,r9
+	shld	r2,r9
+	add	#64,r2
+	cmp/pl	r2
+	shld	r2,r1
+	shld	r2,DBL0L
+	bf	LOCAL(ret_arg1_exp_r3)
+	tst	DBL0L,DBL0L
+	bt LOCAL(large_shift_arg0_end)
+	mov 	#1,DBL0L
+	or		DBL0L,r9 
+LOCAL(large_shift_arg0_end):
+	mov	DBL0H,DBL0L
+	mov	#0,DBL0H
+	add	r1,r9
+	div0s	r8,r0	! compare signs
+	mov	r3,r2	! tentative result exponent
+	bf	LOCAL(add)
+	clrt
+	negc	r9,r9
+	bra	LOCAL(subtract_arg0_arg1_done)
+	 mov	DBL1L,DBLRL
+
+LOCAL(arg1_gt):
+	tst	r0,r2			! r0 = 0x7ff00000 r2 = exp0
+	mov	#-20,r9
+	mov	DBL1H,r8		! tentative result sign
+	and	r1,DBL1H
+	bt/s	LOCAL(denorm_arg0)
+	cmp/hs	r0,r3
+	bt	LOCAL(inf_nan_arg1)
+	sub	r3,r2
+LOCAL(denorm_arg0_done):
+	shad	r9,r2			! r2 <- shifting value
+	mov.w	LOCAL(m32),r9
+	mov	DBL0H,r0		! the 'other' sign
+	and	r1,DBL0H
+	cmp/ge	r9,r2
+	mov	DBL0H,r1
+	bf/s	LOCAL(large_shift_arg0)
+		shld	r2,DBL0H
+LOCAL(small_shift_arg0):
+	mov	DBL0L,r9
+	shld	r2,DBL0L
+	mov.l	r3,@-r15
+	mov 	#32,r3
+	add	r3,r2
+	cmp/ge	r3,r2
+	bf LOCAL(shifting)
+	mov	#0,r1
+	mov 	r1,r9
+LOCAL(shifting):
+	shld	r2,r1
+	mov	r2,r3
+	shld	r3,r9
+	div0s	r8,r0		! compare signs
+	mov.l	@r15+,r2	! tentative result exponent
+	bf/s	LOCAL(add)
+	or	r1,DBL0L
+	clrt
+	negc	r9,r9
+	mov	DBL1L,DBLRL
+LOCAL(subtract_arg0_arg1_done):
+	subc	DBL0L,DBLRL
+	mov	DBL1H,DBLRH
+	mov.l	LOCAL(x001f0000),r3
+	subc	DBL0H,DBLRH
+/* Since the exponents were different, the difference is positive.  */
+/* Fall through */
+LOCAL(subtract_done):
+/* First check if a shift by a few bits is sufficient.  This not only
+   speeds up this case, but also alleviates the need for considering
+   lower bits from r9 or rounding in the other code.
+   Moreover, by handling the upper 1+4 bits of the fraction here, long_norm
+   can assume that DBLRH fits into 20 (20 < 16) bit.  */
+	tst	r3,DBLRH
+	mov.l	LOCAL(x80000000),r3
+	mov.l	LOCAL(x00100000),DBL0H
+	bt/s	LOCAL(long_norm)
+	and	r3,r8
+	mov.l	LOCAL(x7fffffff),r3
+LOCAL(norm_loop_2):	! Well, this used to be a loop...
+	tst	DBL0H,DBLRH
+	sub	DBL0H,r2
+	bf	LOCAL(norm_round)
+	shll	r9
+	rotcl	DBLRL
+	
+	rotcl	DBLRH
+	
+	 subc	DBL0H,r2
+LOCAL(norm_loop_1):
+	bt	LOCAL(denorm0_n)
+	tst	DBL0H,DBLRH
+	bf	LOCAL(norm_round)
+	shll	DBLRL
+	rotcl	DBLRH	! clears T
+	bra	LOCAL(norm_loop_1)
+	 subc	DBL0H,r2
+	 
+LOCAL(denorm_arg0):
+	bt	LOCAL(inf_nan_arg1)
+	mov.l	LOCAL(x00100000),r2
+	shlr	r1				! r1 <- 0xfffff
+	bra	LOCAL(denorm_arg0_done)
+	 sub	r3,r2			! r2 <- 1 - exp1
+
+LOCAL(inf_nan_arg1):
+	mov	DBL1L,DBLRL
+	bra	LOCAL(pop_r8_r9)
+	 mov	r8,DBLRH
+
+LOCAL(ret_arg1_exp_r3):
+	mov.l	LOCAL(x800fffff),DBLRH
+	bra	LOCAL(ret_arg)
+	 mov	DBL1L,DBLRL
+
+LOCAL(pos_difference_0):
+	tst	r3,DBL0H
+	mov	DBL0L,DBLRL
+	mov.l	LOCAL(x80000000),DBL0L
+	mov	DBL0H,DBLRH
+	mov.l	LOCAL(x00100000),DBL0H
+	bt/s	LOCAL(long_norm)
+	and	DBL0L,r8
+	bra	LOCAL(norm_loop_2)
+	 not	DBL0L,r3
+
+#ifdef __pic__
+	.balign 8
+#endif
+LOCAL(m32):
+	.word	-32
+LOCAL(d0):
+	.word	0
+#ifndef __pic__
+	.balign 8
+#endif
+! Because we had several bits of cancellations, we know that r9 contains
+! only one bit.
+! We'll normalize by shifting words so that DBLRH:DBLRL contains
+! the fraction with 0 < DBLRH <= 0x1fffff, then we shift DBLRH:DBLRL
+! up by 21 minus the number of non-zero bits in DBLRH.
+LOCAL(long_norm):
+	tst	DBLRH,DBLRH
+	mov.w	LOCAL(xff),DBL0L
+	mov	#21,r3
+	bf	LOCAL(long_norm_highset)
+	mov.l	LOCAL(x02100000),DBL1L	! shift 32, implicit 1
+	tst	DBLRL,DBLRL
+	extu.w	DBLRL,DBL0H
+	bt	LOCAL(zero_or_ulp)
+	mov	DBLRL,DBLRH
+	cmp/hi	DBL0H,DBLRL
+	bf	0f
+	mov.l	LOCAL(x01100000),DBL1L	! shift 16, implicit 1
+	clrt
+	shlr16  DBLRH
+	xtrct	DBLRL,r9
+	mov     DBLRH,DBL0H
+LOCAL(long_norm_ulp_done):
+0:	mov	r9,DBLRL	! DBLRH:DBLRL == fraction; DBL0H == DBLRH
+	subc	DBL1L,r2
+	bt	LOCAL(denorm1_b)
+#ifdef __pic__
+	mov.l	LOCAL(c__clz_tab),DBL1H
+LOCAL(long_norm_lookup):
+	mov	r0,r9
+	mova	LOCAL(c__clz_tab),r0
+	add	DBL1H,r0
+#else
+	mov	r0,r9
+LOCAL(long_norm_lookup):
+	mov.l	LOCAL(c__clz_tab),r0
+#endif /* __pic__ */
+	cmp/hi	DBL0L,DBL0H
+	bf	0f
+	shlr8	DBL0H
+0:	mov.b	@(r0,DBL0H),r0
+	bf	0f
+	add	#-8,r3
+0:	mov.w	LOCAL(d20),DBL0L
+	mov	#-20,DBL0H
+	clrt
+	sub	r0,r3
+	mov	r9,r0
+	mov	r3,DBL1H
+	shld	DBL0L,DBL1H
+	subc	DBL1H,r2
+	!
+	bf	LOCAL(no_denorm)
+	shad	DBL0H,r2
+	bra	LOCAL(denorm1_done)
+	add	r2,r3
+	
+LOCAL(norm_round):
+	cmp/pz	r2
+	mov	#0,DBL1H
+	bf	LOCAL(denorm0_1)
+	or	r8,r2
+	mov	DBLRL,DBL1L
+	shlr	DBL1L
+	addc	r3,r9
+	mov.l	@r15+,r9
+	addc	DBL1H,DBLRL	! round to even
+	mov.l	@r15+,r8
+	rts
+	 addc	r2,DBLRH
+
+LOCAL(norm_pack):
+	add	r8,DBLRH
+	mov.l	@r15+,r8
+	rts
+	add	r2,DBLRH
+
+LOCAL(denorm0_1):
+	mov.l	@r15+,r9
+	mov	r8,DBL0L
+	mov.l	@r15+,r8
+LOCAL(denorm0_shift):
+	shlr	DBLRH
+	rotcr	DBLRL
+
+	rts
+	add	DBL0L,DBLRH
+
+LOCAL(denorm0_n):
+	mov.l	@r15+,r9
+	mov	r8,DBL0L
+	addc	DBL0H,r2
+	mov.l	@r15+,r8
+	bra LOCAL(denorm0_shift)
+		  nop
+
+LOCAL(no_denorm):
+	add	r2,r8		! add (exponent - 1) to sign
+
+LOCAL(denorm1_done):
+	shld	r3,DBLRH
+	mov	DBLRL,DBL0L
+	shld	r3,DBLRL
+
+	add	r8,DBLRH	! add in sign and (exponent - 1)
+	mov.l	@r15+,r9
+	add	#-32,r3
+	mov.l	@r15+,r8
+	shld	r3,DBL0L
+
+	rts
+	add	DBL0L,DBLRH
+
+LOCAL(long_norm_highset):
+	mov.l	LOCAL(x00200000),DBL1L	! shift 1, implicit 1
+	shll	r9
+	rotcl	DBLRL
+	mov	DBLRH,DBL0H
+	rotcl	DBLRH	! clears T
+#ifdef __pic__
+	mov.l	LOCAL(c__clz_tab),DBL1H
+#else
+	mov	r0,r9
+#endif /* __pic__ */
+	subc	DBL1L,r2
+	add	#-1,r3
+	bf	LOCAL(long_norm_lookup)
+LOCAL(denorm1_a):
+	shlr	DBLRH
+	rotcr	DBLRL
+	mov.l	@r15+,r9
+	or	r8,DBLRH
+
+	rts
+	mov.l	@r15+,r8
+
+	.balign	4
+LOCAL(denorm1_b):
+	mov	#-20,DBL0L
+	shad	DBL0L,r2
+	mov	DBLRH,DBL0L
+	shld	r2,DBLRH
+	shld	r2,DBLRL
+	or	r8,DBLRH
+	mov.l	@r15+,r9
+	add	#32,r2
+	mov.l	@r15+,r8
+	shld	r2,DBL0L
+	rts
+	or	DBL0L,DBLRL
+
+LOCAL(zero_or_ulp):
+	tst	r9,r9
+	bf	LOCAL(long_norm_ulp_done)
+	! return +0.0
+LOCAL(pop_r8_r9):
+	mov.l	@r15+,r9
+	rts
+	mov.l	@r15+,r8
+
+LOCAL(d20):
+	.word	20
+LOCAL(xff):
+	.word 0xff
+	.balign	4
+LOCAL(x7ff00000):
+	.long	0x7ff00000
+LOCAL(x001fffff):
+	.long	0x001fffff
+LOCAL(x80000000):
+	.long	0x80000000
+LOCAL(x000fffff):
+	.long	0x000fffff
+LOCAL(x800fffff):
+	.long	0x800fffff
+LOCAL(x001f0000):
+	.long	0x001f0000
+LOCAL(x00200000):
+	.long	0x00200000
+LOCAL(x7fffffff):
+	.long	0x7fffffff
+LOCAL(x00100000):
+	.long	0x00100000
+LOCAL(x02100000):
+	.long	0x02100000
+LOCAL(x01100000):
+	.long	0x01100000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(adddf3))
+ENDFUNC(GLOBAL(subdf3))
+
Index: gcc/config/sh/IEEE-754/m3/mulsf3.S
===================================================================
--- gcc/config/sh/IEEE-754/m3/mulsf3.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/m3/mulsf3.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,269 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! mulsf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+
+	.balign 4
+	.global GLOBAL(mulsf3)
+	FUNC(GLOBAL(mulsf3))
+GLOBAL(mulsf3):
+   mov.l   LOCAL(x7f800000),r1
+   not     r4,r2
+	mov		r4,r3
+	not		r5,r0
+	tst		r1,r2
+	or			r1,r3
+	bt/s		LOCAL(inf_nan_arg0)
+	 tst		r1,r0
+	bt			LOCAL(inf_nan_arg1)
+	tst		r1,r5
+	mov		r1,r2
+	shll8		r3
+	or			r5,r1
+	bt/s		LOCAL(zero_denorm_arg1)
+	 shll8		r1
+	tst		r2,r4
+	bt			LOCAL(zero_denorm_arg0)
+	dmulu.l	r3,r1
+	mov		r4,r0
+	and		r2,r0
+LOCAL(arg_norm):
+	and		r5,r2
+	mov.l 	LOCAL(x3f800000),r3
+	sts		mach,r1
+	sub		r3,r0
+	sts		macl,r3
+	add		r2,r0
+	cmp/pz	r1
+	mov.w 	LOCAL(x100),r2
+	bf/s		LOCAL(norm_frac) 
+	 tst		r3,r3
+	shll2		r1	 ! Shift one up, replace leading 1 with 0.  
+	shlr		r1
+	tst		r3,r3
+LOCAL(norm_frac):
+	mov.w 	LOCAL(mx80),r3
+	bf			LOCAL(round_frac)
+	tst		r2,r1
+LOCAL(round_frac):
+	mov.l 	LOCAL(xff000000),r2
+	subc		r3,r1	! Even overflow gives right result: exp++, frac=0. 
+	shlr8 	r1
+	add		r1,r0
+	shll		r0
+	bt			LOCAL(ill_exp)
+	tst		r2,r0
+	bt			LOCAL(denorm)
+	cmp/hs	r2,r0
+	bt			LOCAL(inf)
+LOCAL(insert_sign):
+	div0s	r4,r5
+	rts
+	rotcr		r0
+LOCAL(denorm0):
+	tst	r1,r1
+	mov.w 	LOCAL(x100),r2
+	bf			LOCAL(round_den0)
+	tst		r2,r0
+LOCAL(round_den0):
+	mov 		#-7,r2
+	mov.w 	LOCAL(mx80),r3
+	subc 		r3,r0
+	bra		LOCAL(insert_sign)
+	 shld 		r2,r0
+LOCAL(zero_denorm_arg1):
+	mov.l 	LOCAL(x60000000),r2	/* Check exp0 >= -64	*/
+	add		r1,r1
+	tst		r1,r1	/* arg1 == 0 ? */
+	mov		#0,r0
+	bt			LOCAL(insert_sign) /* argument 1 is zero ==> return 0  */
+	tst		r4,r2
+	bt			LOCAL(insert_sign) /* exp0 < -64  ==> return 0 */
+	mov.l 	LOCAL(c__clz_tab),r0
+	mov		r3,r2
+	mov		r1,r3
+	bra		LOCAL(arg_normalize)
+	mov		r2,r1
+LOCAL(zero_denorm_arg0):
+	mov.l 	LOCAL(x60000000),r2	/* Check exp1 >= -64	*/
+	add		r3,r3
+	tst		r3,r3	/* arg0 == 0 ? */
+	mov		#0,r0
+	bt			LOCAL(insert_sign) /* argument 0 is zero ==> return 0  */
+	tst		r5,r2
+	bt			LOCAL(insert_sign) /* exp1 < -64  ==> return 0 */
+	mov.l 	LOCAL(c__clz_tab),r0
+LOCAL(arg_normalize):
+	mov.l	r7,@-r15
+	extu.w	r3,r7
+	cmp/eq	r3,r7
+	mov.l 	LOCAL(xff000000),r7
+	mov		#-8,r2
+	bt			0f
+	tst		r7,r3
+	mov		#-16,r2
+	bt			0f
+	mov		#-24,r2
+0:
+	mov		r3,r7
+	shld		r2,r7
+#ifdef __pic__
+	add		r0,r7
+	mova  	LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r7),r0
+	add		#32,r2
+	mov		r2,r7
+	mov		#23,r2
+	sub		r0,r7
+	mov.l	LOCAL(x7f800000),r0
+	shld		r7,r3
+	shld		r2,r7
+	mov		r0,r2
+	and		r4,r0
+	sub		r7,r0
+	mov.l	@r15+,r7
+	bra		LOCAL(arg_norm)
+	 dmulu.l	r3,r1
+#if 0 /* This is slightly slower, but could be used if table lookup causes
+         cache thrashing.  */
+	bt			LOCAL(insert_sign) /* exp1 < -64  ==> return 0 */
+	mov.l 	LOCAL(xff000000),r2
+	mov		r4,r0
+LOCAL(arg_normalize):
+	tst		r2,r3
+	bf			LOCAL(arg_bit_norm)
+LOCAL(arg_byte_loop):
+	tst		r2,r3
+	add		r2,r0
+	shll8		r3
+	bt			LOCAL(arg_byte_loop)
+	add		r4,r0
+LOCAL(arg_bit_norm):
+	mov.l 	LOCAL(x7f800000),r2
+	rotl		r3
+LOCAL(arg_bit_loop):
+	add		r2,r0
+	bf/s		LOCAL(arg_bit_loop)
+	 rotl		r3
+	rotr		r3
+	rotr		r3
+	sub		r2,r0
+	bra		LOCAL(arg_norm)
+	 dmulu.l	r3,r1
+#endif /* 0 */
+LOCAL(inf):
+	bra		LOCAL(insert_sign)
+	 mov		r2,r0
+LOCAL(inf_nan_arg0):
+	bt			LOCAL(inf_nan_both)
+	add		r0,r0
+!	cmp/eq	#-1,r0	Here : modif -1 replace by -2 
+	cmp/eq	#-2,r0	/* arg1 zero? -> NAN */
+	bt			LOCAL(insert_sign)
+	mov		r4,r0
+LOCAL(inf_insert_sign):
+	bra		LOCAL(insert_sign)
+	 add		r0,r0
+LOCAL(inf_nan_both):
+	mov		r4,r0
+	bra		LOCAL(inf_insert_sign)
+	 or		r5,r0
+LOCAL(inf_nan_arg1):
+	mov		r2,r0
+	add		r0,r0
+! cmp/eq	#-1,r0	Here : modif -1 replace by -2 
+	cmp/eq	#-2,r0	/* arg0 zero? */
+	bt			LOCAL(insert_sign)
+	bra		LOCAL(inf_insert_sign)
+	 mov		r5,r0
+LOCAL(ill_exp):
+	cmp/pz	r0
+	bt			LOCAL(inf)
+LOCAL(denorm):
+	mov		#-24,r3
+	add		r1,r1
+	mov		r0,r2
+	sub		r1,r2	! remove fraction to get back pre-rounding exponent.
+	tst 		r2,r2
+	sts		mach,r0
+	sts		macl,r1
+	bt			LOCAL(denorm0)
+	shad		r3,r2
+	mov		r0,r3
+	shld		r2,r0
+	add		#32,r2
+	cmp/pz	r2
+	shld		r2,r3
+	bf			LOCAL(zero)
+	or			r1,r3
+	mov		#-1,r1
+	tst		r3,r3
+	mov.w	LOCAL(x100),r3
+	bf/s		LOCAL(denorm_round_up)
+	mov		#-0x80,r1
+	tst		r3,r0
+LOCAL(denorm_round_up):
+	mov		#-7,r3
+	subc		r1,r0
+	bra		LOCAL(insert_sign)
+	 shld		r3,r0
+LOCAL(zero):
+	bra		LOCAL(insert_sign)
+	 mov 	#0,r0
+LOCAL(x100):
+	.word	0x100
+LOCAL(x200):
+	.word	0x200
+LOCAL(x17f):
+	.word	0x17f
+LOCAL(x80):
+	.word	0x80
+LOCAL(mx80):
+	.word	-0x80
+	.balign	4
+LOCAL(mx100):
+	.word	-0x100
+	.balign	4
+LOCAL(x7f800000):
+	.long 0x7f800000
+LOCAL(x3f800000):
+	.long 0x3f800000
+LOCAL(x1000000):
+	.long	0x1000000
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x60000000):
+	.long	0x60000000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+	ENDFUNC(GLOBAL(mulsf3))
Index: gcc/config/sh/IEEE-754/m3/floatsisf.S
===================================================================
--- gcc/config/sh/IEEE-754/m3/floatsisf.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/m3/floatsisf.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,106 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! floatsisf for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+FUNC(GLOBAL(floatsisf))
+	.global GLOBAL(floatsisf)
+	.balign	4
+GLOBAL(floatsisf):
+	cmp/pz	r4
+	mov	r4,r5
+	bt	0f
+	neg	r4,r5
+0:	mov.l	LOCAL(c_clz_tab),r0
+	extu.w	r5,r1
+	mov.w	LOCAL(xff00),r3
+	cmp/eq	r5,r1
+	mov	#24,r2
+	bt	0f
+	mov	r5,r1
+	shlr16	r1
+	add	#-16,r2
+0:	tst	r3,r1	! 0xff00
+	bt	0f
+	shlr8	r1
+0:	bt	0f
+	add	#-8,r2
+0:
+#ifdef	__PIC__
+	add	r0,r1
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r1),r1
+	cmp/pz	r4
+	mov.l	LOCAL(x4a800000),r3	! bias + 23 - implicit 1
+	bt	0f
+	mov.l	LOCAL(xca800000),r3	! sign + bias + 23 - implicit 1
+0:	mov	r5,r0
+	sub	r1,r2
+	mov.l	LOCAL(x80000000),r1
+	shld	r2,r0
+	cmp/pz	r2
+	add	r3,r0
+	bt	LOCAL(noround)
+	add	#31,r2
+	shld	r2,r5
+	add	#-31,r2
+	rotl	r5
+	cmp/hi	r1,r5
+	mov	#0,r3
+	addc	r3,r0
+	mov	#23,r1
+	shld	r1,r2
+	rts
+	sub	r2,r0
+	.balign	8
+LOCAL(noround):
+	mov	#23,r1
+	tst	r4,r4
+	shld	r1,r2
+	bt	LOCAL(ret0)
+	rts
+	sub	r2,r0
+LOCAL(ret0):
+	rts
+	mov	#0,r0
+
+LOCAL(xff00):	.word 0xff00
+	.balign	4
+LOCAL(x4a800000): .long 0x4a800000
+LOCAL(xca800000): .long 0xca800000
+LOCAL(x80000000): .long 0x80000000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(floatsisf))
Index: gcc/config/sh/IEEE-754/m3/muldf3.S
===================================================================
--- gcc/config/sh/IEEE-754/m3/muldf3.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/m3/muldf3.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,502 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! muldf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+! Normal numbers are multiplied in 53 or 54 cycles on SH4-200.
+
+FUNC(GLOBAL(muldf3))
+	.global GLOBAL(muldf3)
+LOCAL(normalize_arg53):
+	tst	r2,DBL0H
+	mov	#1,r2
+	bt	LOCAL(normalize_arg48)
+	mov	DBL0H,r1
+	shlr16	r1
+	bra	LOCAL(normalize_DBL0H)
+	mov	#21-16,r3
+
+LOCAL(normalize_arg16):
+	mov.w	LOCAL(m31),r2 ! 1-32
+	mov	#0,DBL0L
+LOCAL(normalize_arg48):
+	mov	DBL0H,r1
+	mov	#21,r3
+LOCAL(normalize_DBL0H):
+	extu.b	r1,r8
+	mov.l	LOCAL(c__clz_tab),r0
+	cmp/eq	r8,r1
+	!
+	bt	0f
+	shlr8	r1
+0:
+#ifdef	__pic__
+	add	r0,r1
+
+	mova	LOCAL(c__clz_tab),r0
+
+#endif /* __pic__ */
+	mov.b	@(r0,r1),r8
+	mov	DBL0L,r1
+	mov.l	@r15+,r0
+	bt	0f
+	add	#-8,r3
+0:	clrt
+	sub	r8,r3
+	mov.w	LOCAL(d20),r8
+	shld	r3,DBL0H 	! Normalization
+	shld	r3,DBL0L
+	sub	r3,r2 	! r2 <- shifting number
+	add	#-32,r3
+	shld	r3,r1
+	or	r1,DBL0H
+	shld	r8,r2 	! positioning r2 for exp
+	mov.l	@r15+,r8
+
+	! Here :  test tinyness 
+	mov 	DBL1H, r1
+	neg 	r2,r3
+	shll 	r1
+	shll 	r3
+	cmp/hi r1,r3
+	bt LOCAL(zero)
+	
+	add	r2,DBL1H
+	mov.l	LOCAL(x001fffff),r2
+	mov.l LOCAL(x00100000),r3
+	dmulu.l	DBL0L,DBL1L
+	bra	LOCAL(arg_denorm_done)
+	or	r3,r0		! set implicit 1 bit
+
+LOCAL(inf_nan_denorm_or_zero_a):
+	mov.l r8,@-r15
+	sub 	r3,DBL0H 	! isolate high fraction (r3 = 0xfff00000)
+	mov.l @(4,r15),r8 ! original DBL0H (with sign & exp)
+	mov.l r0,@-r15
+	mov.l	r10,@-r15
+	mov 	r1,r10
+	sub 	r3,r1 	! r1 <- 0x7ff00000
+	mov.l LOCAL(x60000000),r3
+	shll16 	r2 	! r2 <- 0xffff0000
+	!			  no stall here for sh4-200
+	!
+	tst 	r1,r8 	! test DBL0 Inf or NaN ?
+	bf LOCAL(inf_nan_a)
+	tst r10,r0 	! test for DBL1 inf, nan or small
+	mov.l	@r15+,r10
+	bt LOCAL(ret_inf_nan_zero)
+LOCAL(normalize_arg):
+	tst 	DBL0H,DBL0H
+	bf LOCAL(normalize_arg53)
+	tst 	DBL0L,DBL0L 	! test for DBL0 is zero
+	bt LOCAL(a_zero)
+	tst 	r2,DBL0L 	! test DBL0L = 0x0000xxxx
+	mov 	DBL0L,DBL0H ! left shift 32
+	bt LOCAL(normalize_arg16)
+	shlr16 	DBL0H
+	mov.w LOCAL(m15),r2	! 1-16
+	bra 	LOCAL(normalize_arg48)
+	shll16 	DBL0L
+
+LOCAL(a_zero):
+	mov.l	@(4,r15),r8
+	add	#8,r15
+LOCAL(zero):
+	mov	#0,DBLRH
+	bra	LOCAL(pop_ret)
+	mov	#0,DBLRL
+
+! both inf / nan -> result is nan if at least one is none, else inf.
+! BBL0 inf/nan, DBL1 zero   -> result is nan
+! DBL0 inf/nan, DBL1 finite -> result is DBL0 with sign adjustemnt
+LOCAL(inf_nan_a):
+	mov.l	@r15+,r10
+	mov	r8,DBL0H
+	mov.l	@(4,r15),r8
+	tst	r1,r0	! arg1 inf/nan ?
+	mov	DBL0H,DBLRH
+	add	#8,r15
+	mov	DBL0L,DBLRL
+	bt	LOCAL(both_inf_nan)
+	tst	DBL1L,DBL1L
+	mov	DBL1H,r2
+	bf	LOCAL(pop_ret)
+	add	r2,r2
+	tst	r2,r2
+	!
+	bf	LOCAL(pop_ret)
+LOCAL(nan):
+	mov	#-1,DBLRL
+	bra	LOCAL(pop_ret)
+	mov	#-1,DBLRH
+
+LOCAL(both_inf_nan):
+	or	DBL1L,DBLRL
+	bra	LOCAL(pop_ret)
+	or	DBL1H,DBLRH
+
+LOCAL(ret_inf_nan_zero):
+	tst	r1,r0
+	mov.l	@(4,r15),r8
+	or	DBL0L,DBL0H
+	bf/s	LOCAL(zero)
+	add	#8,r15
+	tst	DBL0H,DBL0H
+	bt	LOCAL(nan)
+LOCAL(inf_nan_b):
+	mov	DBL1L,DBLRL
+	mov	DBL1H,DBLRH
+LOCAL(pop_ret):
+	mov.l	@r15+,DBL0H
+	add	DBLRH,DBLRH
+
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+
+	.balign	4
+/* Argument a has already been tested for being zero or denorm.
+   On the other side, we have to swap a and b so that we can share the
+   normalization code.
+   a: sign/exponent : @r15 fraction: DBL0H:DBL0L
+   b: sign/exponent: DBL1H fraction:    r0:DBL1L  */
+LOCAL(inf_nan_denorm_or_zero_b):
+	sub	r3,r1		! 0x7ff00000
+	mov.l	@r15,r2		! get original DBL0H
+	tst	r1,DBL1H
+	sub	r3,r0		! isolate high fraction
+	bf	LOCAL(inf_nan_b)
+	mov.l	DBL1H,@r15
+	mov	r0,DBL0H
+	mov.l	r8,@-r15
+	mov	r2,DBL1H
+	mov.l	LOCAL(0xffff0000),r2
+	mov.l	DBL1H,@-r15
+	mov	DBL1L,r1
+	mov	DBL0L,DBL1L
+	bra	LOCAL(normalize_arg)
+	mov	r1,DBL0L
+
+LOCAL(d20):
+	.word	20
+LOCAL(m15):
+	.word	-15
+LOCAL(m31):
+	.word	-31
+LOCAL(xff):
+	.word	0xff
+
+	.balign	4
+LOCAL(0xffff0000): .long 0xffff0000
+
+	! calculate a (DBL0H:DBL0L) * b (DBL1H:DBL1L)
+	.balign	4
+GLOBAL(muldf3):
+	mov.l	LOCAL(xfff00000),r3
+	mov	DBL1H,r0
+	dmulu.l	DBL0L,DBL1L
+	mov.l	LOCAL(x7fe00000),r1
+	sub	r3,r0
+	mov.l	DBL0H,@-r15
+	sub	r3,DBL0H
+	tst	r1,DBL0H
+	or	r3,DBL0H
+	mov.l	LOCAL(x001fffff),r2
+	bt	LOCAL(inf_nan_denorm_or_zero_a)
+	tst	r1,r0
+	or	r3,r0		! r0:DBL1L    := b fraction ; u12.52
+	bt	LOCAL(inf_nan_denorm_or_zero_b) ! T clear on fall-through
+LOCAL(arg_denorm_done):
+	and	r2,r0		! r0:DBL1L    := b fraction ; u12.52
+	sts	macl,r3
+	sts	mach,r1
+	dmulu.l	DBL0L,r0 ! r0 = DBL1H - exp
+	and	r2,DBL0H	! DBL0H:DBL0L := a fraction ; u12.52
+	mov.l	r8,@-r15
+	mov	#0,DBL0L
+	mov.l	r9,@-r15
+	sts	macl,r2
+	sts	mach,r8
+	dmulu.l	DBL0H,DBL1L
+	addc	r1,r2
+
+	addc	DBL0L,r8	! add T; clears T
+
+	sts	macl,r1
+	sts	mach,DBL1L
+	dmulu.l	DBL0H,r0
+	addc	r1,r2
+	mov.l	LOCAL(x7ff00000),DBL0H
+	addc	DBL1L,r8	! clears T
+	mov.l	@(8,r15),DBL1L	! a sign/exp w/fraction
+	sts	macl,DBLRL
+	sts	mach,DBLRH
+	and	DBL0H,DBL1L	! a exponent
+	mov.w	LOCAL(x200),r9
+	addc	r8,DBLRL
+	mov.l	LOCAL(x3ff00000),r8	! bias
+	addc	DBL0L,DBLRH	! add T
+	cmp/hi	DBL0L,r3	! 32 guard bits -> sticky: T := r3 != 0
+	movt	r3
+	tst	r9,DBLRH	! T := fraction < 2
+	or	r3,r2		! DBLRH:DBLRL:r2 := result fraction; u24.72
+	bt/s	LOCAL(shll12)
+	sub	r8,DBL1L
+	mov.l	LOCAL(x002fffff),r8
+	and	DBL1H,DBL0H	! b exponent
+	mov.l	LOCAL(x00100000),r9
+	add	DBL0H,DBL1L ! result exponent - 1
+	tst	r8,r2
+	mov.w	LOCAL(m20),r8
+	subc	DBL0L,r9
+	addc	r2,r9 ! r2 value is still needed for denormal rounding
+	mov.w	LOCAL(d11),DBL0L
+	rotcr	r9
+	clrt
+	shld	r8,r9
+	mov.w	LOCAL(m21),r8
+	mov	DBLRL,r3
+	shld	DBL0L,DBLRL
+	addc	r9,DBLRL
+	mov.l	@r15+,r9
+	shld	r8,r3
+	mov.l	@r15+,r8
+	shld	DBL0L,DBLRH
+	mov.l	@r15+,DBL0H
+	addc	r3,DBLRH
+	mov.l	LOCAL(x7ff00000),DBL0L
+	add	DBL1L,DBLRH	! implicit 1 adjusts exponent
+	mov.l	LOCAL(xffe00000),r3
+	cmp/hs	DBL0L,DBLRH
+	add	DBLRH,DBLRH
+	bt	LOCAL(ill_exp_11)
+	tst	r3,DBLRH
+	bt	LOCAL(denorm_exp0_11)
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+
+
+LOCAL(shll12):
+	mov.l	LOCAL(x0017ffff),r8
+	extu.b	DBLRH,DBLRH	! remove implicit 1.
+	mov.l	LOCAL(x00080000),r9
+	and	DBL1H,DBL0H	! b exponent
+	add	DBL0H,DBL1L	! result exponent
+	tst	r8,r2		! rounding adjust for lower guard ...
+	mov.w	LOCAL(m19),r8
+	subc	DBL0L,r9	! ... bits and round to even; clear T
+	addc	r2,r9 ! r2 value is still needed for denormal rounding
+	mov.w	LOCAL(d12),DBL0L
+	rotcr	r9
+	clrt
+	shld	r8,r9
+	mov.w	LOCAL(m20),r8
+	mov	DBLRL,r3
+	shld	DBL0L,DBLRL
+	addc	r9,DBLRL
+	mov.l	@r15+,r9
+	shld	r8,r3
+	mov.l	@r15+,r8
+	shld	DBL0L,DBLRH
+	mov.l	LOCAL(x7ff00000),DBL0L
+	addc	r3,DBLRH
+	mov.l	@r15+,DBL0H
+	add	DBL1L,DBLRH
+	mov.l	LOCAL(xffe00000),r3
+	cmp/hs	DBL0L,DBLRH
+	add	DBLRH,DBLRH
+	bt	LOCAL(ill_exp_12)
+	tst	r3,DBLRH
+	bt	LOCAL(denorm_exp0_12)
+LOCAL(insert_sign):
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+
+LOCAL(overflow):
+	mov	r3,DBLRH
+	mov	#0,DBLRL
+	bra	LOCAL(insert_sign)
+	mov.l	@r15+,r8
+
+LOCAL(denorm_exp0_11):
+	mov.l	r8,@-r15
+	mov	#-21,r8
+	mov.l	r9,@-r15
+	bra	LOCAL(denorm)
+	mov	#-2,DBL1L	! one for denormal, and one for sticky bit
+
+LOCAL(ill_exp_11):
+	mov	DBL1H,DBL1L
+	and	r3,DBL0L	! 0x7fe00000
+	add	DBL1L,DBL1L
+	mov.l	r8,@-r15
+	cmp/hi	DBL1L,DBL0L	! check if exp a was large
+	mov	#-20,DBL0L
+	bf	LOCAL(overflow)
+	mov	#-21,r8
+	mov	DBLRH,DBL1L
+	rotcr	DBL1L		! shift in negative sign
+	mov.l	r9,@-r15
+	shad	DBL0L,DBL1L	! exponent ; s32
+	bra	LOCAL(denorm)
+	add	#-2,DBL1L	! add one for denormal, and one for sticky bit
+
+LOCAL(denorm_exp0_12):
+	mov.l	r8,@-r15
+	mov	#-20,r8
+	mov.l	r9,@-r15
+	bra	LOCAL(denorm)
+	mov	#-2,DBL1L	! one for denormal, and one for sticky bit
+
+	.balign 4		! also aligns LOCAL(denorm)
+LOCAL(ill_exp_12):
+	and	r3,DBL0L	! 0x7fe00000
+	mov	DBL1H,DBL1L
+	add	DBL1L,DBL1L
+	mov.l	r8,@-r15
+	cmp/hi	DBL1L,DBL0L	! check if exp a was large
+	bf	LOCAL(overflow)
+	mov	DBLRH,DBL1L
+	rotcr	DBL1L		! shift in negative sign
+	mov	#-20,r8
+	shad	r8,DBL1L	! exponent ; s32
+	mov.l	r9,@-r15
+	add	#-2,DBL1L	! add one for denormal, and one for sticky bit
+LOCAL(denorm):
+	not	r3,r9		! 0x001fffff
+	mov.l	r10,@-r15
+	mov	r2,r10
+	shld	r8,r10	! 11 or 12 lower bit valid
+	and	r9,DBLRH ! Mask away vestiges of exponent.
+	add	#32,r8
+	sub	r3,DBLRH ! Make leading 1 explicit.
+	shld	r8,r2	! r10:r2 := unrounded result lowpart
+	shlr	DBLRH	! compensate for doubling at end of normal code
+	sub	DBLRL,r10	! reconstruct effect of previous rounding
+	exts.b	r10,r9
+	shad	r3,r10	! sign extension
+	mov	#0,r3
+	clrt
+	addc	r9,DBLRL	! Undo previous rounding.
+	bt LOCAL(unround_done)
+	addc	r9,DBLRH
+LOCAL(unround_done):
+	mov.w	LOCAL(m32),r9
+	cmp/hi	r3,r2
+	rotcl	DBLRL	! fit in the rest of r2 as a sticky bit.
+	mov.l	@r15+,r10
+	rotcl	DBLRH
+	cmp/ge	r9,DBL1L
+	bt	LOCAL(small_norm_shift)
+	cmp/hi	r3,DBLRL
+	add	#31,DBL1L
+	movt	DBLRL
+	shll 	DBLRH
+	cmp/ge	r9,DBL1L
+	or	DBLRH,DBLRL
+	bt/s	LOCAL(small_norm_shift)
+	mov	r3,DBLRH
+	mov	r3,DBLRL	! exponent too negative to shift - return zero
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	.balign	4
+LOCAL(small_norm_shift):
+	mov	DBLRL,r2	! stash away guard bits
+	shld	DBL1L,DBLRL
+	mov	DBLRH,DBL0L
+	shld	DBL1L,DBLRH
+	mov.l	LOCAL(x7fffffff),r9
+	add	#32,DBL1L
+	shld	DBL1L,r2
+	shld	DBL1L,DBL0L
+	or	DBL0L,DBLRL
+	or	DBLRL,DBL0L
+	shlr	DBL0L
+	addc	r2,r9
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+	addc	r3,DBLRL
+	addc	r3,DBLRH
+	div0s	DBL0H,DBL1H
+	add	DBLRH,DBLRH
+	rts
+	rotcr	DBLRH
+
+
+LOCAL(x200):
+	.word 0x200
+LOCAL(m19):
+	.word	-19
+LOCAL(m20):
+	.word	-20
+LOCAL(m21):
+	.word	-21
+LOCAL(m32):
+	.word	-32
+LOCAL(d11):
+	.word	11
+LOCAL(d12):
+	.word	12
+	.balign	4
+LOCAL(x60000000):
+	.long	0x60000000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+LOCAL(xfff00000):
+	.long	0xfff00000
+LOCAL(x7fffffff):
+	.long	0x7fffffff
+LOCAL(x00100000):
+	.long	0x00100000
+LOCAL(x7fe00000):
+	.long	0x7fe00000
+LOCAL(x001fffff):
+	.long	0x001fffff
+LOCAL(x7ff00000):
+	.long	0x7ff00000
+LOCAL(x3ff00000):
+	.long	0x3ff00000
+LOCAL(x002fffff):
+	.long	0x002fffff
+LOCAL(xffe00000):
+	.long	0xffe00000
+LOCAL(x0017ffff):
+	.long	0x0017ffff
+LOCAL(x00080000):
+	.long	0x00080000
+ENDFUNC(GLOBAL(muldf3))
Index: gcc/config/sh/IEEE-754/m3/floatsidf.S
===================================================================
--- gcc/config/sh/IEEE-754/m3/floatsidf.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/m3/floatsidf.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,103 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! floatsidf for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+FUNC(GLOBAL(floatsidf))
+	.global GLOBAL(floatsidf)
+	.balign	4
+GLOBAL(floatsidf):
+	tst	r4,r4
+	mov	r4,r1
+	bt	LOCAL(ret0)
+	cmp/pz	r4
+	bt	0f
+	neg	r4,r1
+0:	mov.l	LOCAL(c_clz_tab),r0
+	extu.w	r1,r5
+	mov.w	LOCAL(xff00),r3
+	cmp/eq	r1,r5
+	mov	#21,r2
+	bt	0f
+	mov	r1,r5
+	shlr16	r5
+	add	#-16,r2
+0:	tst	r3,r5	! 0xff00
+	bt	0f
+	shlr8	r5
+0:	bt	0f
+	add	#-8,r2
+0:
+#ifdef	__PIC__
+	add	r0,r5
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r5),r5
+	cmp/pz	r4
+	mov.l	LOCAL(x41200000),r3	! bias + 20 - implicit 1
+	bt	0f
+	mov.l	LOCAL(xc1200000),r3	! sign + bias + 20 - implicit 1
+0:	mov	r1,r0	! DBLRL & DBLRH
+	sub	r5,r2
+	mov	r2,r5
+	shld	r2,DBLRH
+	cmp/pz	r2
+	add	r3,DBLRH
+	add	#32,r2
+	shld	r2,DBLRL
+	bf	0f
+	mov.w	LOCAL(d0),DBLRL
+0:	mov	#20,r2
+	shld	r2,r5
+	rts
+	sub	r5,DBLRH
+LOCAL(ret0):
+	mov	#0,DBLRL
+	rts
+	mov	#0,DBLRH
+
+LOCAL(xff00):	.word 0xff00
+	.balign	4
+LOCAL(x41200000):
+#ifdef __LITTLE_ENDIAN__
+LOCAL(d0):	  .word 0
+		  .word 0x4120
+#else
+		  .word 0x4120
+LOCAL(d0):	  .word 0
+#endif
+LOCAL(xc1200000): .long 0xc1200000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(floatsidf))
Index: gcc/config/sh/IEEE-754/m3/fixdfsi.S
===================================================================
--- gcc/config/sh/IEEE-754/m3/fixdfsi.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/m3/fixdfsi.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,113 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!! fixdfsi for Renesas SH / STMicroelectronics ST40 CPUs
+!! Contributed by J"orn Rennecke joern.rennecke@st.com
+
+	! What is a bit unusal about this implementation is that the
+	! sign bit influences the result for NANs: for cleared sign bit, you
+	! get UINT_MAX, for set sign bit, you get 0.
+	! However, since the result for NANs is undefined, this should be no
+	! problem.
+	! N.B. This is scheduled both for SH4-200 and SH4-300
+	.balign 4
+	.global GLOBAL(fixdfsi)
+	FUNC(GLOBAL(fixdfsi))
+	.balign	4
+GLOBAL(fixdfsi):
+	mov.w	LOCAL(x413),r1
+	mov	DBL0H,r0
+	shll	DBL0H
+	mov.l	LOCAL(mask),r3
+	mov	#-21,r2
+	shld	r2,DBL0H	! SH4-200 will start this insn in a new cycle
+	bt/s	LOCAL(neg)
+	sub	r1,DBL0H
+	cmp/pl	DBL0H		! SH4-200 will start this insn in a new cycle
+	and	r3,r0
+	bf/s	LOCAL(ignore_low)
+	addc	r3,r0	! uses T == 1; sets implict 1
+	mov	#10,r2
+	shld	DBL0H,r0	! SH4-200 will start this insn in a new cycle
+	cmp/gt	r2,DBL0H
+	add	#-32,DBL0H
+	bt	LOCAL(retmax)
+	shld	DBL0H,DBL0L
+	rts
+	or	DBL0L,r0
+
+	.balign	8
+LOCAL(ignore_low):
+	mov	#-21,r2
+	cmp/gt	DBL0H,r2	! SH4-200 will start this insn in a new cycle
+	bf	0f		! SH4-200 will start this insn in a new cycle
+	mov	#-31,DBL0H	! results in 0 return
+0:	add	#1,r0
+	rts
+	shld	DBL0H,r0
+
+	.balign 4
+LOCAL(neg):
+	cmp/pl	DBL0H
+	and	r3,r0
+	bf/s	LOCAL(ignore_low_neg)
+	addc	r3,r0	! uses T == 1; sets implict 1
+	mov	#10,r2
+	shld	DBL0H,r0	! SH4-200 will start this insn in a new cycle
+	cmp/gt	r2,DBL0H
+	add	#-32,DBL0H
+	bt	LOCAL(retmin)
+	shld	DBL0H,DBL0L
+	or	DBL0L,r0	! SH4-200 will start this insn in a new cycle
+	rts
+	neg	r0,r0
+
+	.balign 4
+LOCAL(ignore_low_neg):
+	mov	#-21,r2
+	cmp/gt	DBL0H,r2	! SH4-200 will start this insn in a new cycle
+	add	#1,r0
+	shld	DBL0H,r0
+	bf	0f
+	mov	#0,r0		! results in 0 return
+0:	rts
+	neg	r0,r0
+
+LOCAL(retmax):
+	mov	#-1,r0
+	rts
+	shlr	r0
+
+LOCAL(retmin):
+	mov	#1,r0
+	rts
+	rotr	r0
+
+LOCAL(x413): .word 0x413
+
+	.balign 4
+LOCAL(mask): .long 0x000fffff
+	ENDFUNC(GLOBAL(fixdfsi))
Index: gcc/config/sh/IEEE-754/divdf3.S
===================================================================
--- gcc/config/sh/IEEE-754/divdf3.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/divdf3.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,598 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!division of two double precision floating point numbers
+!Author:Aanchal Khanna
+!
+!Entry:
+!r4,r5:dividend
+!
+!r6,r7:divisor
+!
+!Exit:
+!r0,r1:quotient
+
+!Notes: dividend is passed in regs r4 and r5 and divisor is passed in regs 
+!r6 and r7, quotient is returned in regs r0 and r1. dividend is referred as op1
+!and divisor as op2.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align	5
+	.global	GLOBAL (divdf3)
+	FUNC (GLOBAL (divdf3))
+
+GLOBAL (divdf3):
+
+#ifdef  __LITTLE_ENDIAN__
+	mov	r4,r1
+	mov	r5,r4
+	mov	r1,r5
+
+        mov     r6,r1
+        mov     r7,r6
+        mov     r1,r7
+#endif
+	mov	r4,r2
+	mov.l	.L_inf,r1
+
+	and	r1,r2
+	mov.l   r8,@-r15
+
+	cmp/eq	r1,r2
+	mov     r6,r8
+
+	bt	.L_a_inv
+	and	r1,r8
+
+	cmp/eq	r1,r8
+	mov.l	.L_high_mant,r3
+
+	bf	.L_chk_zero
+	and	r6,r3
+
+	mov.l   .L_mask_sign,r8	
+	cmp/pl	r7
+
+	mov	r8,r0
+	bt	.L_ret_b	!op2=NaN,return op2
+
+	and	r4,r8
+	cmp/pl	r3
+
+	and	r6,r0
+	bt	.L_ret_b	!op2=NaN,return op2
+
+	xor     r8,r0           !op1=normal no,op2=Inf, return Zero
+	mov     #0,r1
+	
+#ifdef __LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+
+.L_ret_b:
+	mov	r7,r1
+	mov     r6,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	mov.l   @r15+,r8
+
+.L_a_inv:
+	!chk if op1 is Inf or NaN
+	mov.l   .L_high_mant,r2
+	cmp/pl  r5
+
+	and	r4,r2
+	bt	.L_ret_a
+
+	and	r1,r8		!r1 contains infinity
+	cmp/pl	r2
+
+	bt	.L_ret_a
+	cmp/eq	r1,r8
+
+	mov	r1,DBLRH
+	add	DBLRH,DBLRH
+	bf	0f
+	mov	#-1,DBLRH	! Inf/Inf, return NaN.
+0:	div0s	r4,r6
+	mov.l   @r15+,r8	
+	rts
+	rotcr	DBLRH
+
+.L_ret_a:
+	!return op1
+	mov	r5,r1
+	mov	r4,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+        mov.l   @r15+,r8
+
+.L_chk_zero:
+	!chk if op1=0
+	mov.l   .L_mask_sign,r0
+        mov     r4,r3
+
+        and     r0,r3
+        shll    r4
+
+        and     r6,r0
+        shlr    r4
+
+        xor     r3,r0
+        shll    r6
+
+	shlr	r6
+	tst	r4,r4
+
+
+	bf      .L_op1_not_zero	
+	tst	r5,r5
+	
+        bf      .L_op1_not_zero
+	tst	r7,r7
+
+	mov.l   @r15+,r8
+	bf	.L_ret_zero
+
+	tst	r6,r6
+	bf	.L_ret_zero
+
+	rts
+	mov     #-1,DBLRH       !op1=op2=0, return NaN
+	
+.L_ret_zero:
+	!return zero
+	mov	r0,r1
+	rts
+#ifdef __LITTLE__ENDIAN
+	mov	#0,r0
+#else
+	mov	#0,r1		!op1=0,op2=normal no,return zero
+#endif
+
+.L_norm_b:
+	!normalize op2
+        shll    r7
+        mov.l   .L_imp_bit,r3
+
+        rotcl   r6
+        tst     r3,r6
+
+        add     #-1,r8
+        bt      .L_norm_b
+
+        bra     .L_divide
+        add     #1,r8
+
+.L_op1_not_zero:
+	!op1!=0, chk if op2=0
+	tst	r7,r7	
+	mov	r1,r3
+	
+	mov	#0,r1
+	bf	.L_normal_nos
+
+	tst	r6,r6
+	bf      .L_normal_nos
+
+	mov.l   @r15+,r8
+	or	r3,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	nop
+
+.L_normal_nos:
+	!op1 and op2 are normal nos
+	tst	r2,r2
+	mov	#-20,r1
+
+! The subsequent branch is for the upper compare
+! Shifting will not alter the result, for the
+! macro is declared with care.
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld    r1,r2
+#else
+	SHLR20 (r2)
+#endif
+	bt	.L_norm_a	!normalize dividend
+	
+.L_chk_b:
+	mov.l	r9,@-r15
+	tst	r8,r8
+
+        mov.l   .L_high_mant,r9
+
+! The subsequent branch is for the upper compare
+! Shifting will not alter the result, for the
+! macro is declared with care.
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r8
+#else
+        SHLR20 (r8)
+#endif
+				! T set -> normalize divisor
+	SL(bt,	.L_norm_b,
+	 and	r9,r4)
+
+.L_divide:
+	mov.l   .L_2047,r1
+	sub	r8,r2
+
+	mov.l	.L_1023,r8
+	and	r9,r6
+
+	!resultant exponent
+	add	r8,r2
+	!chk the exponent for overflow
+	cmp/ge	r1,r2
+	
+	mov.l	.L_imp_bit,r1
+	bt	.L_overflow
+	
+	mov	#0,r8
+	or	r1,r4
+	
+	or      r1,r6	
+	mov	#-24,r3
+
+	!chk if the divisor is 1(mantissa only)
+	cmp/eq	r8,r7
+	bf	.L_div2
+
+	cmp/eq	 r6,r1
+	bt	.L_den_one
+
+.L_div2:
+	!divide the mantissas
+	shll8	r4
+	mov	r5,r9
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r3,r9
+#else
+        SHLR24 (r9)
+#endif
+	shll8	r6
+
+	or	r9,r4
+	shll8   r5
+
+	mov	r7,r9
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r3,r9
+#else
+        SHLR24 (r9)
+#endif
+	mov	r8,r3
+	shll8	r7
+
+	or	r9,r6	
+	cmp/gt	r4,r6
+
+	mov	r3,r9
+	bt	.L_shift
+
+	cmp/eq	r4,r6
+	bf	.L_loop
+
+	cmp/gt	r5,r7
+	bf	.L_loop
+
+.L_shift:
+	add	#-1,r2
+	shll	r5
+	rotcl	r4
+
+.L_loop:
+	!actual division loop
+	cmp/gt	r6,r4
+	bt	.L_subtract
+
+	cmp/eq	r6,r4
+	bf	.L_skip
+
+	cmp/ge	r7,r5
+	bf	.L_skip
+
+.L_subtract:
+	clrt
+	subc	r7,r5
+	
+	or	r1,r8
+	subc	r6,r4
+
+.L_skip:
+	shlr	r1
+	shll	r5
+
+	rotcl	r4
+	cmp/eq	r1,r3
+
+	bf	.L_loop
+	mov.l	.L_imp_bit,r1
+
+	!chk if the divison was for the higher word of the quotient
+	tst	r1,r9
+	bf	.L_chk_exp
+
+	mov	r8,r9
+	mov.l   .L_mask_sign,r1
+
+	!divide for the lower word of the quotient
+	bra	.L_loop
+	mov	r3,r8
+
+.L_chk_exp:
+	!chk if the result needs to be denormalized
+	cmp/gt	r2,r3
+	bf	.L_round
+	mov     #-53,r7
+
+.L_underflow:
+	!denormalize the result
+	add	#1,r2
+	cmp/gt	r2,r7
+
+	or      r4,r5           !remainder
+	add	#-2,r2
+
+	mov	#32,r4
+	bt      .L_return_zero
+
+	add	r2,r4
+	cmp/ge	r3,r4
+
+	mov	r2,r7
+	mov	r3,r1
+
+	mov     #-54,r2
+	bt	.L_denorm
+	mov	#-32,r7
+
+.L_denorm:
+	shlr	r8
+	rotcr	r1
+
+	shll	r8
+	add     #1,r7
+
+	shlr	r9
+	rotcr	r8
+
+	cmp/eq	r3,r7
+	bf	.L_denorm
+
+	mov	r4,r7
+	cmp/eq	r2,r4
+
+	bt	.L_break
+	mov     r3,r6
+
+	cmp/gt	r7,r3
+	bf	.L_break
+
+	mov	r2,r4
+	mov	r1,r6
+
+	mov	r3,r1
+	bt	.L_denorm
+
+.L_break:
+	mov     #0,r2
+
+	cmp/gt	r1,r2
+
+	addc	r2,r8
+	mov.l   .L_comp_1,r4
+
+	addc	r3,r9		
+	or	r9,r0
+
+	cmp/eq	r5,r3
+	bf	.L_return	
+
+	cmp/eq	r3,r6
+	mov.l	.L_mask_sign,r7
+
+	bf	.L_return
+	cmp/eq	r7,r1
+
+	bf	.L_return
+	and	r4,r8
+
+.L_return:
+	mov.l	@r15+,r9
+	mov     r8,r1
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+
+.L_norm_a:
+        !normalize op1
+        shll    r5
+        mov.l   .L_imp_bit,r3
+
+        rotcl   r4
+        tst     r3,r4
+
+        add     #-1,r2
+        bt      .L_norm_a
+
+        bra     .L_chk_b
+        add     #1,r2
+
+.L_overflow:
+	!overflow, return inf
+	mov.l   .L_inf,r2
+#ifdef __LITTLE_ENDIAN__
+	or	r2,r1	
+	mov	#0,r0
+#else
+	or	r2,r0
+	mov	#0,r1
+#endif
+        mov.l   @r15+,r9
+        rts
+        mov.l   @r15+,r8
+
+.L_den_one:
+	!denominator=1, result=numerator
+        mov     r4,r9
+        mov   	#-53,r7
+
+	cmp/ge	r2,r8
+	mov	r8,r4
+
+	mov	r5,r8
+	mov	r4,r3
+
+	!chk the exponent for underflow
+	SL(bt,	.L_underflow,
+	 mov     r4,r5)
+
+	mov.l	.L_high_mant,r7
+        bra     .L_pack
+	mov     #20,r6
+
+.L_return_zero:
+	!return zero
+	mov	r3,r1
+	mov.l	@r15+,r9
+
+	rts
+	mov.l   @r15+,r8
+
+.L_round:
+	!apply rounding
+	cmp/eq	r4,r6
+	bt	.L_lower
+
+	clrt
+	subc    r6,r4
+
+	bra     .L_rounding
+	mov	r4,r6
+	
+.L_lower:
+	clrt
+	subc	r7,r5
+	mov	r5,r6
+	
+.L_rounding:
+	!apply rounding
+	mov.l   .L_invert,r1
+	mov	r3,r4
+
+	movt	r3
+	clrt
+	
+	not	r3,r3
+	and	r1,r3	
+
+	addc	r3,r8
+	mov.l   .L_high_mant,r7
+
+	addc	r4,r9
+	cmp/eq	r4,r6
+
+	mov.l   .L_comp_1,r3
+	SL (bf,	.L_pack,
+	 mov     #20,r6)
+	and	r3,r8
+
+.L_pack:
+	!pack the result, r2=exponent,r0=sign,r8=lower mantissa, r9=higher mantissa
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld    r6,r2
+#else
+        SHLL20 (r2)
+#endif
+	and	r7,r9
+
+	or	r2,r0
+	mov	r8,r1
+
+	or      r9,r0
+	mov.l	@r15+,r9
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+	.align	2
+
+.L_mask_sign:
+	.long	0x80000000
+.L_high_mant:
+	.long	0x000fffff
+.L_inf:
+	.long	0x7ff00000
+.L_1023:
+	.long	1023
+.L_2047:
+	.long	2047
+.L_imp_bit:
+	.long	0x00100000	
+.L_comp_1:
+	.long	0xfffffffe
+.L_invert:
+	.long	0x00000001
+
+ENDFUNC (GLOBAL (divdf3))
Index: gcc/config/sh/IEEE-754/floatunssisf.S
===================================================================
--- gcc/config/sh/IEEE-754/floatunssisf.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/floatunssisf.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,137 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Conversion of unsigned integer to floating point
+
+! Author: Rakesh Kumar
+
+! Argument: r4
+! Result: r0
+
+! r4 is referred as op1
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (floatunsisf)
+	FUNC (GLOBAL (floatunsisf))
+
+GLOBAL (floatunsisf):
+	tst	r4,r4
+	mov	#23,r6
+
+	mov.l	.L_set_24_bits,r7
+	SL(bt,	.L_return,
+	 not	r7,r3)
+
+	! Decide the direction for shifting
+	mov.l	.L_set_24_bit,r5
+	cmp/hi	r7,r4
+
+	not	r5,r2
+	SL(bt,	.L_shift_right,
+	 mov	#0,r7)
+
+	tst	r5,r4
+	
+	mov	#0,r0
+	bf	.L_pack_sf
+
+! Shift the bits to the left. Adjust the exponent
+.L_shift_left:
+	shll	r4
+	tst	r5,r4
+
+	add	#-1,r6
+	bt	.L_shift_left
+
+! Pack the value in floating point format.
+! r6 has unbiased exponent, r4 has mantissa
+.L_pack_sf:
+	mov	#23,r3
+	add	#127,r6
+
+	! Align the exponent
+	and	r2,r4
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+        SHLL23 (r6)
+#else
+	shld	r3,r6
+#endif
+
+	or	r6,r0
+	rts
+	or	r4,r0
+
+! Shift right the number with rounding
+.L_shift_right:
+	shlr	r4
+	rotcr	r7
+
+	tst	r4,r3
+	add	#1,r6
+
+	bf	.L_shift_right
+	
+	tst	r7,r7
+	bt	.L_sh_rt_1
+
+	shll	r7
+	movt	r1
+
+	add	r1,r4
+
+	tst	r7,r7
+	bf	.L_sh_rt_1
+
+	! Halfway between two numbers.
+	! Round towards LSB = 0
+	shlr	r4
+	shll	r4
+
+.L_sh_rt_1:
+	mov	r4,r0
+
+	! Rounding may have misplaced MSB. Adjust.
+	and	r3,r0
+	cmp/eq	#0,r0
+
+	bf	.L_shift_right
+	bt	.L_pack_sf
+
+.L_return:
+	rts
+	mov	r4,r0
+
+	.align 2
+.L_set_24_bit:
+	.long 0x00800000
+
+.L_set_24_bits:
+	.long 0x00FFFFFF
+
+ENDFUNC (GLOBAL (floatunsisf))
Index: gcc/config/sh/IEEE-754/fixunssfsi.S
===================================================================
--- gcc/config/sh/IEEE-754/fixunssfsi.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/fixunssfsi.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,155 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Conversion from floating point to unsigned integer
+
+! Author: Rakesh Kumar
+
+! Argument: r4 (in floating point format)
+! Result: r0
+
+! For negative floating point numbers, it returns zero
+
+! The argument is referred as op1
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 5
+	.global	GLOBAL (fixunssfsi)
+	FUNC (GLOBAL (fixunssfsi))
+
+GLOBAL (fixunssfsi):
+	mov.l	.L_sign,r0
+	mov	r4,r2
+
+	! Check for NaN
+	mov.l	.L_inf,r1
+	and	r4,r0
+
+	mov.l	.L_mask_sign,r7
+	mov	#127,r5
+
+	! Remove sign bit
+	cmp/eq	#0,r0
+	and	r7,r2
+
+	! If number is negative, return 0
+	! LIBGCC deviates from standard in this regard.
+	mov	r4,r3
+	SL(bf,	.L_epil,
+	 mov	#0,r0)
+
+	mov.l	.L_frac,r6
+	cmp/gt	r1,r2
+
+	shll	r2
+	SL1(bt,	.L_epil,
+	 shlr16	r2)
+
+	shlr8	r2	! r2 has exponent
+	mov.l	.L_24bit,r1
+
+	and	r6,r3	! r3 has fraction
+	cmp/gt	r2,r5
+
+	! If exponent is less than 127, return 0
+	or	r1,r3
+	bt	.L_epil
+
+	! Process only if exponent is less than 158
+	mov.l	.L_158,r1
+	shll8	r3
+
+	cmp/gt	r1,r2
+	sub	r2,r1
+
+	neg	r1,r1
+	bt	.L_ret_max
+
+! Shift the mantissa with exponent difference from 158
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r1,r3
+#else
+	cmp/gt	r0,r1
+	bt	.L_mov_left
+
+.L_mov_right:
+	cmp/eq	r1,r0
+	bt	.L_ret
+
+	add	#1,r1
+	bra	.L_mov_right
+	shlr	r3
+
+.L_mov_left:
+	add	#-1,r1
+	
+	shll	r3
+	cmp/eq	r1,r0
+
+	bf	.L_mov_left
+
+.L_ret:	
+#endif
+	rts
+	mov	r3,r0
+
+! r0 already has appropriate value
+.L_epil:
+	rts
+	nop
+
+! Return the maximum unsigned integer value
+.L_ret_max:
+	mov.l	.L_max,r3
+
+	rts
+	mov	r3,r0
+
+	.align 2
+.L_inf:
+	.long 0x7F800000
+
+.L_158:
+	.long 158
+
+.L_max:
+	.long 0xFFFFFFFF
+
+.L_frac:
+	.long 0x007FFFFF
+
+.L_sign:
+	.long 0x80000000
+
+.L_24bit:
+	.long 0x00800000
+
+.L_mask_sign:
+	.long 0x7FFFFFFF
+
+ENDFUNC (GLOBAL (fixunssfsi))
Index: gcc/config/sh/IEEE-754/floatunssidf.S
===================================================================
--- gcc/config/sh/IEEE-754/floatunssidf.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/floatunssidf.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,76 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!conversion of unsigned integer to double precision floating point number
+!Author:Rakesh Kumar
+!Rewritten for SH1 support: Joern Rennecke
+!
+!Entry:
+!r4:operand
+!
+!Exit:
+!r0,r1:result
+!
+!Note:argument is passed in reg r4 and the result is returned in
+!regs r0 and r1.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (floatunsidf)
+	FUNC (GLOBAL (floatunsidf))
+
+GLOBAL (floatunsidf):
+	mov.w	LOCAL(x41f0),DBLRH	! bias + 32
+	tst	r4,r4			! check for zero
+	bt	.L_ret_zero
+.L_loop:
+	shll	r4	
+	SL(bf,	.L_loop,
+	 add	#-16,DBLRH)
+
+	mov	r4,DBLRL
+
+        SHLL20 (DBLRL)
+
+        shll16	DBLRH ! put exponent in proper place
+
+        SHLR12 (r4)
+
+	rts
+	or	r4,DBLRH
+	
+.L_ret_zero:
+	mov	#0,r1
+	rts
+	mov	#0,r0
+
+LOCAL(x41f0):	.word	0x41f0
+	.align 2
+
+ENDFUNC (GLOBAL (floatunsidf))
Index: gcc/config/sh/IEEE-754/fixunsdfsi.S
===================================================================
--- gcc/config/sh/IEEE-754/fixunsdfsi.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/fixunsdfsi.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,181 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!conversion of double precision floating point number to unsigned integer
+!Author:Aanchal Khanna
+!
+!Entry:
+!r4,r5:operand
+!
+!Exit:
+!r0:result
+!
+!Note:argument is passed in regs r4 and r5, the result is returned in
+!reg r0.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 5
+	.global GLOBAL (fixunsdfsi)
+	FUNC (GLOBAL (fixunsdfsi))
+
+GLOBAL (fixunsdfsi):
+
+#ifdef  __LITTLE_ENDIAN__
+        mov     r4,r1
+        mov     r5,r4
+        mov     r1,r5
+#endif
+	mov.l	.L_p_inf,r2
+	mov     #-20,r1
+	
+	mov	r2,r7
+	mov.l   .L_1023,r3
+
+	and	r4,r2
+	shll    r4
+
+        movt    r6		! r6 contains the sign bit
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r2           ! r2 contains the exponent
+#else
+        SHLR20 (r2)
+#endif
+	shlr    r4
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r7
+#else
+        SHLR20 (r7)
+#endif
+	tst	r6,r6	
+	SL(bf,	.L_epil,
+	 mov	#0,r0)
+
+	cmp/hi	r2,r3		! if exp < 1023,return 0
+	mov.l	.L_high_mant,r1
+
+	SL(bt,	.L_epil,
+	 and	r4,r1)		! r1 contains high mantissa
+
+	cmp/eq	r2,r7		! chk if exp is invalid
+	mov.l	.L_1054,r7
+
+	bt	.L_inv_exp
+	mov	#11,r0
+	
+	cmp/hi	r7,r2		! If exp > 1054,return maxint
+	sub     r2,r7		!r7 contains the number of shifts
+
+	mov.l	.L_21bit,r2
+	bt	.L_ret_max
+
+	or	r2,r1
+	mov	r7,r3
+
+	shll8   r1
+	neg     r7,r7
+
+	shll2	r1
+
+        shll	r1
+	cmp/hi	r3,r0
+
+	SL(bt,	.L_lower_mant,
+	 mov	#21,r0)
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r1
+#else
+.L_sh_loop:
+        tst	r7,r7
+        bt      .L_break
+        add     #1,r7
+        bra     .L_sh_loop
+        shlr    r1
+
+.L_break:
+#endif
+	rts
+	mov     r1,r0
+
+.L_lower_mant:
+	neg	r0,r0
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r0,r5
+#else
+        SHLR21 (r5)
+#endif
+	or	r5,r1		!pack lower and higher mantissas
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r1
+#else
+.L_loop:
+        tst	r7,r7
+        bt      .L_break1
+        add     #1,r7
+        bra     .L_loop
+        shlr    r1
+
+.L_break1:
+#endif
+	mov	r1,r0
+.L_epil:
+	rts
+	nop
+
+.L_inv_exp:
+	cmp/hi	r0,r5
+	bt	.L_epil
+
+	cmp/hi	r0,r1		!compare high mantissa,r1
+	bt	.L_epil
+
+.L_ret_max:
+	mov.l   .L_maxint,r0
+
+	rts
+	nop
+
+	.align	2
+
+.L_maxint:
+	.long	0xffffffff
+.L_p_inf:
+	.long	0x7ff00000
+.L_high_mant:
+	.long	0x000fffff
+.L_1023:
+	.long	0x000003ff
+.L_1054:
+	.long	1054
+.L_21bit:
+	.long	0x00100000
+
+ENDFUNC (GLOBAL (fixunsdfsi))
Index: gcc/config/sh/IEEE-754/addsf3.S
===================================================================
--- gcc/config/sh/IEEE-754/addsf3.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/addsf3.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,535 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Add floating point numbers in r4, r5.
+
+! Author: Rakesh Kumar
+
+! Arguments are in r4, r5 and result in r0
+
+! Entry points: ___subsf3, ___addsf3
+
+! r4 and r5 are referred as op1 and op2
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 5
+        .global GLOBAL (subsf3)
+	.global	GLOBAL (addsf3)
+	FUNC (GLOBAL (subsf3))
+	FUNC (GLOBAL (addsf3))
+
+GLOBAL (subsf3):
+        mov.l   .L_sign_bit,r1
+        xor     r1,r5
+
+GLOBAL (addsf3):
+	mov.l	r8,@-r15
+	mov	r4,r3
+
+	mov.l	.L_pinf,r2
+	mov	#0,r8
+
+	and	r2,r3 ! op1's exponent.
+	mov	r5,r6
+
+	! Check NaN or Infinity
+	and	r2,r6 ! op2's exponent.
+	cmp/eq	r2,r3
+
+	! go if op1 is NaN or INF. 
+	mov.l	.L_sign_bit,r0
+	SL(bt,	.L_inv_op1,
+	 mov	#-23,r1)
+	
+	! Go if op2 is NaN/INF.
+	cmp/eq	r2,r6
+	mov	r0,r7
+	bt	.L_ret_op2
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLR23 (r3)
+#else
+	shld	r1,r3
+#endif
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLR23 (r6)
+#else
+	shld	r1,r6
+#endif
+
+	! Check for negative zero
+	cmp/eq	r0,r5
+
+	mov	r5,r1
+	SL(bt,	.L_ret_op1,
+	 and	r7,r1)
+
+	cmp/eq	r0,r4
+	bt	.L_ret_op2
+
+	! if op1 is zero return op2
+	tst	r4,r4
+	bt	.L_ret_op2
+
+	! Equal numbers with opposite sign
+	mov	r4,r2
+	xor	r5,r2
+
+	cmp/eq	r0,r2
+	bt	.L_ret_zero
+
+	! if op2 is zero return op1
+	mov.l	.L_mask_fra,r2
+	tst	r5,r5
+
+	! Extract the mantissa
+	mov	r4,r0
+	SL(bt,	.L_ret_op1,
+	 and	r2,r5)
+
+	and	r2,r4
+
+	mov.l	.L_imp_bit,r2
+	and	r7,r0	! sign bit of op1
+
+	! Check for denormals
+	tst	r3,r3
+	bt	.L_norm_op1
+
+	! Attach the implicit bit
+	or	r2,r4
+	tst	r6,r6
+
+	bt	.L_norm_op2
+
+	or	r2,r5
+	tst	r0,r0
+
+	! operands are +ve or -ve??
+	bt	.L_ptv_op1
+
+	neg	r4,r4
+
+.L_ptv_op1:
+	tst	r1,r1
+	bt	.L_ptv_op2
+
+	neg	r5,r5
+
+! Test exponents for equality
+.L_ptv_op2:
+	cmp/eq	r3,r6
+	bt	.L_exp_eq
+
+! Make exponents of two arguments equal
+.L_exp_ne:
+	! r0, r1 contain sign bits.
+	! r4, r5 contain mantissas.
+	! r3, r6 contain exponents.
+	! r2, r7 scratch.
+
+	! Calculate result exponent.
+	mov	r6,r2
+	sub	r3,r2	! e2 - e1
+
+	cmp/pl	r2
+	mov	#23,r7
+
+	! e2 - e1 is -ve
+	bf	.L_exp_ne_1
+
+	mov	r6,r3 ! Result exp.
+	cmp/gt	r7,r2 ! e2-e1 > 23
+
+	mov	#1,r7
+	bt	.L_pack_op2_0
+
+	! Align the mantissa
+.L_loop_ne:
+	shar	r4
+
+	rotcr	r8
+	cmp/eq	r7,r2
+
+	add	#-1,r2
+	bf	.L_loop_ne
+
+	bt	.L_exp_eq
+
+! Exponent difference is too high.
+! Return op2 after placing pieces in proper place
+.L_pack_op2_0:
+	! If op1 is -ve
+	tst	r1,r1
+	bt	.L_pack_op2
+
+	neg	r5,r5
+
+! r6 has exponent
+! r5 has mantissa, r1 has sign
+.L_pack_op2:
+	mov.l	.L_nimp_bit,r2
+	mov	#23,r3
+
+	mov	r1,r0
+	
+	and	r2,r5
+	mov.l	@r15+,r8
+
+	or	r5,r0
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r6)
+#else
+	shld	r3,r6
+#endif
+        rts
+	or	r6,r0
+
+! return op1. It is NAN or INF or op2 is zero.
+.L_ret_op1:
+	mov	r4,r0
+
+	rts
+	mov.l	@r15+,r8
+
+! return zero
+.L_ret_zero:
+	mov	#0,r0
+
+	rts
+	mov.l	@r15+,r8
+
+! return op2. It is NaN or INF or op1 is zero.
+.L_ret_op2:
+	mov	r5,r0
+
+	rts
+	mov.l	@r15+,r8
+
+! op2 is denormal. Normalize it.
+.L_norm_op2:
+	shll	r5
+	add	#-1,r6
+
+	tst	r2,r5
+	bt	.L_norm_op2
+
+	! Check sign
+	tst	r1,r1
+	bt	.L_norm_op2_2
+
+	neg	r5,r5
+
+.L_norm_op2_2:
+	add	#1,r6
+	cmp/eq	r3,r6
+
+	bf	.L_exp_ne
+	bt	.L_exp_eq
+
+! Normalize op1
+.L_norm_op1:
+	shll	r4
+	add	#-1,r3
+
+	tst	r2,r4
+	bt	.L_norm_op1
+
+	! Check sign
+	tst	r0,r0
+	bt	.L_norm_op1_1
+
+	neg	r4,r4
+
+.L_norm_op1_1:
+	! Adjust biasing
+	add	#1,r3
+
+	! Check op2 for denormalized value
+	tst	r6,r6
+	bt	.L_norm_op2
+
+	mov.l	.L_imp_bit,r2
+
+	tst	r1,r1	! Check sign
+	or	r2,r5	! Attach 24th bit
+
+	bt	.L_norm_op1_2
+
+	neg	r5,r5
+
+.L_norm_op1_2:
+	cmp/eq	r3,r6
+
+	bt	.L_exp_eq
+	bf	.L_exp_ne
+
+! op1 is NaN or Inf
+.L_inv_op1:
+	! Return op1 if it is NAN. 
+	! r2 is infinity
+	cmp/gt	r2,r4
+	bt	.L_ret_op1
+
+	! op1 is +/- INF
+	! If op2 is same return now.
+	cmp/eq	r4,r5
+	bt	.L_ret_op1
+
+	! return op2 if it is NAN
+	cmp/gt	r2,r5
+	bt	.L_ret_op2
+
+	! Check if op2 is inf
+	cmp/eq	r2,r6
+	bf	.L_ret_op1
+	
+	! Both op1 and op2 are infinities 
+	!of opp signs, or there is -NAN. Return a NAN.
+	mov.l	@r15+,r8
+	rts
+	mov	#-1,r0
+
+! Make unequal exponents equal.
+.L_exp_ne_1:
+	mov	#-25,r7
+	cmp/gt	r2,r7 ! -23 > e2 - e1
+
+	add	#1,r2
+	bf	.L_exp_ne_2
+
+	tst	r0,r0
+	bt	.L_pack_op1
+
+.L_pack_op1_0:
+	bra	.L_pack_op1
+	neg	r4,r4
+
+! Accumulate the shifted bits in r8
+.L_exp_ne_2:
+	! Shift with rounding
+	shar	r5
+	rotcr	r8
+
+	tst	r2,r2
+
+	add	#1,r2
+	bf	.L_exp_ne_2
+
+! Exponents of op1 and op2 are equal (or made so)
+! The mantissas are in r4-r5 and remaining bits in r8
+.L_exp_eq:
+	add	r5,r4 ! Add fractions.
+	mov.l	.L_sign_bit,r2
+
+	! Check for negative result
+	mov	#0,r0
+	tst	r2,r4
+
+	mov.l	.L_255,r5
+	bt	.L_post_add
+
+	negc	r8,r8
+	negc	r4,r4
+	or	r2,r0
+
+.L_post_add:
+	! Check for extra MSB
+	mov.l	.L_chk_25,r2
+
+	tst	r2,r4
+	bt	.L_imp_check
+
+	shar 	r4
+	rotcr	r8
+
+	add	#1,r3
+	cmp/ge	r5,r3
+
+	! Return Inf if exp > 254
+	bt	.L_ret_inf
+
+! Check for implicit (24th) bit in result
+.L_imp_check:
+        mov.l	.L_imp_bit,r2
+	tst	r2,r4
+
+	bf	.L_pack_op1
+
+! Result needs left shift
+.L_lft_shft:
+	shll	r8
+	rotcl	r4
+
+	add	#-1,r3
+	tst	r2,r4
+
+	bt	.L_lft_shft
+	
+! Pack the result after rounding
+.L_pack_op1:
+	! See if denormalized result is possible 
+	mov.l	.L_chk_25,r5
+	cmp/pl	r3
+
+	bf	.L_denorm_res
+
+	! Are there any bits shifted previously?
+	tst	r8,r8
+	bt	.L_pack_1
+
+	! Round
+	shll	r8
+	movt	r6
+
+	add	r6,r4
+
+	! If we are halfway between two numbers,
+	! round towards LSB = 0
+	tst	r8,r8
+
+	bf	.L_pack_1
+
+	shlr	r4
+	shll	r4
+
+.L_pack_1:
+	! Adjust extra MSB generated after rounding
+	tst	r4,r5
+	mov.l	.L_255,r2
+
+	bt	.L_pack_2
+	shar	r4
+
+	add	#1,r3 
+	cmp/ge	r2,r3	! Check for exp overflow
+
+	bt	.L_ret_inf
+	
+! Pack it finally
+.L_pack_2:
+	! Do not store implicit bit
+	mov.l	.L_nimp_bit,r2
+	mov	#23,r1
+
+	and	r2,r4
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r3)
+#else
+	shld	r1,r3
+#endif
+	mov.l	@r15+,r8
+
+	or	r4,r0
+        rts
+	or	r3,r0
+
+! Return infinity
+.L_ret_inf:
+	mov.l	.L_pinf,r2
+
+	mov.l	@r15+,r8
+	rts
+	or	r2,r0
+
+! Result must be denormalized
+.L_denorm_res:
+	mov	#0,r2
+	
+! Denormalizing loop with rounding
+.L_den_1:
+	shar	r4
+	movt	r6
+
+	tst	r3,r3
+	bt	.L_den_2
+
+	! Increment the exponent
+	add	#1,r3
+
+	tst	r6,r6
+	bt	.L_den_0
+
+	! Count number of ON bits shifted
+	add	#1,r2
+
+.L_den_0:
+	bra	.L_den_1
+	nop
+
+! Apply rounding
+.L_den_2:
+	cmp/eq	r6,r1
+	bf	.L_den_3
+
+	add	r6,r4
+	mov	#1,r1
+
+	! If halfway between two numbers,
+	! round towards LSB = 0
+	cmp/eq	r2,r1
+	bf	.L_den_3
+
+	shar	r4
+	shll	r4
+
+.L_den_3:
+
+	mov.l	@r15+,r8
+	rts
+	or	r4,r0
+	
+	.align 2
+.L_imp_bit:
+        .long   0x00800000
+
+.L_nimp_bit:
+	.long	0xFF7FFFFF
+
+.L_mask_fra:
+        .long   0x007FFFFF
+
+.L_pinf:
+        .long   0x7F800000
+
+.L_sign_bit:
+	.long	0x80000000
+
+.L_bit_25:
+	.long	0x01000000
+
+.L_chk_25:
+        .long   0x7F000000
+
+.L_255:
+	.long	0x000000FF
+
+ENDFUNC (GLOBAL (addsf3))
+ENDFUNC (GLOBAL (subsf3))
Index: gcc/config/sh/IEEE-754/adddf3.S
===================================================================
--- gcc/config/sh/IEEE-754/adddf3.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/adddf3.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,799 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Routine for adding two double numbers
+
+! Author: Rakesh Kumar
+! SH1 Support by Joern Rennecke
+! Sticky Bit handling : Joern Rennecke
+
+! Arguments: r4-r5, r6-r7
+! Result: r0-r1
+
+! The value in r4-r5 is referred to as op1
+! and that in r6-r7 is referred to as op2
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+        .align 5
+	.global	GLOBAL (subdf3)
+	FUNC (GLOBAL (subdf3))
+        .global GLOBAL (adddf3)
+	FUNC (GLOBAL (adddf3))
+
+GLOBAL (subdf3):
+#ifdef __LITTLE_ENDIAN__
+	mov	r4,r1
+	mov	r6,r2
+
+	mov	r5,r4
+	mov	r7,r6
+
+	mov	r1,r5
+	mov	r2,r7
+#endif
+	mov.l	.L_sign,r2
+	bra	.L_adddf3_1
+	xor	r2,r6
+
+GLOBAL (adddf3):
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+	mov	r6,r2
+
+	mov	r5,r4
+	mov	r7,r6
+
+	mov	r1,r5
+	mov	r2,r7
+#endif
+	
+.L_adddf3_1:
+	mov.l	r8,@-r15
+	mov	r4,r1
+
+	mov.l 	.L_inf,r2
+	mov	r6,r3
+
+	mov.l	r9,@-r15
+	and	r2,r1		!Exponent of op1 in r1
+
+	mov.l	r10,@-r15
+	and	r2,r3		!Exponent of op2 in r3
+
+	! Check for Nan or Infinity
+	mov.l	.L_sign,r9
+	cmp/eq	r2,r1
+
+	mov	r9,r10
+	bt	.L_thread_inv_exp_op1
+
+	mov	r9,r0
+	cmp/eq	r2,r3
+! op1 has a valid exponent. We need not check it again.
+! Return op2 straight away.
+	and	r4,r9		!r9 has sign bit for op1
+	bt	.L_ret_op2
+
+	! Check for -ve zero
+	cmp/eq	r4,r0
+	and	r6,r10		!r10 has sign bit for op2
+
+	bt	.L_op1_nzero
+
+	cmp/eq	r6,r0
+	bt	.L_op2_nzero
+
+! Check for zero
+.L_non_zero:
+	tst	r4,r4
+	bt	.L_op1_zero
+
+	! op1 is not zero, check op2 for zero
+	tst	r6,r6
+	bt	.L_op2_zero
+
+! r1 and r3 has masked out exponents, r9 and r10 has signs
+.L_add:
+	mov.l	.L_high_mant,r8
+	mov	#-20,r2
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r1		! r1 now has exponent for op1 in its lower bits
+#else
+	SHLR20 (r1)
+#endif
+	and	r8,r6	! Higher bits of mantissa of op2
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r3		! r3 has exponent for op2 in its lower bits
+#else
+	SHLR20 (r3)
+#endif
+	and	r8,r4	! Higher bits of mantissa of op1
+
+	mov.l	.L_21bit,r8
+
+	tst	r1,r1
+	bt	.L_norm_op1
+
+	! Set the 21st bit.
+	or	r8,r4
+	tst	r3,r3
+
+	bt	.L_norm_op2
+	or	r8,r6
+
+! Check for negative mantissas. Make them positive by negation
+! r9 and r10 have signs of op1 and op2 respectively
+.L_neg_mant:
+	tst	r9,r9
+	bf	.L_neg_op1
+
+	tst	r10,r10
+	bf	.L_neg_op2
+
+.L_add_1:
+	cmp/ge	r1,r3
+
+	mov	r1,r0
+	bt	.L_op2_exp_greater
+
+	sub	r3,r0
+	! If exponent difference is greater than 54, the resultant exponent
+	! won't be changed. Return op1 straight away.
+	mov	#54,r2
+	cmp/gt	r2,r0
+
+	bt	.L_pack_op1
+
+	mov	r1,r3
+	clrt
+
+	cmp/eq	#0,r0
+	bt	.L_add_mant
+
+	! Shift left the first operand and apply rest of shifts to second operand.
+	mov	#0,r2
+	shll	r5
+
+	rotcl	r4
+
+	add	#-1,r3
+	dt	r0
+
+	bt	.L_add_mant
+	dt	r0
+
+	bt	LOCAL(got_guard)
+	dt	r0
+
+	bt	LOCAL(got_sticky)
+
+! Shift the mantissa part of op2 so that both exponents are equal
+.L_shfrac_op2:
+	shar	r6
+	or	r7,r2	! sticky bit
+
+	rotcr	r7
+	dt	r0
+
+	bf	.L_shfrac_op2
+
+	shlr	r2
+
+	subc	r2,r2	! spread sticky bit across r2
+LOCAL(got_sticky):
+	shar	r6
+
+	rotcr	r7
+
+	rotcr	r2
+LOCAL(got_guard):
+	shar	r6
+
+	rotcr	r7
+
+	rotcr	r2
+
+
+! Add the psotive mantissas and check for overflow by checking the
+! MSB of the resultant. In case of overflow, negate the result.
+.L_add_mant:
+	clrt
+	addc	r7,r5
+
+	mov	#0,r10	! Assume resultant to be positive
+	addc	r6,r4
+
+	cmp/pz	r4
+
+	bt	.L_mant_ptv
+	negc	r2,r2
+
+	negc	r5,r5
+
+	mov.l	.L_sign,r10 ! The assumption was wrong, result is negative
+	negc	r4,r4
+
+! 23rd bit in the high part of mantissa could be set.
+! In this case, right shift the mantissa.
+.L_mant_ptv:
+	mov.l	.L_23bit,r0
+
+	tst	r4,r0
+	bt	.L_mant_ptv_0
+
+	shlr	r4
+	rotcr	r5
+
+	add	#1,r3
+	bra	.L_mant_ptv_1
+	rotcr	r2
+
+.L_mant_ptv_0:
+	mov.l	.L_22bit,r0
+	tst	r4,r0
+
+	bt	.L_norm_mant
+
+.L_mant_ptv_1:
+	! 22 bit of resultant mantissa is set. Shift right the mantissa
+	! and add 1 to exponent
+	add	#1,r3
+	shlr	r4
+	rotcr	r5
+	! The mantissa is already normalized. We don't need to
+	! spend any effort. Branch to epilogue. 
+	bra	.L_epil
+	rotcr	r2
+
+! Normalize operands
+.L_norm_op1:
+	shll	r5
+
+	rotcl	r4
+	add	#-1,r1
+
+	tst	r4,r8
+	bt	.L_norm_op1
+
+	tst	r3,r3
+	SL(bf,	.L_neg_mant,
+	 add	#1,r1)
+
+.L_norm_op2:
+	shll	r7
+
+	rotcl	r6
+	add	#-1,r3
+
+	tst	r6,r8
+	bt	.L_norm_op2
+
+	bra	.L_neg_mant
+	add	#1,r3
+
+! Negate the mantissa of op1
+.L_neg_op1:
+	clrt
+	negc	r5,r5
+
+	negc	r4,r4
+	tst	r10,r10
+
+	bt	.L_add_1
+
+! Negate the mantissa of op2
+.L_neg_op2:
+	clrt
+	negc	r7,r7
+
+	bra	.L_add_1
+	negc	r6,r6
+
+! Thread the jump to .L_inv_exp_op1
+.L_thread_inv_exp_op1:
+	bra	.L_inv_exp_op1
+	nop
+
+.L_ret_op2:
+	mov.l	@r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r6,r1
+#else
+	mov	r6,r0
+#endif
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r7,r0
+#else
+	mov	r7,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+.L_op1_nzero:
+	tst	r5,r5
+	bt	.L_ret_op2
+
+	! op1 is not zero. Check op2 for negative zero
+	cmp/eq	r6,r0
+	bf	.L_non_zero	! both op1 and op2 are not -0
+
+.L_op2_nzero:
+	tst	r7,r7
+	bf	.L_non_zero
+
+	mov.l	@r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+#else
+	mov	r4,r0	! op2 is -0, return op1
+#endif
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r5,r0
+#else
+	mov	r5,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+! High bit of op1 is known to be zero.
+! Check low bit. r2 contains 0x00000000
+.L_op1_zero:
+	tst	r5,r5
+	bt	.L_ret_op2
+
+	! op1 is not zero. Check high bit of op2
+	tst	r6,r6
+	bf	.L_add	! both op1 and op2 are not zero
+
+! op1 is not zero. High bit of op2 is known to be zero.
+! Check low bit of op2. r2 contains 0x00000000
+.L_op2_zero:
+	tst	r7,r7
+	bf	.L_add
+
+	mov.l	@r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+#else
+	mov	r4,r0	! op2 is zero, return op1
+#endif
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r5,r0
+#else
+	mov	r5,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+! exp (op1) is smaller or equal to exp (op2)
+! The logic of same operations is present in .L_add. Kindly refer it for
+! comments
+.L_op2_exp_greater:
+	mov	r3,r0
+	sub	r1,r0
+
+	mov	#54,r2
+	cmp/gt	r2,r0
+
+	bt	.L_pack_op2
+
+	cmp/eq	#0,r0
+	bt	.L_add_mant
+
+	mov	#0,r2
+	shll	r7
+	rotcl	r6
+	add	#-1,r0
+	add	#-1,r3
+
+	cmp/eq	#0,r0
+	bt	.L_add_mant
+.L_shfrac_op1:	
+        add     #-1,r0
+        shar    r4
+
+	rotcr	r5
+	rotcr	r2
+
+        cmp/eq  #0,r0
+        bf      .L_shfrac_op1
+
+	bra	.L_add_mant
+	nop
+
+! Return the value in op1
+.L_ret_op1:
+        mov.l   @r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+#else
+        mov     r4,r0
+#endif
+
+        mov.l   @r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r5,r0
+#else
+        mov     r5,r1
+#endif
+
+        rts
+        mov.l   @r15+,r8
+
+! r1 has exp, r9 has sign, r4 and r5 mantissa
+.L_pack_op1:
+	mov.l	.L_high_mant,r7
+	mov	r4,r0
+
+	tst	r9,r9
+	bt	.L_pack_op1_1
+
+	clrt
+	negc	r5,r5
+	negc	r0,r0
+
+.L_pack_op1_1:
+	and	r7,r0
+	mov	r1,r3
+
+	mov	#20,r2
+	mov	r5,r1
+
+	mov.l	@r15+,r10
+	or	r9,r0
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r3
+#else
+	SHLL20 (r3)
+#endif
+	mov.l	@r15+,r9
+
+	or	r3,r0
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+!r2 has exp, r10 has sign, r6 and r7 mantissa
+.L_pack_op2:
+	mov.l	.L_high_mant,r9
+	mov	r6,r0
+
+	tst	r10,r10
+	bt	.L_pack_op2_1
+
+	clrt
+	negc	r7,r7
+	negc	r0,r0
+
+.L_pack_op2_1:
+	and	r9,r0
+	mov	r7,r1
+
+	mov	#20,r2
+	or	r10,r0
+
+	mov.l	@r15+,r10
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r3
+#else
+	SHLL20 (r3)
+#endif
+
+	mov.l	@r15+,r9
+
+	or	r3,r0
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+! Normalize the mantissa by setting its 21 bit in high part
+.L_norm_mant:
+	mov.l	.L_21bit,r0
+
+	tst	r4,r0
+	bf	.L_epil
+
+	tst	r4,r4
+	bf	.L_shift_till_1
+
+	tst	r5,r5
+	bf	.L_shift_till_1
+
+	! Mantissa is zero, return 0
+	mov.l	@r15+,r10
+	mov	#0,r0
+
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+
+	rts
+	mov	#0,r1
+
+! A loop for making the 21st bit 1 in high part of resultant mantissa
+! It is already ensured that 1 bit is present in the mantissa
+.L_shift_till_1:
+	clrt
+	shll	r5
+
+	rotcl	r4
+	add	#-1,r3
+
+	tst	r4,r0
+	bt	.L_shift_till_1
+
+! Return the result. Mantissa is in r4-r5. Exponent is in r3
+! Sign bit in r10
+.L_epil:
+	cmp/pl	r3
+
+	bf	.L_denorm
+	mov.l	LOCAL(x7fffffff),r0
+
+	mov	r5,r1
+	shlr	r1
+
+	mov	#0,r1
+	addc	r0,r2
+
+! Check extra MSB here
+	mov.l	.L_22bit,r9
+	addc	r1,r5	! round to even
+
+	addc	r1,r4
+	tst	r9,r4
+
+	bf	.L_epil_1
+
+.L_epil_0:
+	mov.l	.L_21bit,r1
+
+	not	r1,r1
+	and	r1,r4
+
+	mov	r4,r0
+	or	r10,r0
+
+	mov.l	@r15+,r10
+	mov	#20,r2
+
+	mov.l	@r15+,r9
+	mov	r5,r1
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r3
+#else
+	SHLL20 (r3)
+#endif
+	or	r3,r0
+
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+.L_epil_1:
+	shlr	r4
+	add	#1,r3
+	bra	.L_epil_0
+	rotcr	r5
+
+.L_denorm:
+	add	#-1,r3
+.L_denorm_1:
+	tst	r3,r3
+	bt	.L_denorm_2
+
+	shlr	r4
+	rotcr	r5
+
+	movt	r1
+	bra	.L_denorm_1
+	add	#1,r3
+
+.L_denorm_2:
+	clrt
+	mov	#0,r2
+	addc	r1,r5
+
+	addc	r2,r4
+	mov	r4,r0
+
+	or	r10,r0
+	mov.l	@r15+,r10
+
+	mov	r5,r1
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+! op1 is known to be positive infinity, and op2 is Inf. The sign
+! of op2 is not known. Return the appropriate value
+.L_op1_pinf_op2_inf:
+	mov.l	.L_sign,r0
+	tst	r6,r0
+
+	bt	.L_ret_op2_1
+
+	! op2 is negative infinity. Inf - Inf is being performed
+	mov.l	.L_inf,r0
+	mov.l	@r15+,r10
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r1
+#endif
+	mov.l	@r15+,r8
+
+	rts
+#ifdef	__LITTLE_ENDIAN__
+	mov	#1,r0
+#else
+	mov	#1,r1	! Any value here will return Nan
+#endif
+	
+.L_ret_op1_1:
+        mov.l   @r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+#else
+        mov     r4,r0
+#endif
+
+        mov.l   @r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r5,r0
+#else
+        mov     r5,r1
+#endif
+
+        rts
+        mov.l   @r15+,r8
+
+.L_ret_op2_1:
+	mov.l	@r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r6,r1
+#else
+	mov	r6,r0
+#endif
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r7,r0
+#else
+	mov	r7,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+! op1 is negative infinity. Check op2 for infinity or Nan
+.L_op1_ninf:
+	cmp/eq	r2,r3
+	bf	.L_ret_op1_1	! op2 is neither Nan nor Inf
+
+	mov.l	@r15+,r9
+	div0s	r4,r6		! different signs -> NaN
+	mov	r4,DBLRH
+	or	r6,DBLRH
+	mov.l	@r15+,r8
+	SL(bf, 0f,
+	 mov	r5,DBLRL)
+	mov	#-1,DBLRH	! return NaN.
+0:	rts
+	or	r7,DBLRL
+
+!r1 contains exponent for op1, r3 contains exponent for op2
+!r2 has .L_inf (+ve Inf)
+!op1 has invalid exponent. Either it contains Nan or Inf
+.L_inv_exp_op1:
+	! Check if a is Nan
+	cmp/pl	r5
+	bt	.L_ret_op1_1
+
+	mov.l	.L_high_mant,r0
+	and	r4,r0
+
+	cmp/pl	r0
+	bt	.L_ret_op1_1
+
+	! op1 is not Nan. It is infinity. Check the sign of it.
+	! If op2 is Nan, return op2
+	cmp/pz	r4
+
+	bf	.L_op1_ninf
+
+	! op2 is +ve infinity here
+	cmp/eq	r2,r3
+	bf	.L_ret_op1_1	! op2 is neither Nan nor Inf
+
+	! r2 is free now
+	mov.l	.L_high_mant,r0
+	tst	r6,r0		! op2 also has invalid exponent
+
+	bf	.L_ret_op2_1	! op2 is Infinity, and op1 is +Infinity
+
+	tst	r7,r7
+	bt	.L_op1_pinf_op2_inf	! op2 is Infinity, and op1 is +Infinity
+	!op2 is not infinity, It is Nan
+	bf	.L_ret_op2_1
+
+	.align 2	
+.L_high_mant:
+	.long 0x000FFFFF
+
+.L_21bits:
+	.long 0x001FFFFF
+
+.L_22bit:
+	.long 0x00200000
+
+.L_23bit:
+	.long 0x00400000
+
+.L_21bit:
+	.long 0x00100000
+
+.L_sign:
+	.long 0x80000000
+
+.L_inf:
+	.long 0x7ff00000
+
+LOCAL(x7fffffff): .long 0x7fffffff
+
+ENDFUNC (GLOBAL (subdf3))
+ENDFUNC (GLOBAL (adddf3))
Index: gcc/config/sh/IEEE-754/mulsf3.S
===================================================================
--- gcc/config/sh/IEEE-754/mulsf3.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/mulsf3.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,352 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Routine for multiplying two floating point numbers
+
+! Author: Rakesh Kumar
+
+! Arguments: r4 and r5
+! Result: r0
+
+! The arguments are referred as op1 and op2
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (mulsf3)
+        FUNC (GLOBAL (mulsf3))
+
+GLOBAL (mulsf3):
+	! Extract the sign bits
+	mov.l	.L_sign,r3
+	mov	r3,r0
+
+	and	r4,r3		! sign bit for op1
+	mov.l	.L_sign_mask,r6
+
+	! Mask out the sign bit from op1 and op2
+	and	r5,r0		! sign bit for op2
+	mov.l	.L_inf,r2
+
+	and	r6,r4
+	xor	r3,r0		! Final sign in r0
+
+	and	r6,r5
+	tst	r4,r4
+
+	! Check for zero
+	mov	r5,r7
+	! Check op1 for zero
+	SL(bt,	.L_op1_zero,
+	 mov	r4,r6)
+
+	tst	r5,r5
+	bt	.L_op2_zero	! op2 is zero
+
+	! Extract the exponents
+	and	r2,r6		! Exponent of op1
+	cmp/eq	r2,r6
+
+	and	r2,r7
+	bt	.L_inv_op1	! op1 is NaN or Inf
+
+	mov.l	.L_mant,r3
+	cmp/eq	r2,r7
+
+	and	r3,r4	! Mantissa of op1
+	bt	.L_ret_op2	! op2 is Nan or Inf
+
+	and	r3,r5	! Mantissa of op2
+
+	mov	#-23,r3
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLR23 (r6)
+	SHLR23 (r7)
+#else
+	shld	r3,r6
+	shld	r3,r7
+#endif
+	! Check for denormals
+	mov.l	.L_24bit,r3
+	tst	r6,r6
+
+	bt	.L_norm_op1	! op1 is denormal
+	add	#-127,r6	! Unbias op1's exp
+
+	tst	r7,r7
+	bt	.L_norm_op2	! op2 is denormal
+
+	add	#-127,r7	! Unbias op2's exp
+
+.L_multiply:
+	add	r6,r7	! Final exponent in r7
+	mov.l	.L_24bit,r1
+
+	! set 24th bit of mantissas
+	mov	#127,r3
+	or	r1,r4
+
+	DMULU_SAVE
+
+	! Multiply
+	or	r1,r5
+	DMULUL	(r4,r5,r4)
+
+	DMULUH	(r5)
+
+	DMULU_RESTORE
+
+	mov.l	.L_16bit,r6
+
+	! Check for extra MSB generated
+	tst	r5,r6
+
+	mov.l	.L_255,r1
+	bf	.L_shift_by_1	! Adjust the extra MSB
+	
+! Normalize the result with rounding
+.L_epil:
+	! Bias the exponent
+	add	#127,r7
+	cmp/ge	r1,r7
+	
+	! Check exponent overflow and underflow
+	bt	.L_ret_inf
+
+	cmp/pl	r7
+	bf	.L_denorm
+
+.L_epil_0:
+	mov	#-23,r3
+	shll	r5
+	mov	#0,r6
+
+! Fit resultant mantissa in 24 bits
+! Apply default rounding
+.L_loop_epil_0:
+        tst	r3,r3
+	bt	.L_loop_epil_out
+
+	add	#1,r3
+	shlr	r4
+
+	bra	.L_loop_epil_0
+	rotcr	r6
+
+! Round mantissa
+.L_loop_epil_out:
+	shll8	r5
+	or	r5,r4
+
+	mov.l	.L_mant,r2
+	mov	#23,r3
+
+	! Check last bit shifted out of result
+	tst	r6,r6
+	bt	.L_epil_2
+
+	! Round
+	shll	r6
+	movt	r5
+
+	add	r5,r4
+
+	! If this is the only ON bit shifted
+	! Round towards LSB = 0
+	tst	r6,r6
+	bf	.L_epil_2
+
+	shlr	r4
+	shll	r4
+
+.L_epil_2:
+	! Rounding may have produced extra MSB.
+	mov.l	.L_25bit,r5
+	tst	r4,r5
+
+	bt	.L_epil_1
+
+	add	#1,r7
+	shlr	r4
+
+.L_epil_1:
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r7)
+#else
+	shld	r3,r7
+#endif
+
+	and	r2,r4
+
+	or	r7,r4
+	rts
+	or	r4,r0
+
+.L_denorm:
+	mov	#0,r3
+
+.L_den_1:
+	shlr	r5
+	rotcr	r4
+
+	cmp/eq	r3,r7
+	bt	.L_epil_0
+
+	bra	.L_den_1
+	add	#1,r7
+	
+
+! Normalize the first argument
+.L_norm_op1:
+	shll	r4
+	tst	r3,r4
+
+	add	#-1,r6
+	bt	.L_norm_op1
+
+	! The biasing is by 126
+	add	#-126,r6
+	tst	r7,r7
+
+	bt      .L_norm_op2
+
+	bra	.L_multiply
+	add	#-127,r7
+
+! Normalize the second argument
+.L_norm_op2:
+	shll	r5
+	tst	r3,r5
+
+	add	#-1,r7
+	bt	.L_norm_op2
+
+	bra	.L_multiply
+	add	#-126,r7
+
+! op2 is zero. Check op1 for exceptional cases
+.L_op2_zero:
+	mov.l	.L_inf,r2
+	and	r2,r6
+
+	! Check if op1 is deterministic
+	cmp/eq	r2,r6
+	SL(bf,	.L_ret_op2,
+	 mov	#1,r1)
+
+	! Return NaN
+	rts
+	mov	#-1,r0
+
+! Adjust the extra MSB
+.L_shift_by_1:
+	shlr	r5
+	rotcr	r4
+
+	add	#1,r7		! Show the shift in exponent
+
+	cmp/gt	r3,r7
+	bf	.L_epil
+
+	! The resultant exponent is invalid
+	mov.l	.L_inf,r1
+	rts
+	or	r1,r0
+
+.L_ret_op1:
+	rts
+	or	r4,r0
+
+! op1 is zero. Check op2 for exceptional cases
+.L_op1_zero:
+	mov.l	.L_inf,r2
+	and	r2,r7
+	
+	! Check if op2 is deterministic
+	cmp/eq	r2,r7
+	SL(bf,	.L_ret_op1,
+	 mov	#1,r1)
+
+	! Return NaN
+	rts
+	mov	#-1,r0
+
+.L_inv_op1:
+	mov.l	.L_mant,r3
+	mov	r4,r6
+
+	and	r3,r6
+	tst	r6,r6
+
+	bf	.L_ret_op1	! op1 is Nan
+	! op1 is not Nan. It is Inf
+
+	cmp/eq	r2,r7
+	bf	.L_ret_op1	! op2 has a valid exponent
+
+! op2 has a invalid exponent. It could be Inf, -Inf, Nan.
+! It doesn't make any difference.
+.L_ret_op2:
+	rts
+	or	r5,r0
+
+.L_ret_inf:
+	rts
+	or	r2,r0
+
+.L_ret_zero:
+	mov	#0,r2
+	rts
+	or	r2,r0
+
+	
+	.align 2
+.L_mant:
+	.long 0x007FFFFF
+
+.L_inf:
+	.long 0x7F800000
+
+.L_24bit:
+	.long 0x00800000
+
+.L_25bit:
+	.long 0x01000000
+
+.L_16bit:
+	.long 0x00008000
+
+.L_sign:
+	.long 0x80000000
+
+.L_sign_mask:
+	.long 0x7FFFFFFF
+
+.L_255:
+	.long 0x000000FF
+
+ENDFUNC (GLOBAL (mulsf3))
Index: gcc/config/sh/IEEE-754/floatsisf.S
===================================================================
--- gcc/config/sh/IEEE-754/floatsisf.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/floatsisf.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,200 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Conversion of integer to floating point
+
+! Author: Rakesh Kumar
+
+! Argument: r4
+! Result: r0
+
+! r4 is referred as op1
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (floatsisf)
+        FUNC (GLOBAL (floatsisf))
+
+GLOBAL (floatsisf):
+	mov.l	.L_sign,r2
+	mov	#23,r6
+
+	! Check for zero
+	tst	r4,r4
+	mov.l	.L_24_bits,r7
+
+	! Extract sign
+	and	r4,r2
+	bt	.L_ret
+
+	! Negative ???
+	mov.l	.L_imp_bit,r5
+	cmp/pl	r4
+
+	not	r7,r3
+	bf	.L_neg
+
+	! Decide the direction for shifting
+	cmp/gt	r7,r4
+	mov	r4,r0
+
+	and	r5,r0
+	bt	.L_shr_0
+
+	! Number may already be in normalized form
+	cmp/eq	#0,r0
+	bf	.L_pack
+
+! Shift the bits to the left. Adjust the exponent
+.L_shl:
+	shll	r4
+	mov	r4,r0
+
+	and	r5,r0
+	cmp/eq	#0,r0
+
+	SL(bt,	.L_shl,
+	 add	#-1,r6)
+
+! Pack the value in floating point format.
+! r6 has unbiased exponent, r4 has mantissa, r2 has sign
+.L_pack:
+	mov	#23,r3
+	not	r5,r5
+
+	mov	r2,r0
+	add	#127,r6
+
+	and	r5,r4
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r6)
+#else
+	shld	r3,r6
+#endif
+
+	or	r6,r0
+	rts
+	or	r4,r0
+
+! Negate the number
+.L_neg:
+	! Take care for -2147483648.
+	mov	r4,r0
+	shll	r0
+	
+	cmp/eq	#0,r0
+	SL(bt,	.L_ret_min,
+	 neg	r4,r4)
+
+        cmp/gt  r7,r4
+        bt	.L_shr_0
+
+	mov	r4,r0
+	and	r5,r0
+
+	cmp/eq	#0,r0
+	bf	.L_pack
+	bt	.L_shl
+	
+.L_shr_0:
+	mov	#0,r1
+
+! Shift right the number with rounding
+.L_shr:
+	shlr	r4
+	movt	r7
+
+	tst	r7,r7
+
+	! Count number of ON bits shifted
+	bt	.L_shr_1
+	add	#1,r1
+
+.L_shr_1:
+	mov	r4,r0
+	add	#1,r6
+
+	and	r3,r0
+	cmp/eq	#0,r0
+
+	! Add MSB of shifted bits
+	bf	.L_shr
+	add	r7,r4
+
+	tst	r7,r7
+	bt	.L_pack
+
+.L_pack1:
+	mov	#1,r0
+	cmp/eq	r1,r0
+
+	bt	.L_rnd
+	mov	r4,r0
+
+	! Rounding may have misplaced MSB. Adjust.
+	and	r3,r0
+	cmp/eq	#0,r0
+
+	bf	.L_shr
+	bt	.L_pack
+
+! If only MSB of shifted bits is ON, we are halfway
+! between two numbers. Round towards even LSB of
+! resultant mantissa.
+.L_rnd:
+	shlr	r4
+	bra	.L_pack
+	shll	r4
+
+.L_ret:
+	rts
+	mov	r4,r0
+
+! Return value for -2147483648
+.L_ret_min:
+	mov.l	.L_min_val,r0
+	rts
+	nop
+
+	.align 2
+.L_sign:
+	.long 0x80000000
+
+.L_imp_bit:
+	.long 0x00800000
+
+.L_24_bits:
+	.long 0x00FFFFFF
+
+.L_nsign:
+	.long 0x7FFFFFFF
+
+.L_min_val:
+	.long 0xCF000000
+
+ENDFUNC (GLOBAL (floatsisf))
Index: gcc/config/sh/IEEE-754/muldf3.S
===================================================================
--- gcc/config/sh/IEEE-754/muldf3.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/muldf3.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,601 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!multiplication of two double precision floating point numbers
+!Author:Aanchal Khanna
+!SH1 Support / Simplifications: Joern Rennecke
+!
+!Entry:
+!r4,r5:operand 1
+!
+!r6,r7:operand 2
+!
+!Exit:
+!r0,r1:result
+!
+!Notes: argument 1 is passed in regs r4 and r5 and argument 2 is passed in regs
+!r6 and r7, result is returned in regs r0 and r1. operand 1 is referred as op1
+!and operand 2 as op2.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+	.text
+	.align	5	
+	.global	GLOBAL (muldf3)
+	FUNC (GLOBAL (muldf3))
+
+GLOBAL (muldf3):
+
+#ifdef  __LITTLE_ENDIAN__
+        mov     r4,r1
+        mov     r5,r4
+        mov     r1,r5
+
+        mov     r6,r1
+        mov     r7,r6
+        mov     r1,r7
+#endif
+	mov.l	.L_mask_sign,r0
+	mov	r4,r2
+
+	and	r0,r2		
+	mov	#0,r1
+
+	shll	r4
+	and	r6,r0		
+	
+	xor     r2,r0		!r0 contains the result's sign bit
+	shlr	r4
+
+	mov.l   .L_inf,r2
+	shll	r6
+
+	mov	r4,r3
+	shlr	r6
+	
+.L_chk_a_inv:
+	!chk if op1 is Inf/NaN
+	and	r2,r3
+	mov.l	r8,@-r15
+
+	cmp/eq	r3,r2
+	mov.l	.L_mask_high_mant,r8
+
+	mov	r2,r3
+	bf	.L_chk_b_inv
+
+	mov	r8,r3
+	and	r4,r8
+
+	cmp/hi  r1,r8		
+	bt	.L_return_a	!op1 NaN, return op1
+
+	cmp/hi  r1,r5	
+	mov	r2,r8
+
+	bt      .L_return_a	!op1 NaN, return op1
+	and	r6,r8
+
+	cmp/eq	r8,r2		
+	and	r6,r3
+
+	bt      .L_b_inv
+	cmp/eq	r1,r6		
+
+	bf	.L_return_a	!op1 Inf,op2= normal no return op1
+	cmp/eq	r1,r7
+
+	bf	.L_return_a	!op1 Inf,op2= normal no return op1
+	mov.l   @r15+,r8	
+
+	rts
+	mov	#-1,DBLRH	!op1=Inf, op2=0,return nan
+
+.L_b_inv:
+	!op2 is NaN/Inf
+	cmp/hi	r1,r7
+	mov	r1,r2
+
+	mov	r5,r1
+	bt	.L_return_b	!op2=NaN,return op2
+
+	cmp/hi	r2,r6
+	or	r4,r0
+
+	bt	.L_return_b	!op2=NaN,return op2
+	mov.l   @r15+,r8
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts			!op1=Inf,op2=Inf,return Inf with sign
+	nop
+
+.L_chk_b_inv:
+	!Chk if op2 is NaN/Inf
+	and	r6,r2
+	cmp/eq	r3,r2
+
+	bf	.L_chk_a_for_zero
+	and	r6,r8
+
+	cmp/hi	r1,r8
+	bt	.L_return_b	 !op2=NaN,return op2
+
+	cmp/hi	r1,r7
+	bt	.L_return_b	 !op2=NaN,return op2
+
+	cmp/eq	r5,r1
+	bf      .L_return_b	 !op1=normal number,op2=Inf,return Inf
+
+	mov	r7,r1
+	cmp/eq	r4,r1
+
+	bf	.L_return_b	/* op1=normal number, op2=Inf,return Inf */
+	mov.l   @r15+,r8
+
+	rts
+	mov	#-1,DBLRH	!op1=0,op2=Inf,return NaN
+
+.L_return_a:
+	mov	r5,r1
+	or	r4,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	mov.l   @r15+,r8
+
+.L_return_b:
+	mov	r7,r1
+	or	r6,r0	
+	
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+	
+.L_chk_a_for_zero:
+	!Chk if op1 is zero
+	cmp/eq	r1,r4
+	bf	.L_chk_b_for_zero
+	
+	cmp/eq	r1,r5
+	bf	.L_chk_b_for_zero
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+.L_chk_b_for_zero:
+	!op1=0,chk if op2 is zero
+        cmp/eq  r1,r6
+        mov	r1,r3
+	
+	mov.l   .L_inf,r1
+	bf      .L_normal_nos
+
+        cmp/eq  r3,r7
+        bf      .L_normal_nos
+
+	mov	r3,r1
+	mov.l   @r15+,r8
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	nop
+
+.L_normal_nos:
+	!op1 and op2 are normal nos
+	mov.l	r9,@-r15
+	mov	r4,r3
+
+	mov     #-20,r9	
+	and	r1,r3	
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r9,r2
+#else
+        SHLR20 (r2)
+#endif
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r9,r3
+#else
+        SHLR20 (r3)
+#endif
+	cmp/pl	r3
+
+	bf	.L_norm_a	!normalize op1
+.L_chk_b:	
+	cmp/pl	r2
+	bf	.L_norm_b	!normalize op2
+
+.L_mul1:
+	add	r3,r2
+	mov.l  .L_1023,r1
+	
+	!resultant exponent in r2
+	add     r1,r2
+	mov.l   .L_2047,r1	
+
+	!Chk the exponent for overflow
+	cmp/ge	r1,r2
+	and     r8,r4
+
+	bt	.L_return_inf
+	mov.l	.L_imp_bit,r1
+	
+	or	r1,r4		
+	and	r8,r6
+
+	or	r1,r6
+	clrt
+
+	!multiplying the mantissas
+	DMULU_SAVE
+	DMULUL	(r7,r5,r1) 	!bits 0-31 of product 	
+
+	DMULUH	(r3)
+	
+	DMULUL	(r4,r7,r8)
+
+	addc	r3,r8
+
+	DMULUH	(r3)
+
+	movt	r9
+	clrt
+
+	DMULUL	(r5,r6,r7)
+
+	addc	r7,r8		!bits 63-32 of product
+
+	movt	r7
+	add	r7,r9
+
+	DMULUH	(r7)
+
+	add	r7,r3
+
+	add	r9,r3
+	clrt
+
+	DMULUL	(r4,r6,r7)
+
+	addc	r7,r3		!bits 64-95 of product
+
+	DMULUH	(r7)
+	DMULU_RESTORE
+	
+	mov	#0,r5
+	addc	r5,r7		!bits 96-105 of product
+
+	cmp/eq	r5,r1
+	mov     #1,r4
+
+	bt	.L_skip
+	or	r4,r8
+.L_skip:
+	mov.l   .L_106_bit,r4
+	mov	r8,r9
+
+.L_chk_extra_msb:
+	!chk if exra MSB is generated
+	and     r7,r4
+	cmp/eq	r5,r4
+
+	mov     #12,r4
+	SL(bf,	.L_shift_rt_by_1,
+	 mov     #31,r5)
+	
+.L_pack_mantissa:
+	!scale the mantissa t0 53 bits
+	mov	#-19,r6
+	mov.l	.L_mask_high_mant,r5
+
+        SHLRN (19, r6, r8)
+
+	and	r3,r5
+
+	shlr	r8
+	movt	r1
+
+        SHLLN (12, r4, r5)
+
+	add	#-1,r6
+
+	or	r5,r8		!lower bits of resulting mantissa
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r6,r3
+#else
+        SHLR20 (r3)
+#endif
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r4,r7
+#else
+        SHLL12 (r7)
+#endif
+	clrt
+
+	or	r7,r3		!higher bits of resulting mantissa
+	mov     #0,r7
+
+	!chk the exponent for underflow
+	cmp/ge	r2,r7
+	bt	.L_underflow
+
+	addc    r1,r8           !rounding
+	mov	r8,r1
+
+	addc	r7,r3		!rounding
+	mov.l	.L_mask_22_bit,r5
+
+	and	r3,r5
+	!chk if extra msb is generated after rounding
+	cmp/eq	r7,r5
+
+	mov.l	.L_mask_high_mant,r8
+	bt	.L_pack_result
+
+	add	#1,r2
+	mov.l	.L_2047,r6
+
+	cmp/ge	r6,r2
+
+	bt	.L_return_inf
+	shlr	r3
+
+	rotcr	r1
+
+.L_pack_result:
+	!pack the result, r2=exponent, r3=higher mantissa, r1=lower mantissa
+	!r0=sign bit
+	mov	#20,r6
+	and	r8,r3
+	
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r6,r2
+#else
+        SHLL20 (r2)
+#endif
+	or	r3,r0
+	
+	or      r2,r0
+	mov.l   @r15+,r9
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+
+.L_norm_a:
+	!normalize op1
+	shll	r5
+	mov.l	.L_imp_bit,r1
+
+	rotcl	r4
+	add	#-1,r3
+
+	tst	r1,r4
+	bt	.L_norm_a
+
+	bra	.L_chk_b
+	add	#1,r3
+
+.L_norm_b:
+	!normalize op2
+        shll    r7
+        mov.l   .L_imp_bit,r1
+
+        rotcl   r6
+        add     #-1,r2
+
+        tst     r1,r6
+        bt      .L_norm_b
+
+        bra     .L_mul1
+        add     #1,r2
+
+.L_shift_rt_by_1:
+	!adjust the extra msb
+
+	add     #1,r2           !add 1 to exponent
+	mov.l	.L_2047,r6
+
+	cmp/ge	r6,r2
+	mov	#20,r6
+
+	bt	.L_return_inf
+	shlr	r7		!r7 contains bit 96-105 of product
+
+	rotcr	r3		!r3 contains bit 64-95 of product
+
+	rotcr	r8		!r8 contains bit 32-63 of product
+	bra	.L_pack_mantissa
+
+	rotcr	r1		!r1 contains bit 31-0 of product
+
+.L_return_inf:
+	!return Inf
+	mov.l	.L_inf,r2
+	mov     #0,r1
+
+	or	r2,r0
+	mov.l   @r15+,r9
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+	
+.L_underflow:
+	!check if the result needs to be denormalized
+	mov	#-53,r1
+	add	#1,r2
+
+	cmp/gt	r2,r1
+	mov	#32,r4
+
+	add	#-2,r2
+	bt	.L_return_zero
+
+	add	r2,r4
+	mov	r7,r1
+	
+	cmp/ge	r7,r4
+	mov	r2,r6
+
+	mov	#-54,r2
+	bt	.L_denorm
+
+	mov	#-32,r6
+	
+.L_denorm:
+	!denormalize the result
+	shlr	r8
+	rotcr	r1	
+
+	shll	r8
+	add	#1,r6
+
+	shlr	r3
+	rotcr	r8
+
+	cmp/eq	r7,r6
+	bf	.L_denorm
+
+	mov	r4,r6
+	cmp/eq	r2,r4
+
+	bt	.L_break
+	mov	r7,r5
+
+	cmp/gt	r6,r7
+	bf	.L_break
+
+	mov	r2,r4
+	mov	r1,r5
+
+	mov	r7,r1
+	bt	.L_denorm
+
+.L_break:
+	mov	#0,r2
+
+	cmp/gt	r1,r2
+
+	addc	r2,r8
+	mov.l	.L_comp_1,r4
+	
+	addc	r7,r3
+	or	r3,r0
+
+	cmp/eq	r9,r7
+	bf	.L_return
+
+	cmp/eq	r7,r5
+	mov.l	.L_mask_sign,r6
+
+	bf	.L_return
+	cmp/eq	r1,r6
+	
+	bf	.L_return
+	and	r4,r8
+
+.L_return:
+	mov.l	@r15+,r9
+	mov	r8,r1
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+
+.L_return_zero:
+	mov.l	@r15+,r9
+	mov	r7,r1
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+	.align	2
+
+.L_mask_high_mant:
+	.long	0x000fffff
+.L_inf:
+	.long	0x7ff00000	
+.L_mask_sign:
+	.long	0x80000000
+.L_1023:
+	.long	-1023
+.L_2047:
+	.long	2047
+.L_imp_bit:
+	.long	0x00100000
+.L_mask_22_bit:
+	.long	0x00200000
+.L_106_bit:
+	.long	0x00000200
+.L_comp_1:
+	.long	0xfffffffe
+
+ENDFUNC (GLOBAL (muldf3))
Index: gcc/config/sh/IEEE-754/fixsfsi.S
===================================================================
--- gcc/config/sh/IEEE-754/fixsfsi.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/fixsfsi.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,165 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Conversion routine for float to integer
+
+! Author: Rakesh Kumar
+
+! Arguments: r4 (in floating point format)
+! Return: r0
+
+! r4 is referred as op1
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 5
+	.global	GLOBAL (fixsfsi)
+	FUNC (GLOBAL (fixsfsi))
+
+GLOBAL (fixsfsi):
+	mov.l	.L_mask_sign,r7
+	mov	r4,r2
+
+	! Check for NaN
+	mov.l	.L_inf,r1
+	and	r7,r2
+
+	cmp/gt	r1,r2
+	mov	#127,r5
+
+	mov	r4,r3
+	SL(bt,	.L_epil,
+	 mov	#0,r0)
+
+	shll	r2
+	mov.l	.L_frac,r6
+
+	shlr16	r2
+	and	r6,r3	! r3 has fraction
+
+	shlr8	r2	! r2 has exponent
+	mov.l	.L_24bit,r1
+
+	! If exponent is less than 127, return 0
+	cmp/gt	r2,r5
+	or	r1,r3	! Set the implicit bit
+
+	mov.l	.L_157,r1
+	SL1(bt,	.L_epil,
+	 shll8	r3)
+
+	! If exponent is greater than 157,
+	! return the maximum/minumum integer
+	! value deducing from sign
+	cmp/gt	r1,r2
+	sub	r2,r1
+
+	mov.l	.L_sign,r2
+	SL(bt,	.L_ret_max,
+	 add	#1,r1)
+
+	and	r4,r2	! Sign in r2
+	neg	r1,r1
+
+	! Shift mantissa by exponent difference from 157
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r1,r3
+#else
+        cmp/gt  r0,r1
+        bt      .L_mov_left
+
+.L_mov_right:
+        cmp/eq  r1,r0
+        bt      .L_ret
+
+        add     #1,r1
+        bra     .L_mov_right
+
+        shlr    r3
+
+.L_mov_left:
+        add     #-1,r1
+
+        shll    r3
+        cmp/eq  r1,r0
+
+        bf      .L_mov_left
+.L_ret:
+#endif
+	! If op1 is negative, negate the result
+	cmp/eq	r0,r2
+	SL(bf,	.L_negate,
+	 mov	r3,r0)
+
+! r0 has the appropriate value
+.L_epil:
+	rts
+	nop
+
+! Return the max/min integer value
+.L_ret_max:
+	and	r4,r2	! Sign in r2
+	mov.l	.L_max,r3
+
+	mov.l	.L_sign,r1
+	cmp/eq	r0,r2
+
+	mov	r3,r0
+	bt	.L_epil
+
+	! Negative number, return min int
+	rts
+	mov	r1,r0
+
+! Negate the result
+.L_negate:
+	rts
+	neg	r0,r0
+
+	.align 2
+.L_inf:
+	.long 0x7F800000
+
+.L_157:
+	.long 157
+
+.L_max:
+	.long 0x7FFFFFFF
+
+.L_frac:
+	.long 0x007FFFFF
+
+.L_sign:
+	.long 0x80000000
+
+.L_24bit:
+	.long 0x00800000
+
+.L_mask_sign:
+	.long 0x7FFFFFFF
+
+ENDFUNC (GLOBAL (fixsfsi))
Index: gcc/config/sh/IEEE-754/floatsidf.S
===================================================================
--- gcc/config/sh/IEEE-754/floatsidf.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/floatsidf.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,151 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!conversion of signed integer to double precision floating point number
+!Author:Rakesh Kumar
+!
+!Entry:
+!r4:operand 
+!
+!Exit:
+!r0,r1:result
+!
+!Note:argument is passed in reg r4 and the result is returned in 
+!regs r0 and r1.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (floatsidf)
+	FUNC (GLOBAL (floatsidf))
+
+GLOBAL (floatsidf):
+        mov.l   .L_sign,r0
+        mov     #0,r1
+
+	mov	r0,r2
+	tst	r4,r4 ! check r4 for zero
+
+	! Extract the sign
+	mov	r2,r3
+	SL(bt,	.L_ret_zero,
+	 and	r4,r0)
+
+	cmp/eq	r1,r0
+	not	r3,r3
+
+	mov	r1,r7
+	SL(bt,	.L_loop,
+	 and	r4,r3)
+
+	! Treat -2147483648 as special case
+	cmp/eq	r1,r3
+	neg	r4,r4
+
+	bt	.L_ret_min	
+
+.L_loop:
+	shll	r4	
+	mov	r4,r5
+
+	and	r2,r5
+	cmp/eq	r1,r5
+	
+	add	#1,r7
+	bt	.L_loop
+
+	mov.l	.L_initial_exp,r6
+	not	r2,r2
+	
+	and	r2,r4
+	mov	#21,r3
+
+	sub	r7,r6
+	mov	r4,r1
+
+	mov	#20,r7
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r3,r1
+#else
+        SHLL21 (r1)
+#endif
+	mov	#-11,r2
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r6	! Exponent in proper place
+#else
+        SHLL20 (r6)
+#endif
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r2,r4
+#else
+        SHLR11 (r4)
+#endif
+	or	r6,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+#ifdef __LITTLE_ENDIAN__
+	or	r4,r1
+#else
+	or	r4,r0
+#endif
+	
+.L_ret_zero:
+	rts
+	mov	#0,r0
+
+.L_ret_min:
+	mov.l	.L_min,r0
+	
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	nop
+
+	.align 2
+
+.L_initial_exp:
+	.long 0x0000041E
+
+.L_sign:
+	.long 0x80000000
+
+.L_min:
+	.long 0xC1E00000
+
+ENDFUNC (GLOBAL (floatsidf))
Index: gcc/config/sh/IEEE-754/fixdfsi.S
===================================================================
--- gcc/config/sh/IEEE-754/fixdfsi.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/IEEE-754/fixdfsi.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,200 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!conversion of double precision floating point number to signed integer
+!Author:Aanchal Khanna
+!
+!Entry:
+!r4,r5:operand
+!
+!Exit:
+!r0:result
+!
+!Note:argument is passed in regs r4 and r5, the result is returned in
+!reg r0.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 	5
+	.global GLOBAL (fixdfsi)
+	FUNC (GLOBAL (fixdfsi))
+
+GLOBAL (fixdfsi):
+
+#ifdef  __LITTLE_ENDIAN__
+        mov     r4,r1
+        mov     r5,r4
+        mov     r1,r5
+
+#endif
+	mov.l	.L_p_inf,r2
+	mov     #-20,r1
+	
+	mov	r2,r7
+	mov.l   .L_1023,r3
+
+	and	r4,r2
+	shll    r4
+        
+	movt    r6		! r6 contains the sign bit
+	
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r2		! r2 contains the exponent
+#else
+        SHLR20 (r2)
+#endif
+	 shlr    r4
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r7
+#else
+        SHLR20 (r7)
+#endif
+	cmp/hi	r2,r3		! if exp < 1023,return 0
+	mov.l	.L_mask_high_mant,r1
+
+	SL(bt,	.L_epil,
+	 mov	#0,r0)
+	and	r4,r1		! r1 contains high mantissa
+
+	cmp/eq	r2,r7		! chk if exp is invalid
+	mov.l	.L_1053,r7
+
+	bt	.L_inv_exp
+	mov	#11,r0
+	
+	cmp/hi	r7,r2		! If exp > 1053,return maxint
+	sub     r2,r7
+
+	mov.l	.L_21bit,r2
+	SL(bt,	.L_ret_max,
+	 add	#1,r7)		! r7 contains the number of shifts
+
+	or	r2,r1
+	mov	r7,r3
+	shll8   r1
+
+	neg     r7,r7
+	shll2	r1
+
+        shll	r1
+	cmp/hi	r3,r0
+
+	!chk if the result can be made only from higher mantissa
+	SL(bt,	.L_lower_mantissa,
+	 mov	#21,r0)
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r1
+#else
+.L_loop:
+        tst	r7,r7
+        bt      .L_break1
+        add     #1,r7
+        bra     .L_loop
+        shlr    r1
+
+.L_break1:
+#endif
+	tst	r6,r6
+	SL(bt,	.L_epil,
+	 mov	r1,r0)
+
+	rts
+	neg	r0,r0
+
+.L_lower_mantissa:
+	!result is made from lower mantissa also
+	neg	r0,r0
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r0,r5
+#else
+        SHLR21 (r5)
+#endif
+
+	or	r5,r1		!pack lower and higher mantissas
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r1
+#else
+.L_sh_loop:
+	tst	r7,r7
+	bt	.L_break
+	add	#1,r7
+	bra	.L_sh_loop
+	shlr	r1
+
+.L_break:
+#endif
+	mov	r1,r0
+	bra	.L_chk_sign
+	nop
+
+.L_epil:
+	rts
+	nop
+
+.L_inv_exp:
+	cmp/hi	r0,r5
+	bt	.L_epil
+
+	cmp/hi	r0,r1		!compare high mantissa,r1
+	bt	.L_epil
+
+.L_ret_max:
+	mov.l   .L_maxint,r0
+	tst	r6,r6
+	bt	.L_epil
+
+	rts
+	add	#1,r0
+
+.L_chk_sign:
+	tst	r6,r6		!sign bit is set, number is -ve
+	bt	.L_epil
+	
+	rts
+	neg	r0,r0
+
+	.align	2
+
+.L_maxint:
+	.long	0x7fffffff
+.L_p_inf:
+	.long	0x7ff00000
+.L_mask_high_mant:
+	.long	0x000fffff
+.L_1023:
+	.long	0x000003ff
+.L_1053:
+	.long	1053
+.L_21bit:
+	.long	0x00100000
+
+ENDFUNC (GLOBAL (fixdfsi))
Index: gcc/config/sh/divtab-sh4-300.c
===================================================================
--- gcc/config/sh/divtab-sh4-300.c	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/divtab-sh4-300.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,82 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+   Copyright (C) 2006 STMicroelectronics
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 59 Temple Place - Suite 330,
+Boston, MA 02111-1307, USA.  */
+
+/* Calculate division table for ST40-300 integer division
+   Contributed by Joern Rernnecke
+   joern.rennecke@st.com  */
+
+#include <stdio.h>
+#include <math.h>
+
+int
+main ()
+{
+  int i, j;
+  double q, r, err, max_err = 0, max_s_err = 0;
+
+  puts("/* This table has been generated by divtab-sh4.c.  */");
+  puts ("\t.balign 4");
+  for (i = -128; i < 128; i++)
+    {
+      int n = 0;
+      if (i == 0)
+	{
+	  /* output some dummy number for 1/0.  */
+	  puts ("LOCAL(div_table_clz):\n\t.byte\t0");
+	  continue;
+	}
+      for (j = i < 0 ? -i : i; j < 128; j += j)
+	n++;
+      printf ("\t.byte\t%d\n", n - 7);
+    }
+  puts("\
+/* 1/-128 .. 1/127, normalized.  There is an implicit leading 1 in bit 32,\n\
+   or in bit 33 for powers of two.  */\n\
+	.balign 4");
+  for (i = -128; i < 128; i++)
+    {
+      if (i == 0)
+	{
+	  puts ("LOCAL(div_table_inv):\n\t.long\t0x0");
+	  continue;
+	}
+      j = i < 0 ? -i : i;
+      while (j < 64)
+	j += j;
+      q = 4.*(1<<30)*128/j;
+      r = ceil (q);
+      printf ("\t.long\t0x%X\n", (unsigned) r);
+      err = r - q;
+      if (err > max_err)
+	max_err = err;
+      err = err * j / 128;
+      if (err > max_s_err)
+	max_s_err = err;
+    }
+  printf ("\t/* maximum error: %f scaled: %f*/\n", max_err, max_s_err);
+  exit (0);
+}
Index: gcc/config/sh/crt1.asm
===================================================================
--- gcc/config/sh/crt1.asm	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/crt1.asm	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,5 +1,6 @@
 /* Copyright (C) 2000, 2001, 2003, 2004, 2005 Free Software Foundation, Inc.
    This file was pretty much copied from newlib.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -894,23 +895,10 @@
 	nop
 #ifdef VBR_SETUP
 ! Exception handlers	
-	.balign 256
+	.section .text.vbr, "ax"
 vbr_start:
-	mov.l 2f, r0     ! load the old vbr setting (if any)
-	mov.l @r0, r0
-	cmp/eq #0, r0
-	bf 1f
-	! no previous vbr - jump to own generic handler
-	bra handler
-	nop
-1:	! there was a previous handler - chain them
-	jmp @r0
-	nop
-	.balign 4
-2:
-	.long old_vbr
 
-	.balign 256
+	.org 0x100
 vbr_100:
 	#ifdef PROFILE
 	! Note on register usage.
@@ -1017,50 +1005,7 @@
 2:	
 	.long old_vbr
 
-	.balign 256
-vbr_200:
-	mov.l 2f, r0     ! load the old vbr setting (if any)
-	mov.l @r0, r0
-	cmp/eq #0, r0
-	bf 1f
-	! no previous vbr - jump to own generic handler
-	bra handler
-	nop	
-1:	! there was a previous handler - chain them
-	add #0x7f, r0	 ! 0x7f
-	add #0x7f, r0	 ! 0xfe
-	add #0x7f, r0	 ! 0x17d
-	add #0x7f, r0    ! 0x1fc
-	add #0x4, r0     ! add 0x200 without corrupting another register
-	jmp @r0
-	nop
-	.balign 4
-2:
-	.long old_vbr
-
-	.balign 256
-vbr_300:
-	mov.l 2f, r0     ! load the old vbr setting (if any)
-	mov.l @r0, r0
-	cmp/eq #0, r0
-	bf 1f
-	! no previous vbr - jump to own generic handler
-	bra handler
-	nop	
-1:	! there was a previous handler - chain them
-	rotcr r0
-	rotcr r0
-	add #0x7f, r0	 ! 0x1fc
-	add #0x41, r0	 ! 0x300
-	rotcl r0
-	rotcl r0	 ! Add 0x300 without corrupting another register
-	jmp @r0
-	nop
-	.balign 4
-2:
-	.long old_vbr
-
-	.balign 256	
+	.org 0x400
 vbr_400:	! Should be at vbr+0x400
 	mov.l 2f, r0     ! load the old vbr setting (if any)
 	mov.l @r0, r0
@@ -1103,28 +1048,7 @@
 	jmp @r2
 	nop
 
-	.balign 256
-vbr_500:
-	mov.l 2f, r0     ! load the old vbr setting (if any)
-	mov.l @r0, r0
-	cmp/eq #0, r0
-	! no previous vbr - jump to own generic handler
-	bt handler
-	! there was a previous handler - chain them
-	rotcr r0
-	rotcr r0
-	add #0x7f, r0	 ! 0x1fc
-	add #0x7f, r0	 ! 0x3f8
-	add #0x42, r0	 ! 0x500
-	rotcl r0
-	rotcl r0	 ! Add 0x500 without corrupting another register
-	jmp @r0
-	nop
-	.balign 4
-2:
-	.long old_vbr
-
-	.balign 256
+	.org 0x600
 vbr_600:
 #ifdef PROFILE	
 	! Should be at vbr+0x600
@@ -1140,11 +1064,48 @@
 	mov.l	r6,@-r15
 	mov.l	r7,@-r15
 	sts.l	pr,@-r15
+	sts.l	mach,@-r15
+	sts.l	macl,@-r15
+#if defined(__SH_FPU_ANY__)
+	! Save fpul and fpscr, save fr0-fr7 in 64 bit mode
+	! and set the pervading precision for the timer_handler
+	mov	#0,r0
+	sts.l	fpul,@-r15
+	sts.l	fpscr,@-r15
+	lds	r0,fpscr	! Clear fpscr
+	fmov	fr0,@-r15
+	fmov	fr1,@-r15
+	fmov	fr2,@-r15
+	fmov	fr3,@-r15
+	mov.l	pervading_precision_k,r0
+	fmov	fr4,@-r15
+	fmov	fr5,@-r15
+	mov.l	@r0,r0
+	fmov	fr6,@-r15
+	fmov	fr7,@-r15
+	lds	r0,fpscr
+#endif /* __SH_FPU_ANY__ */
 	! Pass interrupted pc to timer_handler as first parameter (r4).
 	stc    spc, r4
 	mov.l timer_handler_k, r0
 	jsr @r0
 	nop
+#if defined(__SH_FPU_ANY__)
+	mov	#0,r0
+	lds	r0,fpscr	! Clear the fpscr
+	fmov	@r15+,fr7
+	fmov	@r15+,fr6
+	fmov	@r15+,fr5
+	fmov	@r15+,fr4
+	fmov	@r15+,fr3
+	fmov	@r15+,fr2
+	fmov	@r15+,fr1
+	fmov	@r15+,fr0
+	lds.l	@r15+,fpscr
+	lds.l	@r15+,fpul
+#endif /* __SH_FPU_ANY__ */
+	lds.l @r15+,macl
+	lds.l @r15+,mach
 	lds.l @r15+,pr
 	mov.l @r15+,r7
 	mov.l @r15+,r6
@@ -1157,6 +1118,13 @@
 	stc sgr, r15    ! Restore r15, destroyed by this sequence. 
 	rte
 	nop
+#if defined(__SH_FPU_ANY__)
+	.balign 4
+pervading_precision_k:
+#define CONCAT1(A,B) A##B
+#define CONCAT(A,B) CONCAT1(A,B)
+	.long CONCAT(__USER_LABEL_PREFIX__,__fpscr_values)+4
+#endif
 #else
 	mov.l 2f, r0     ! Load the old vbr setting (if any).
 	mov.l @r0, r0
@@ -1205,201 +1173,5 @@
 handler_exit_k:
 	.long _exit
 	.align 2
-! Simulated compile of trap handler.
-	.section	.debug_abbrev,"",@progbits
-.Ldebug_abbrev0:
-	.section	.debug_info,"",@progbits
-.Ldebug_info0:
-	.section	.debug_line,"",@progbits
-.Ldebug_line0:
-	.text
-.Ltext0:
-	.align 5
-	.type	__superh_trap_handler,@function
-__superh_trap_handler:
-.LFB1:
-	mov.l	r14,@-r15
-.LCFI0:
-	add	#-4,r15
-.LCFI1:
-	mov	r15,r14
-.LCFI2:
-	mov.l	r4,@r14
-	lds	r1, pr
-	add	#4,r14
-	mov	r14,r15
-	mov.l	@r15+,r14
-	rts	
-	nop
-.LFE1:
-.Lfe1:
-	.size	__superh_trap_handler,.Lfe1-__superh_trap_handler
-	.section	.debug_frame,"",@progbits
-.Lframe0:
-	.ualong	.LECIE0-.LSCIE0
-.LSCIE0:
-	.ualong	0xffffffff
-	.byte	0x1
-	.string	""
-	.uleb128 0x1
-	.sleb128 -4
-	.byte	0x11
-	.byte	0xc
-	.uleb128 0xf
-	.uleb128 0x0
-	.align 2
-.LECIE0:
-.LSFDE0:
-	.ualong	.LEFDE0-.LASFDE0
-.LASFDE0:
-	.ualong	.Lframe0
-	.ualong	.LFB1
-	.ualong	.LFE1-.LFB1
-	.byte	0x4
-	.ualong	.LCFI0-.LFB1
-	.byte	0xe
-	.uleb128 0x4
-	.byte	0x4
-	.ualong	.LCFI1-.LCFI0
-	.byte	0xe
-	.uleb128 0x8
-	.byte	0x8e
-	.uleb128 0x1
-	.byte	0x4
-	.ualong	.LCFI2-.LCFI1
-	.byte	0xd
-	.uleb128 0xe
-	.align 2
-.LEFDE0:
-	.text
-.Letext0:
-	.section	.debug_info
-	.ualong	0xb3
-	.uaword	0x2
-	.ualong	.Ldebug_abbrev0
-	.byte	0x4
-	.uleb128 0x1
-	.ualong	.Ldebug_line0
-	.ualong	.Letext0
-	.ualong	.Ltext0
-	.string	"trap_handler.c"
-	.string	"xxxxxxxxxxxxxxxxxxxxxxxxxxxx"
-	.string	"GNU C 3.2 20020529 (experimental)"
-	.byte	0x1
-	.uleb128 0x2
-	.ualong	0xa6
-	.byte	0x1
-	.string	"_superh_trap_handler"
-	.byte	0x1
-	.byte	0x2
-	.byte	0x1
-	.ualong	.LFB1
-	.ualong	.LFE1
-	.byte	0x1
-	.byte	0x5e
-	.uleb128 0x3
-	.string	"trap_reason"
-	.byte	0x1
-	.byte	0x1
-	.ualong	0xa6
-	.byte	0x2
-	.byte	0x91
-	.sleb128 0
-	.byte	0x0
-	.uleb128 0x4
-	.string	"unsigned int"
-	.byte	0x4
-	.byte	0x7
-	.byte	0x0
-	.section	.debug_abbrev
-	.uleb128 0x1
-	.uleb128 0x11
-	.byte	0x1
-	.uleb128 0x10
-	.uleb128 0x6
-	.uleb128 0x12
-	.uleb128 0x1
-	.uleb128 0x11
-	.uleb128 0x1
-	.uleb128 0x3
-	.uleb128 0x8
-	.uleb128 0x1b
-	.uleb128 0x8
-	.uleb128 0x25
-	.uleb128 0x8
-	.uleb128 0x13
-	.uleb128 0xb
-	.byte	0x0
-	.byte	0x0
-	.uleb128 0x2
-	.uleb128 0x2e
-	.byte	0x1
-	.uleb128 0x1
-	.uleb128 0x13
-	.uleb128 0x3f
-	.uleb128 0xc
-	.uleb128 0x3
-	.uleb128 0x8
-	.uleb128 0x3a
-	.uleb128 0xb
-	.uleb128 0x3b
-	.uleb128 0xb
-	.uleb128 0x27
-	.uleb128 0xc
-	.uleb128 0x11
-	.uleb128 0x1
-	.uleb128 0x12
-	.uleb128 0x1
-	.uleb128 0x40
-	.uleb128 0xa
-	.byte	0x0
-	.byte	0x0
-	.uleb128 0x3
-	.uleb128 0x5
-	.byte	0x0
-	.uleb128 0x3
-	.uleb128 0x8
-	.uleb128 0x3a
-	.uleb128 0xb
-	.uleb128 0x3b
-	.uleb128 0xb
-	.uleb128 0x49
-	.uleb128 0x13
-	.uleb128 0x2
-	.uleb128 0xa
-	.byte	0x0
-	.byte	0x0
-	.uleb128 0x4
-	.uleb128 0x24
-	.byte	0x0
-	.uleb128 0x3
-	.uleb128 0x8
-	.uleb128 0xb
-	.uleb128 0xb
-	.uleb128 0x3e
-	.uleb128 0xb
-	.byte	0x0
-	.byte	0x0
-	.byte	0x0
-	.section	.debug_pubnames,"",@progbits
-	.ualong	0x27
-	.uaword	0x2
-	.ualong	.Ldebug_info0
-	.ualong	0xb7
-	.ualong	0x67
-	.string	"_superh_trap_handler"
-	.ualong	0x0
-	.section	.debug_aranges,"",@progbits
-	.ualong	0x1c
-	.uaword	0x2
-	.ualong	.Ldebug_info0
-	.byte	0x4
-	.byte	0x0
-	.uaword	0x0
-	.uaword	0x0
-	.ualong	.Ltext0
-	.ualong	.Letext0-.Ltext0
-	.ualong	0x0
-	.ualong	0x0
 #endif /* VBR_SETUP */
 #endif /* ! __SH5__ */
Index: gcc/config/sh/sh1.md
===================================================================
--- gcc/config/sh/sh1.md	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/sh1.md	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,5 +1,6 @@
 ;; DFA scheduling description for Renesas / SuperH SH.
 ;; Copyright (C) 2004, 2007 Free Software Foundation, Inc.
+;; Copyright (c) 2006  STMicroelectronics.
 
 ;; This file is part of GCC.
 
@@ -44,7 +45,7 @@
 
 (define_insn_reservation "sh1_load_store" 2
   (and (eq_attr "pipe_model" "sh1")
-       (eq_attr "type" "load,pcload,pload,store,pstore"))
+       (eq_attr "type" "load,pcload,pload,mem_mac,store,fstore,pstore,mac_mem"))
   "sh1memory*2")
 
 (define_insn_reservation "sh1_arith3" 3
@@ -75,7 +76,7 @@
 
 (define_insn_reservation "sh1_fp" 2
   (and (eq_attr "pipe_model" "sh1")
-       (eq_attr "type" "fp,fmove"))
+       (eq_attr "type" "fp,fpscr_toggle,fp_cmp,fmove"))
   "sh1fp")
 
 (define_insn_reservation "sh1_fdiv" 13
Index: gcc/config/sh/ieee-754-sf.S
===================================================================
--- gcc/config/sh/ieee-754-sf.S	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/ieee-754-sf.S	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,697 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!! libgcc software floating-point routines for Renesas SH /
+!! STMicroelectronics ST40 CPUs
+!! Contributed by J"orn Rennecke joern.rennecke@st.com
+
+#ifndef __SH_FPU_ANY__
+
+#include "lib1funcs.h"
+#include "insn-constants.h"
+
+/* Single-precision floating-point emulation.
+   We handle NANs, +-infinity, and +-zero.
+   However, we assume that for NANs, the topmost bit of the fraction is set.  */
+#ifdef L_nesf2f
+/* -fno-finite-math-only inline version, T := r4:SF == r5:SF
+	cmp/eq	r4,r5
+	mov	r4,r0
+	bt	0f
+	or	r5,r0
+	add	r0,r0
+	tst	r0,r0	! test for +0.0 == -0.0 ; -0.0 == +0.0
+	0:			*/
+	.balign 4
+	.global GLOBAL(nesf2f)
+	HIDDEN_FUNC(GLOBAL(nesf2f))
+GLOBAL(nesf2f):
+        /* If the raw values are unequal, the result is unequal, unless
+	   both values are +-zero.
+	   If the raw values are equal, the result is equal, unless
+	   the values are NaN.  */
+	cmp/eq	r4,r5
+	mov.l   LOCAL(c_SF_NAN_MASK),r1
+	not	r4,r0
+	bt	LOCAL(check_nan)
+	mov	r4,r0
+	or	r5,r0
+	rts
+	add	r0,r0
+LOCAL(check_nan):
+	tst	r1,r0
+	rts
+	movt	r0
+	.balign 4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+	ENDFUNC(GLOBAL(nesf2f))
+#endif /* L_nesf2f */
+
+#ifdef L_unord_sf
+	.balign 4
+	.global GLOBAL(unordsf2)
+	HIDDEN_FUNC(GLOBAL(unordsf2))
+GLOBAL(unordsf2):
+	mov.l	LOCAL(c_SF_NAN_MASK),r1
+	not	r4,r0
+	tst	r1,r0
+	not	r5,r0
+	bt	LOCAL(unord)
+	tst	r1,r0
+LOCAL(unord):
+	rts
+	movt	r0
+	.balign	4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+	ENDFUNC(GLOBAL(unordsf2))
+#endif /* L_unord_sf */
+
+#if defined(L_gtsf2t) || defined(L_gtsf2t_trap)
+/* -fno-finite-math-only inline version, T := r4:SF > r5:SF ? 0 : 1
+	cmp/pz	r4
+	mov	r4,r0
+	bf/s	0f
+	 cmp/hs	r5,r4
+	cmp/ge	r4,r5
+	or	r5,r0
+	bt	0f
+	add	r0,r0
+	tst	r0,r0
+	0:			*/
+#ifdef L_gtsf2t
+#define fun_label GLOBAL(gtsf2t)
+#else
+#define fun_label GLOBAL(gtsf2t_trap)
+#endif
+	.balign 4
+	.global fun_label
+	HIDDEN_FUNC(fun_label)
+fun_label:
+	/* If the raw values compare greater, the result true, unless
+	   any of them is a nan (but infinity is fine), or both values are
+	   +- zero.  Otherwise, the result false.  */
+	mov.l	LOCAL(c_SF_NAN_MASK),r1
+	cmp/pz	r4
+	not	r5,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	r4,r0
+	bt	LOCAL(nan)
+	cmp/gt	r5,r4
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/gt	r4,r1)
+	bf	LOCAL(nan)
+	or	r5,r0
+	rts
+	add	r0,r0
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan)
+	not	r4,r0
+	tst	r1,r0
+	bt	LOCAL(nan)
+	cmp/hi	r4,r5
+#if defined(L_gtsf2t) && defined(DELAYED_BRANCHES)
+LOCAL(check_nan):
+#endif /* DELAYED_BRANCHES */
+	rts
+	movt	r0
+#ifdef L_gtsf2t
+LOCAL(check_nan):
+LOCAL(nan):
+	rts
+	mov	#0,r0
+#else /* ! L_gtsf2t */
+LOCAL(check_nan):
+	SLI(cmp/gt	r4,r1)
+	bf	LOCAL(nan)
+	rts
+	movt	r0
+LOCAL(nan):
+	mov	#0,r0
+	trapa	#0
+#endif /* ! L_gtsf2t */
+	.balign	4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+	ENDFUNC(fun_label)
+#endif /* L_gtsf2t */
+
+#if defined(L_gesf2f) || defined(L_gesf2f_trap)
+/* -fno-finite-math-only inline version, T := r4:SF >= r5:SF */
+	cmp/pz	r5
+	mov	r4,r0
+	bf/s	0f
+	 cmp/hs	r4,r5
+	cmp/ge	r5,r4
+	or	r5,r0
+	bt	0f
+	add	r0,r0
+	tst	r0,r0
+	0:
+#ifdef L_gesf2f
+#define fun_label GLOBAL(gesf2f)
+#else
+#define fun_label GLOBAL(gesf2f_trap)
+#endif
+	.balign 4
+	.global fun_label
+	HIDDEN_FUNC(fun_label)
+fun_label:
+	/* If the raw values compare greater or equal, the result is
+	   true, unless any of them is a nan.  If both are -+zero, the
+	   result is true; otherwise, it is false.
+	   We use 0 as true and nonzero as false for this function.  */
+	mov.l	LOCAL(c_SF_NAN_MASK),r1
+	cmp/pz	r5
+	not	r4,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	r4,r0
+	bt	LOCAL(nan)
+	cmp/gt	r4,r5
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/ge	r1,r5)
+	bt	LOCAL(nan)
+	or	r5,r0
+	rts
+	add	r0,r0
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan)
+	not	r5,r0
+	tst	r1,r0
+	bt	LOCAL(nan)
+	cmp/hi	r5,r4
+#if defined(L_gesf2f) && defined(DELAYED_BRANCHES)
+LOCAL(nan): LOCAL(check_nan):
+#endif
+	rts
+	movt	r0
+#if defined(L_gesf2f) && ! defined(DELAYED_BRANCHES)
+LOCAL(check_nan):
+	cmp/ge	r1,r5
+LOCAL(nan):
+	rts
+	movt	r0
+#endif /* ! DELAYED_BRANCHES */
+#ifdef L_gesf2f_trap
+LOCAL(check_nan):
+	SLI(cmp/ge	r1,r5)
+	bt	LOCAL(nan)
+	rts
+LOCAL(nan):
+	movt	r0
+	trapa	#0
+#endif /* L_gesf2f_trap */
+	.balign	4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+	ENDFUNC(GLOBAL(gesf2f))
+#endif /* L_gesf2f */
+
+#ifndef DYN_SHIFT /* SH1 / SH2 code */
+#ifdef L_addsub_sf
+#include "IEEE-754/addsf3.S"
+#endif /* _addsub_sf */
+
+#ifdef L_mul_sf
+#include "IEEE-754/mulsf3.S"
+#endif /* L_mul_sf */
+
+#ifdef L__fixunssfsi
+#include "IEEE-754/fixunssfsi.S"
+#endif /* L_fixunssfsi */
+
+#ifdef L_sf_to_si
+#include "IEEE-754/fixsfsi.S"
+#endif /* L_sf_to_si */
+
+#ifdef L_usi_to_sf
+#include "IEEE-754/floatunssisf.S"
+#endif /* L_usi_to_sf */
+
+#ifdef L_si_to_sf
+#include "IEEE-754/floatsisf.S"
+#endif /* L_si_to_sf */
+
+#ifdef L_div_sf
+#include "IEEE-754/divsf3.S"
+#endif /* L_div_sf */
+#endif /* ! DYN_SHIFT */
+
+/* The actual arithmetic uses dynamic shift.  Supporting SH1 / SH2 here would
+   make this code too hard to maintain, so if you want to add SH1 / SH2
+   support, do it in a separate copy.  */
+#ifdef DYN_SHIFT
+#ifdef L_addsub_sf
+#include "IEEE-754/m3/addsf3.S"
+#endif /* L_addsub_sf */
+
+#ifdef L_mul_sf
+#include "IEEE-754/m3/mulsf3.S"
+#endif /* L_mul_sf */
+
+#ifdef L_fixunssfsi
+	! What is a bit unusal about this implementation is that the
+	! sign bit influences the result for NANs: for cleared sign bit, you
+	! get UINT_MAX, for set sign bit, you get 0.
+	! However, since the result for NANs is undefined, this should be no
+	! problem.
+	! N.B. This is scheduled both for SH4-200 and SH4-300
+	.balign 4
+	.global GLOBAL(fixunssfsi)
+	FUNC(GLOBAL(fixunssfsi))
+GLOBAL(fixunssfsi):
+	mov.l	LOCAL(max),r2
+	mov	#-23,r1
+	mov	r4,r0
+	shad	r1,r4
+	mov.l	LOCAL(mask),r1
+	add	#-127,r4
+	cmp/ge	r2,r0
+	or	r2,r0
+	bt	LOCAL(retmax)
+	cmp/pz	r4
+	and	r1,r0
+	bf	LOCAL(ret0)
+	add	#-23,r4
+	rts
+	shld	r4,r0
+LOCAL(ret0):
+LOCAL(retmax):
+	rts
+	subc	r0,r0
+	.balign 4
+LOCAL(mask):
+	.long	0x00ffffff
+LOCAL(max):
+	.long	0x4f800000
+	ENDFUNC(GLOBAL(fixunssfsi))
+#endif /* L_fixunssfsi */
+
+#ifdef L_sf_to_si
+	! What is a bit unusal about this implementation is that the
+	! sign bit influences the result for NANs: for cleared sign bit, you
+	! get INT_MAX, for set sign bit, you get INT_MIN.
+	! However, since the result for NANs is undefined, this should be no
+	! problem.
+	! N.B. This is scheduled both for SH4-200 and SH4-300
+	.balign 4
+	.global GLOBAL(fixsfsi)
+	FUNC(GLOBAL(fixsfsi))
+	.balign	4
+GLOBAL(fixsfsi):
+	mov	r4,r0
+	shll	r4
+	mov	#-24,r1
+	bt	LOCAL(neg)
+	mov.l	LOCAL(max),r2
+	shld	r1,r4
+	mov.l	LOCAL(mask),r1
+	add	#-127,r4
+	cmp/pz	r4
+	add	#-23,r4
+	bf	LOCAL(ret0)
+	cmp/gt	r0,r2
+	bf	LOCAL(retmax)
+	and	r1,r0
+	addc	r1,r0
+	rts
+	shld	r4,r0
+
+	.balign	4
+LOCAL(neg):
+	mov.l	LOCAL(min),r2
+	shld	r1,r4
+	mov.l	LOCAL(mask),r1
+	add	#-127,r4
+	cmp/pz	r4
+	add	#-23,r4
+	bf	LOCAL(ret0)
+	cmp/gt	r0,r2
+	bf	LOCAL(retmin)
+	and	r1,r0
+	addc	r1,r0
+	shld	r4,r0	! SH4-200 will start this insn on a new cycle
+	rts
+	neg	r0,r0
+
+	.balign	4
+LOCAL(ret0):
+	rts
+	mov	#0,r0
+
+LOCAL(retmax):
+	mov	#-1,r0
+	rts
+	shlr	r0
+
+LOCAL(retmin):
+	mov	#1,r0
+	rts
+	rotr	r0
+
+	.balign 4
+LOCAL(mask):
+	.long	0x007fffff
+LOCAL(max):
+	.long	0x4f000000
+LOCAL(min):
+	.long	0xcf000000
+	ENDFUNC(GLOBAL(fixsfsi))
+#endif /* L_sf_to_si */
+
+#ifdef L_usi_to_sf
+#include "IEEE-754/m3/floatunssisf.S"
+#endif /* L_usi_to_sf */
+
+#ifdef L_si_to_sf
+#include "IEEE-754/m3/floatsisf.S"
+#endif /* L_si_to_sf */
+
+#ifdef L_div_sf
+#include "IEEE-754/m3/divsf3.S"
+#endif /* L_div_sf */
+
+#ifdef L_hypotf
+	.balign 4
+	.global GLOBAL(hypotf)
+	FUNC(GLOBAL(hypotf))
+GLOBAL(hypotf):
+/* This integer implementation takes 71 to 72 cycles in the main path.
+   This is a bit slower than the SH4 can do this computation using double
+   precision hardware floating point - 57 cycles, or 69 with mode switches.  */
+ /* First, calculate x (r4) as the sum of the square of the fractions -
+    the exponent is calculated separately in r3.
+    Then, calculate sqrt(x) for the fraction by reciproot iteration.
+    We get an 7.5 bit inital value using linear approximation with two slopes
+    that are powers of two.
+    x (- [1. .. 2.)  y0 := 1.25 - x/4 - tab(x)   y (- (0.8 .. 1.0)
+    x (- [2. .. 4.)  y0 := 1.   - x/8 - tab(x)   y (- (0.5 .. 0.8)
+ x is represented with two bits before the point,
+ y with 0 bits before the binary point.
+ Thus, to calculate y0 := 1. - x/8 - tab(x), all you have to do is to shift x
+ right by 1, negate it, and subtract tab(x).  */
+
+ /* y1 := 1.5*y0 - 0.5 * (x * y0) * (y0 * y0)
+    z0 := x * y1
+    z1 := z0 + 0.5 * (y1 - (y1*y1) * z0) */
+
+	mov.l	LOCAL(xff000000),r1
+	add	r4,r4
+	mov	r4,r0
+	add	r5,r5
+	cmp/hs	r5,r4
+	sub	r5,r0
+	mov	#-24,r2
+	bf/s	LOCAL(r5_large)
+	shad	r2,r0
+	mov	r4,r3
+	shll8	r4
+	rotcr	r4
+	tst	#0xe0,r0
+	neg	r0,r0
+	bt	LOCAL(ret_abs_r3)
+	tst	r1,r5
+	shll8	r5
+	bt/s	LOCAL(denorm_r5)
+	cmp/hi	r3,r1
+	dmulu.l	r4,r4
+	bf	LOCAL(inf_nan)
+	rotcr	r5
+	shld	r0,r5
+LOCAL(denorm_r5_done):
+	sts	mach,r4
+	dmulu.l	r5,r5
+	mov.l	r6,@-r15
+	mov	#20,r6
+
+	sts	mach,r5
+LOCAL(add_frac):
+	mova	LOCAL(tab)-32,r0
+	mov.l	r7,@-r15
+	mov.w	LOCAL(x1380),r7
+	and	r1,r3
+	addc	r5,r4
+	mov.w	LOCAL(m25),r2	! -25
+	bf	LOCAL(frac_ok)
+	sub	r1,r3
+	rotcr	r4
+	cmp/eq	r1,r3	! did we generate infinity ?
+	bt	LOCAL(inf_nan)
+	shlr	r4
+	mov	r4,r1
+	shld	r2,r1
+	mov.b	@(r0,r1),r0
+	mov	r4,r1
+	shld	r6,r1
+	bra	LOCAL(frac_low2)
+	sub	r1,r7
+
+LOCAL(frac_ok):
+	mov	r4,r1
+	shld	r2,r1
+	mov.b	@(r0,r1),r1
+	cmp/pz	r4
+	mov	r4,r0
+	bt/s	LOCAL(frac_low)
+	shld	r6,r0
+	mov.w	LOCAL(xf80),r7
+	shlr	r0
+LOCAL(frac_low):
+	sub	r0,r7
+LOCAL(frac_low2):
+	mov.l	LOCAL(x40000080),r0 ! avoid denorm results near 1. << r3
+	sub	r1,r7	! {0.12}
+	mov.l	LOCAL(xfffe0000),r5 ! avoid rounding overflow near 4. << r3
+	swap.w	r7,r1	! {0.28}
+	dmulu.l	r1,r4 /* two issue cycles */
+	mulu.w	r7,r7  /* two issue cycles */
+	sts	mach,r2	! {0.26}
+	mov	r1,r7
+	shlr	r1
+	sts	macl,r6	! {0.24}
+	cmp/hi	r0,r4
+	shlr2	r2
+	bf	LOCAL(near_one)
+	shlr	r2	! {0.23} systemic error of linear approximation keeps y1 < 1
+	dmulu.l	r2,r6
+	cmp/hs	r5,r4
+	add	r7,r1	! {1.28}
+	bt	LOCAL(near_four)
+	shlr2	r1	! {1.26}
+	sts	mach,r0	! {0.15} x*y0^3 == {0.16} 0.5*x*y0^3
+	shlr2	r1	! {1.24}
+	shlr8	r1	! {1.16}
+	sett		! compensate for truncation of subtrahend, keep y1 < 1
+	subc	r0,r1   ! {0.16} y1;  max error about 3.5 ulp
+	swap.w	r1,r0
+	dmulu.l	r0,r4	! { 1.30 }
+	mulu.w	r1,r1
+	sts	mach,r2
+	shlr2	r0
+	sts	macl,r1
+	add	r2,r0
+	mov.l	LOCAL(xff000000),r6
+	add	r2,r0
+	dmulu.l	r1,r2
+	add	#127,r0
+	add	r6,r3	! precompensation for adding leading 1
+	sts	mach,r1
+	shlr	r3
+	mov.l	@r15+,r7
+	sub	r1,r0	! {0.31} max error about 50 ulp (+127)
+	mov.l	@r15+,r6
+	shlr8	r0	! {0.23} max error about 0.7 ulp
+	rts
+	add	r3,r0
+	
+LOCAL(r5_large):
+	mov	r5,r3
+	mov	#-31,r2
+	cmp/ge	r2,r0
+	shll8	r5
+	bf	LOCAL(ret_abs_r3)
+	rotcr	r5
+	tst	r1,r4
+	shll8	r4
+	bt/s	LOCAL(denorm_r4)
+	cmp/hi	r3,r1
+	dmulu.l	r5,r5
+	bf	LOCAL(inf_nan)
+	rotcr	r4
+LOCAL(denorm_r4_done):
+	shld	r0,r4
+	sts	mach,r5
+	dmulu.l	r4,r4
+	mov.l	r6,@-r15
+	mov	#20,r6
+	bra	LOCAL(add_frac)
+	sts	mach,r4
+
+LOCAL(near_one):
+	bra	LOCAL(assemble_sqrt)
+	mov	#0,r0
+LOCAL(near_four):
+	! exact round-to-nearest would add 255.  We add 256 for speed & compactness.
+	mov	r4,r0
+	shlr8	r0
+	add	#1,r0
+	tst	r0,r0
+	addc	r0,r3	! might generate infinity.
+LOCAL(assemble_sqrt):
+	mov.l	@r15+,r7
+	shlr	r3
+	mov.l	@r15+,r6
+	rts
+	add	r3,r0
+LOCAL(inf_nan):
+LOCAL(ret_abs_r3):
+	mov	r3,r0
+	rts
+	shlr	r0
+LOCAL(denorm_r5):
+	bf	LOCAL(inf_nan)
+	tst	r1,r4
+	bt	LOCAL(denorm_both)
+	dmulu.l	r4,r4
+	bra	LOCAL(denorm_r5_done)
+	shld	r0,r5
+LOCAL(denorm_r4):
+	bf	LOCAL(inf_nan)
+	tst	r1,r5
+	dmulu.l	r5,r5
+	bf	LOCAL(denorm_r4_done)
+LOCAL(denorm_both):	! normalize according to r3.
+	extu.w	r3,r2
+	mov.l	LOCAL(c__clz_tab),r0
+	cmp/eq	r3,r2
+	mov	#-8,r2
+	bt	0f
+	tst	r1,r3
+	mov	#-16,r2
+	bt	0f
+	mov	#-24,r2
+0:
+	shld	r2,r3
+	mov.l	r7,@-r15
+#ifdef __pic__
+	add	r0,r3
+	mova	 LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r3),r0
+	add	#32,r2
+	sub	r0,r2
+	shld	r2,r4
+	mov	r2,r7
+	dmulu.l	r4,r4
+	sts.l	pr,@-r15
+	mov	#1,r3
+	bsr	LOCAL(denorm_r5_done)
+	shld	r2,r5
+	mov.l	LOCAL(x01000000),r1
+	neg	r7,r2
+	lds.l	@r15+,pr
+	tst	r1,r0
+	mov.l	@r15+,r7
+	bt	0f
+	add	#1,r2
+	sub	r1,r0
+0:
+	rts
+	shld	r2,r0
+
+LOCAL(m25):
+	.word	-25
+LOCAL(x1380):
+	.word	0x1380
+LOCAL(xf80):
+	.word	0xf80
+	.balign	4
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x40000080):
+	.long	0x40000080
+LOCAL(xfffe0000):
+	.long	0xfffe0000
+LOCAL(x01000000):
+	.long	0x01000000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+
+/*
+double err(double x)
+{
+  return (x < 2. ? 1.25 - x/4. : 1. - x/8.) - 1./sqrt(x);
+}
+
+int
+main ()
+{
+  int i = 0;
+  double x, s, v;
+  double lx, hx;
+
+  s = 1./32.;
+  for (x = 1.; x < 4; x += s, i++)
+    {
+      lx = x;
+      hx = x + s - 1. / (1 << 30);
+      v = 0.5 * (err (lx) + err (hx));
+      printf ("%s% 4d%c",
+              (i & 7) == 0 ? "\t.byte\t" : "",
+              (int)(v * 4096 + 0.5) - 128,
+              (i & 7) == 7 ? '\n' : ',');
+    }
+  return 0;
+} */
+
+	.balign	4
+LOCAL(tab):
+	.byte	-113, -84, -57, -33, -11,   8,  26,  41
+	.byte	  55,  67,  78,  87,  94, 101, 106, 110
+	.byte	 113, 115, 115, 115, 114, 112, 109, 106
+	.byte	 101,  96,  91,  84,  77,  69,  61,  52
+	.byte	  51,  57,  63,  68,  72,  77,  80,  84
+	.byte	  87,  89,  91,  93,  95,  96,  97,  97
+	.byte	  97,  97,  97,  96,  95,  94,  93,  91
+	.byte	  89,  87,  84,  82,  79,  76,  72,  69
+	.byte	  65,  61,  57,  53,  49,  44,  39,  34
+	.byte	  29,  24,  19,  13,   8,   2,  -4, -10
+	.byte	 -17, -23, -29, -36, -43, -50, -57, -64
+	.byte	 -71, -78, -85, -93,-101,-108,-116,-124
+	ENDFUNC(GLOBAL(hypotf))
+#endif /* L_hypotf */
+#endif /* DYN_SHIFT */
+
+#endif /* __SH_FPU_ANY__ */
Index: gcc/config/sh/sh4-300.md
===================================================================
--- gcc/config/sh/sh4-300.md	(.../vendor/tags/4.2.4)	(revision 0)
+++ gcc/config/sh/sh4-300.md	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,290 @@
+;; DFA scheduling description for ST40-300.
+;; Copyright (C) 2004 Free Software Foundation, Inc.
+;; Copyright (C) 2006 STMicroelectronics (Will be assigned to FSF when
+;; patch is contributed.)
+
+;; This file is part of GCC.
+
+;; GCC is free software; you can redistribute it and/or modify
+;; it under the terms of the GNU General Public License as published by
+;; the Free Software Foundation; either version 2, or (at your option)
+;; any later version.
+
+;; GCC is distributed in the hope that it will be useful,
+;; but WITHOUT ANY WARRANTY; without even the implied warranty of
+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+;; GNU General Public License for more details.
+
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING.  If not, write to
+;; the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+;; Boston, MA 02110-1301, USA.
+
+;; Load and store instructions save a cycle if they are aligned on a
+;; four byte boundary.  Using a function unit for stores encourages
+;; gcc to separate load and store instructions by one instruction,
+;; which makes it more likely that the linker will be able to word
+;; align them when relaxing.
+
+;; The following description models the ST40-300 pipeline using the DFA based
+;; scheduler.
+
+;; Two automata are defined to reduce number of states
+;; which a single large automaton will have. (Factoring)
+
+(define_automaton "sh4_300_inst_pipeline,sh4_300_fpu_pipe")
+
+;; This unit is basically the decode unit of the processor.
+;; Since SH4 is a dual issue machine,it is as if there are two
+;; units so that any insn can be processed by either one
+;; of the decoding unit.
+
+(define_cpu_unit "sh4_300_pipe_01,sh4_300_pipe_02" "sh4_300_inst_pipeline")
+
+;; The floating point units.
+
+(define_cpu_unit "sh4_300_fpt,sh4_300_fpu,sh4_300_fds" "sh4_300_fpu_pipe")
+
+;; integer multiplier unit
+
+(define_cpu_unit "sh4_300_mul" "sh4_300_inst_pipeline")
+
+;; LS unit
+
+(define_cpu_unit "sh4_300_ls" "sh4_300_inst_pipeline")
+
+;; The address calculator used for branch instructions.
+;; This will be reserved after "issue" of branch instructions
+;; and this is to make sure that no two branch instructions
+;; can be issued in parallel.
+
+(define_cpu_unit "sh4_300_br" "sh4_300_inst_pipeline")
+
+;; ----------------------------------------------------
+;; This reservation is to simplify the dual issue description.
+
+(define_reservation  "sh4_300_issue"  "sh4_300_pipe_01|sh4_300_pipe_02")
+
+(define_reservation "all" "sh4_300_pipe_01+sh4_300_pipe_02")
+
+;;(define_insn_reservation "nil" 0 (eq_attr "type" "nil") "nothing")
+
+;; MOV RM,RN / MOV #imm8,RN / STS PR,RN
+(define_insn_reservation "sh4_300_mov" 0
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "move,movi8,prget"))
+  "sh4_300_issue")
+
+;; Fixed STS from MACL / MACH
+(define_insn_reservation "sh4_300_mac_gp" 0
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "mac_gp"))
+  "sh4_300_issue+sh4_300_mul")
+
+;; Fixed LDS to MACL / MACH
+(define_insn_reservation "sh4_300_gp_mac" 1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "gp_mac"))
+  "sh4_300_issue+sh4_300_mul")
+
+;; Instructions without specific resource requirements with latency 1.
+
+(define_insn_reservation "sh4_300_simple_arith" 1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "mt_group,arith,dyn_shift,prset"))
+  "sh4_300_issue")
+
+;; Load and store instructions have no alignment peculiarities for the ST40-300,
+;; but they use the load-store unit, which they share with the fmove type
+;; insns (fldi[01]; fmov frn,frm; flds; fsts; fabs; fneg) .
+;; Loads have a latency of three.
+
+;; Load Store instructions.
+(define_insn_reservation "sh4_300_load" 3
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "load,pcload,load_si,pcload_si,pload"))
+  "sh4_300_issue+sh4_300_ls")
+
+(define_insn_reservation "sh4_300_mac_load" 3
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "mem_mac"))
+  "sh4_300_issue+sh4_300_ls+sh4_300_mul")
+
+(define_insn_reservation "sh4_300_fload" 4
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fload,pcfload"))
+  "sh4_300_issue+sh4_300_ls+sh4_300_fpt")
+
+;; sh_adjust_cost describes the reduced latency of the feeding insns of a store.
+;; The latency of an auto-increment register is 1; the latency of the memory
+;; output is not actually considered here anyway.
+(define_insn_reservation "sh4_300_store" 1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "store,pstore"))
+  "sh4_300_issue+sh4_300_ls")
+
+(define_insn_reservation "sh4_300_fstore" 1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fstore"))
+  "sh4_300_issue+sh4_300_ls+sh4_300_fpt")
+
+;; Fixed STS.L from MACL / MACH
+(define_insn_reservation "sh4_300_mac_store" 1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "mac_mem"))
+  "sh4_300_issue+sh4_300_mul+sh4_300_ls")
+
+(define_insn_reservation "sh4_300_gp_fpul" 2
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "gp_fpul"))
+  "sh4_300_issue+sh4_300_fpt")
+
+(define_insn_reservation "sh4_300_fpul_gp" 1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fpul_gp"))
+  "sh4_300_issue+sh4_300_fpt")
+
+;; Branch (BF,BF/S,BT,BT/S,BRA)
+;; Branch Far (JMP,RTS,BRAF)
+;; Group:	BR
+;; When displacement is 0 for BF / BT, we have effectively conditional
+;; execution of one instruction, without pipeline disruption.
+;; Otherwise, the latency depends on prediction success.
+;; We can't really do much with the latency, even if we could express it,
+;; but the pairing restrictions are useful to take into account.
+;; ??? If the branch is likely, and not paired with a preceding insn,
+;; or likely and likely not predicted, we might want to fill the delay slot.
+;; However, there appears to be no machinery to make the compiler
+;; recognize these scenarios.
+
+(define_insn_reservation "sh4_300_branch"  1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "cbranch,jump,return,jump_ind"))
+  "sh4_300_issue+sh4_300_br")
+
+;; RTE
+(define_insn_reservation "sh4_300_return_from_exp" 9
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "rte"))
+  "sh4_300_pipe_01+sh4_300_pipe_02*9")
+
+;; OCBP, OCBWB
+;; Group:	CO
+;; Latency: 	1-5
+;; Issue Rate: 	1
+
+;; cwb is used for the sequence ocbwb @%0; extu.w %0,%2; or %1,%2; mov.l %0,@%2
+;; This description is likely inexact, but this pattern should not actually
+;; appear when compiling for sh4-300; we should use isbi instead.
+;; If a -mtune option is added later, we should use the icache array
+;; dispatch method instead.
+(define_insn_reservation "sh4_300_ocbwb"  3
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "cwb"))
+  "all*3")
+
+;; JSR,BSR,BSRF
+;; Calls have a mandatory delay slot, which we'd like to fill with an insn
+;; that can be paired with the call itself.
+;; Scheduling runs before reorg, so we approximate this by saying that we
+;; want the call to be paired with a preceding insn.
+;; In most cases, the insn that loads the address of the call should have
+;; a non-zero latency (mov rn,rm doesn't make sense since we could use rn
+;; for the address then).  Thus, a preceding insn that can be paired with
+;; a call should be elegible for the delay slot.
+;;
+;; calls introduce a longisch delay that is likely to flush the pipelines
+;; of the caller's instructions.  Ordinary functions tend to end with a
+;; load to restore a register (in the delay slot of rts), while sfuncs
+;; tend to end with an EX or MT insn.  But that is not actually relevant,
+;; since there are no instructions that contend for memory access early.
+;; We could, of course, provide exact scheduling information for specific
+;; sfuncs, if that should prove useful.
+
+(define_insn_reservation "sh4_300_call" 16
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "call,sfunc"))
+  "sh4_300_issue+sh4_300_br,all*15")
+
+;; FMOV.S / FMOV.D
+(define_insn_reservation "sh4_300_fmov" 1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fmove"))
+  "sh4_300_issue+sh4_300_fpt")
+
+;; LDS to FPSCR
+(define_insn_reservation "sh4_300_fpscr_load" 8
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "gp_fpscr"))
+  "sh4_300_issue+sh4_300_fpu+sh4_300_fpt")
+
+;; LDS.L to FPSCR
+(define_insn_reservation "sh4_300_fpscr_load_mem" 8
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type"  "mem_fpscr"))
+  "sh4_300_issue+sh4_300_fpu+sh4_300_fpt+sh4_300_ls")
+
+
+;; Fixed point multiplication (DMULS.L DMULU.L MUL.L MULS.W,MULU.W)
+(define_insn_reservation "multi" 2
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "smpy,dmpy"))
+  "sh4_300_issue+sh4_300_mul")
+
+;; FPCHG, FRCHG, FSCHG
+(define_insn_reservation "fpscr_toggle"  1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fpscr_toggle"))
+  "sh4_300_issue+sh4_300_fpu+sh4_300_fpt")
+
+;; FCMP/EQ, FCMP/GT
+(define_insn_reservation "fp_cmp"  3
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fp_cmp,dfp_cmp"))
+  "sh4_300_issue+sh4_300_fpu")
+
+;; Single precision floating point (FADD,FLOAT,FMAC,FMUL,FSUB,FTRC)
+;; Double-precision floating-point (FADD,FCNVDS,FCNVSD,FLOAT,FSUB,FTRC)
+(define_insn_reservation "fp_arith"  6
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fp,ftrc_s,dfp_arith,dfp_conv"))
+  "sh4_300_issue+sh4_300_fpu")
+
+;; Single Precision FDIV/SQRT
+(define_insn_reservation "fp_div" 19
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fdiv"))
+  "sh4_300_issue+sh4_300_fpu+sh4_300_fds,sh4_300_fds*15")
+
+;; Double-precision floating-point FMUL
+(define_insn_reservation "dfp_mul" 9
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "dfp_mul"))
+  "sh4_300_issue+sh4_300_fpu,sh4_300_fpu*3")
+
+;; Double precision FDIV/SQRT
+(define_insn_reservation "dp_div" 35
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "dfdiv"))
+  "sh4_300_issue+sh4_300_fpu+sh4_300_fds,sh4_300_fds*31")
+
+
+;; ??? We don't really want these for sh4-300.
+;; this pattern itself is likely to finish in 3 cycles, but also
+;; to disrupt branch prediction for taken branches for the following
+;; condbranch.
+(define_insn_reservation "sh4_300_arith3" 5
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "arith3"))
+  "sh4_300_issue,all*4")
+
+;; arith3b insns without brach redirection make use of the 0-offset 0-latency
+;; branch feature, and thus schedule the same no matter if the branch is taken
+;; or not.  If the branch is redirected, the taken branch might take longer,
+;; but then, we don't have to take the next branch.
+;; ??? should we suppress branch redirection for sh4-300 to improve branch
+;; target hit rates?
+(define_insn_reservation "arith3b" 2
+  (and (eq_attr "pipe_model" "sh4")
+       (eq_attr "type" "arith3"))
+  "issue,all")
Index: gcc/config/sh/t-linux
===================================================================
--- gcc/config/sh/t-linux	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/t-linux	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,5 +1,4 @@
-TARGET_LIBGCC2_CFLAGS = -fpic -DNO_FPSCR_VALUES
-LIB1ASMFUNCS_CACHE = _ic_invalidate _ic_invalidate_array
+TARGET_LIBGCC2_CFLAGS += -fpic -DNO_FPSCR_VALUES
 
 LIB2FUNCS_EXTRA= $(srcdir)/config/sh/linux-atomic.asm
 
Index: gcc/config/sh/superh64.h
===================================================================
Index: gcc/config/sh/t-elf
===================================================================
--- gcc/config/sh/t-elf	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/t-elf	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,5 +1,6 @@
 EXTRA_MULTILIB_PARTS= crt1.o crti.o crtn.o \
-	crtbegin.o crtend.o crtbeginS.o crtendS.o $(IC_EXTRA_PARTS) $(OPT_EXTRA_PARTS)
+	crtbegin.o crtend.o crtbeginS.o crtendS.o $(IC_EXTRA_PARTS) \
+	$(OPT_EXTRA_PARTS) trap-handler.o
 
 # Compile crtbeginS.o and crtendS.o with pic.
 CRTSTUFF_T_CFLAGS_S = -fPIC
Index: gcc/config/sh/sh.md
===================================================================
--- gcc/config/sh/sh.md	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/config/sh/sh.md	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -3,6 +3,7 @@
 ;;  2003, 2004, 2005, 2006, 2007 Free Software Foundation, Inc.
 ;;  Contributed by Steve Chamberlain (sac@cygnus.com).
 ;;  Improved by Jim Wilson (wilson@cygnus.com).
+;;  Copyright (c) 2009  STMicroelectronics.
 
 ;; This file is part of GCC.
 
@@ -46,6 +47,8 @@
 ;;    l -- pr
 ;;    z -- r0
 ;;
+;;    R03 -- r0, r1, r2 or r3  - experimental constraint for SH4-300
+;;
 ;; Special formats used for outputting SH instructions:
 ;;
 ;;   %.  --  print a .s if insn needs delay slot
@@ -106,6 +109,7 @@
   (DR0_REG	64)
   (DR2_REG	66)
   (DR4_REG	68)
+  (FR4_REG	68)
   (FR23_REG	87)
 
   (TR0_REG	128)
@@ -161,6 +165,17 @@
   (UNSPECV_CONST8	6)
   (UNSPECV_WINDOW_END	10)
   (UNSPECV_CONST_END	11)
+  (UNSPEC_DB_INSN	12)
+
+  ;; NaN handling for software floating point:
+  ;; We require one bit specific for a precision to be set in all NaNs,
+  ;; so that we can test them with a not / tst sequence.
+  ;; ??? Ironically, this is the quiet bit for now, because that is the
+  ;; only bit set by __builtin_nan ("").
+  ;; ??? Should really use one bit lower and force it set by using
+  ;; a custom encoding function.
+  (SF_NAN_MASK		0x7fc00000)
+  (DF_NAN_MASK		0x7ff80000)
 ])
 
 ;; -------------------------------------------------------------------------
@@ -202,7 +217,9 @@
 ;; load_si	Likewise, SImode variant for general register.
 ;; fload	Likewise, but load to fp register.
 ;; store	to memory
+;; fstore	floating point register to memory
 ;; move		general purpose register to register
+;; movi8	8 bit immediate to general purpose register
 ;; mt_group	other sh4 mt instructions
 ;; fmove	register to register, floating point
 ;; smpy		word precision integer multiply
@@ -219,11 +236,15 @@
 ;; sfunc	special function call with known used registers
 ;; call		function call
 ;; fp		floating point
+;; fpscr_toggle	toggle a bit in the fpscr
 ;; fdiv		floating point divide (or square root)
 ;; gp_fpul	move from general purpose register to fpul
 ;; fpul_gp	move from fpul to general purpose register
 ;; mac_gp	move from mac[lh] to general purpose register
-;; dfp_arith, dfp_cmp,dfp_conv
+;; gp_mac	move from general purpose register to mac[lh]
+;; mac_mem	move from mac[lh] to memory
+;; mem_mac	move from memory to mac[lh]
+;; dfp_arith,dfp_mul, fp_cmp,dfp_cmp,dfp_conv
 ;; ftrc_s	fix_truncsfsi2_i4
 ;; dfdiv	double precision floating point divide (or square root)
 ;; cwb		ic_invalidate_line_i
@@ -261,7 +282,7 @@
 ;; nil		no-op move, will be deleted.
 
 (define_attr "type"
- "mt_group,cbranch,jump,jump_ind,arith,arith3,arith3b,dyn_shift,load,load_si,fload,store,move,fmove,smpy,dmpy,return,pload,prset,pstore,prget,pcload,pcload_si,pcfload,rte,sfunc,call,fp,fdiv,ftrc_s,dfp_arith,dfp_cmp,dfp_conv,dfdiv,gp_fpul,fpul_gp,mac_gp,mem_fpscr,gp_fpscr,cwb,movua,fsrra,fsca,tls_load,arith_media,cbranch_media,cmp_media,dfdiv_media,dfmul_media,dfparith_media,dfpconv_media,dmpy_media,fcmp_media,fdiv_media,fload_media,fmove_media,fparith_media,fpconv_media,fstore_media,gettr_media,invalidate_line_media,jump_media,load_media,pt_media,ptabs_media,store_media,mcmp_media,mac_media,d2mpy_media,atrans_media,ustore_media,nil,other"
+ "mt_group,cbranch,jump,jump_ind,arith,arith3,arith3b,dyn_shift,load,load_si,fload,store,fstore,move,movi8,fmove,smpy,dmpy,return,pload,prset,pstore,prget,pcload,pcload_si,pcfload,rte,sfunc,call,fp,fpscr_toggle,fdiv,ftrc_s,dfp_arith,dfp_mul,fp_cmp,dfp_cmp,dfp_conv,dfdiv,gp_fpul,fpul_gp,mac_gp,gp_mac,mac_mem,mem_mac,mem_fpscr,gp_fpscr,cwb,movua,fsrra,fsca,tls_load,arith_media,cbranch_media,cmp_media,dfdiv_media,dfmul_media,dfparith_media,dfpconv_media,dmpy_media,fcmp_media,fdiv_media,fload_media,fmove_media,fparith_media,fpconv_media,fstore_media,gettr_media,invalidate_line_media,jump_media,load_media,pt_media,ptabs_media,store_media,mcmp_media,mac_media,d2mpy_media,atrans_media,ustore_media,nil,other"
   (const_string "other"))
 
 ;; We define a new attribute namely "insn_class".We use
@@ -277,12 +298,12 @@
 (define_attr "insn_class"
   "mt_group,ex_group,ls_group,br_group,fe_group,co_group,none"
   (cond [(eq_attr "type" "move,mt_group") (const_string "mt_group")
-         (eq_attr "type" "arith,dyn_shift") (const_string "ex_group")
-	 (eq_attr "type" "fmove,load,pcload,load_si,pcload_si,fload,pcfload,store,gp_fpul,fpul_gp") (const_string "ls_group")
+         (eq_attr "type" "movi8,arith,dyn_shift") (const_string "ex_group")
+	 (eq_attr "type" "fmove,load,pcload,load_si,pcload_si,fload,pcfload,store,fstore,gp_fpul,fpul_gp") (const_string "ls_group")
 	 (eq_attr "type" "cbranch,jump") (const_string "br_group")
-	 (eq_attr "type" "fp,fdiv,ftrc_s,dfp_arith,dfp_conv,dfdiv")
+	 (eq_attr "type" "fp,fp_cmp,fdiv,ftrc_s,dfp_arith,dfp_mul,dfp_conv,dfdiv")
 	   (const_string "fe_group")
-	 (eq_attr "type" "jump_ind,smpy,dmpy,mac_gp,return,pload,prset,pstore,prget,rte,sfunc,call,dfp_cmp,mem_fpscr,gp_fpscr,cwb") (const_string "co_group")]
+	 (eq_attr "type" "jump_ind,smpy,dmpy,mac_gp,return,pload,prset,pstore,prget,rte,sfunc,call,dfp_cmp,mem_fpscr,gp_fpscr,cwb,gp_mac,mac_mem,mem_mac") (const_string "co_group")]
 	(const_string "none")))
 ;; nil are zero instructions, and arith3 / arith3b are multiple instructions,
 ;; so these do not belong in an insn group, although they are modeled
@@ -454,6 +475,12 @@
 
 (define_attr "needs_delay_slot" "yes,no" (const_string "no"))
 
+(define_attr "banked" "yes,no"
+	(cond [(eq (symbol_ref "sh_loads_bankedreg_p (insn)")
+		   (const_int 1))
+	       (const_string "yes")]
+	      (const_string "no")))
+
 ;; ??? This should be (nil) instead of (const_int 0)
 (define_attr "hit_stack" "yes,no"
 	(cond [(eq (symbol_ref "find_regno_note (insn, REG_INC, SP_REG)")
@@ -494,14 +521,14 @@
 ;; SH4 Double-precision computation with double-precision result -
 ;; the two halves are ready at different times.
 (define_attr "dfp_comp" "yes,no"
-  (cond [(eq_attr "type" "dfp_arith,dfp_conv,dfdiv") (const_string "yes")]
+  (cond [(eq_attr "type" "dfp_arith,dfp_mul,dfp_conv,dfdiv") (const_string "yes")]
 	(const_string "no")))
 
 ;; Insns for which the latency of a preceding fp insn is decreased by one.
 (define_attr "late_fp_use" "yes,no" (const_string "no"))
 ;; And feeding insns for which this relevant.
 (define_attr "any_fp_comp" "yes,no"
-  (cond [(eq_attr "type" "fp,fdiv,ftrc_s,dfp_arith,dfp_conv,dfdiv")
+  (cond [(eq_attr "type" "fp,fdiv,ftrc_s,dfp_arith,dfp_mul,dfp_conv,dfdiv")
 	 (const_string "yes")]
 	(const_string "no")))
 
@@ -532,8 +559,9 @@
 		  (eq_attr "type" "!pload,prset"))
 	     (and (eq_attr "interrupt_function" "yes")
 		  (ior
-		   (ne (symbol_ref "TARGET_SH3") (const_int 0))
-		   (eq_attr "hit_stack" "no"))))) (nil) (nil)])
+		   (eq (symbol_ref "TARGET_SH3") (const_int 0))
+		   (eq_attr "hit_stack" "no")
+		   (eq_attr "banked" "no"))))) (nil) (nil)])
 
 ;; Since a call implicitly uses the PR register, we can't allow
 ;; a PR register store in a jsr delay slot.
@@ -588,6 +616,14 @@
 	cmp/eq	%1,%0"
    [(set_attr "type" "mt_group")])
 
+(define_insn "fpcmp_i1"
+  [(set (reg:SI T_REG)
+	(match_operator:SI 1 "soft_fp_comparison_operator"
+	  [(match_operand 0 "soft_fp_comparison_operand" "r") (const_int 0)]))]
+  "TARGET_SH1_SOFTFP"
+  "tst	%0,%0"
+   [(set_attr "type" "mt_group")])
+
 (define_insn "cmpgtsi_t"
   [(set (reg:SI T_REG)
 	(gt:SI (match_operand:SI 0 "arith_reg_operand" "r,r")
@@ -609,15 +645,37 @@
    [(set_attr "type" "mt_group")])
 
 ;; -------------------------------------------------------------------------
+;; SImode compare and branch
+;; -------------------------------------------------------------------------
+
+(define_expand "cbranchsi4"
+  [(set (pc)
+	(if_then_else (match_operator 0 "comparison_operator"
+			[(match_operand:SI 1 "arith_operand" "")
+			 (match_operand:SI 2 "arith_operand" "")])
+		      (label_ref (match_operand 3 "" ""))
+		      (pc)))
+   (clobber (reg:SI T_REG))]
+  "TARGET_SH1"
+  "expand_cbranchsi4 (operands, CODE_FOR_nothing, -1); DONE;")
+
+;; -------------------------------------------------------------------------
 ;; SImode unsigned integer comparisons
 ;; -------------------------------------------------------------------------
 
-(define_insn "cmpgeusi_t"
+(define_insn_and_split "cmpgeusi_t"
   [(set (reg:SI T_REG)
 	(geu:SI (match_operand:SI 0 "arith_reg_operand" "r")
-		(match_operand:SI 1 "arith_reg_operand" "r")))]
+		(match_operand:SI 1 "arith_reg_or_0_operand" "rN")))]
   "TARGET_SH1"
   "cmp/hs	%1,%0"
+  "&& operands[0] == CONST0_RTX (SImode)"
+  [(pc)]
+  "
+{
+  emit_insn (gen_sett ());
+  DONE;
+}"
    [(set_attr "type" "mt_group")])
 
 (define_insn "cmpgtusi_t"
@@ -647,12 +705,64 @@
 }")
 
 ;; -------------------------------------------------------------------------
-;; DImode signed integer comparisons
+;; DImode compare and branch
 ;; -------------------------------------------------------------------------
 
-;; ??? Could get better scheduling by splitting the initial test from the
-;; rest of the insn after reload.  However, the gain would hardly justify
-;; the sh.md size increase necessary to do that.
+
+;; arith3 patterns don't work well with the sh4-300 branch prediction mechanism.
+;; Therefore, we aim to have a set of three branches that go straight to the
+;; destination, i.e. only one of them is taken at any one time.
+;; This mechanism should also be slightly better for the sh4-200.
+
+(define_expand "cbranchdi4"
+  [(parallel [(set (pc)
+		   (if_then_else (match_operator 0 "comparison_operator"
+				   [(match_operand:DI 1 "arith_operand" "")
+				    (match_operand:DI 2 "arith_operand" "")])
+				 (label_ref (match_operand 3 "" ""))
+				 (pc)))
+	      (clobber (match_dup 4))
+	      (clobber (reg:SI T_REG))])]
+  "TARGET_CBRANCHDI4"
+  "
+{
+  enum rtx_code comparison;
+
+  if (TARGET_EXPAND_CBRANCHDI4)
+    {
+      if (expand_cbranchdi4 (operands, CODE_FOR_nothing))
+	DONE;
+    }
+  comparison = prepare_cbranch_operands (operands, DImode, CODE_FOR_nothing);
+  if (comparison != GET_CODE (operands[0]))
+    operands[0]
+      = gen_rtx_fmt_ee (VOIDmode, comparison, operands[1], operands[2]);
+   operands[4] = gen_rtx_SCRATCH (SImode);
+}")
+
+(define_insn_and_split "cbranchdi4_i"
+  [(set (pc)
+	(if_then_else (match_operator 0 "comparison_operator"
+			[(match_operand:DI 1 "arith_operand" "r,r")
+			 (match_operand:DI 2 "arith_operand" "rN,i")])
+		      (label_ref (match_operand 3 "" ""))
+		      (pc)))
+   (clobber (match_scratch:SI 4 "=X,&r"))
+   (clobber (reg:SI T_REG))]
+  "TARGET_CBRANCHDI4"
+  "#"
+  "&& reload_completed"
+  [(pc)]
+  "
+{
+  if (!expand_cbranchdi4 (operands, GET_CODE (operands[0])))
+    FAIL;
+  DONE;
+}")
+
+;; -------------------------------------------------------------------------
+;; DImode signed integer comparisons
+;; -------------------------------------------------------------------------
 
 (define_insn ""
   [(set (reg:SI T_REG)
@@ -1131,7 +1241,7 @@
 
 (define_insn "*movsicc_t_false"
   [(set (match_operand:SI 0 "arith_reg_dest" "=r,r")
-	(if_then_else (eq (reg:SI T_REG) (const_int 0))
+	(if_then_else:SI (eq (reg:SI T_REG) (const_int 0))
 		      (match_operand:SI 1 "general_movsrc_operand" "r,I08")
 		      (match_operand:SI 2 "arith_reg_operand" "0,0")))]
   "TARGET_PRETEND_CMOVE
@@ -1144,7 +1254,7 @@
 
 (define_insn "*movsicc_t_true"
   [(set (match_operand:SI 0 "arith_reg_dest" "=r,r")
-	(if_then_else (ne (reg:SI T_REG) (const_int 0))
+	(if_then_else:SI (ne (reg:SI T_REG) (const_int 0))
 		      (match_operand:SI 1 "general_movsrc_operand" "r,I08")
 		      (match_operand:SI 2 "arith_reg_operand" "0,0")))]
   "TARGET_PRETEND_CMOVE
@@ -1373,7 +1483,7 @@
 		       true_regnum (operands[2])
 		       + (TARGET_LITTLE_ENDIAN ? 1 : 0));
   emit_insn (gen_clrt ());
-  emit_insn (gen_addc (low0, low0, gen_lowpart (SImode, operands[2])));
+  emit_insn (gen_addc(low0, low0, gen_lowpart (SImode, operands[2])));
   emit_insn (gen_addc1 (high0, high0, high2));
   DONE;
 }")
@@ -2755,6 +2865,64 @@
   "mul.l	%1,%0"
   [(set_attr "type" "dmpy")])
 
+(define_insn "mulr03"
+  [(set (match_operand:SI 0 "arith_reg_operand" "=r")
+	(mult:SI (match_operand:SI 1 "arith_reg_operand" "%0")
+		 (match_operand:SI 2 "arith_reg_operand" "R03")))]
+  "TARGET_R0R3_TO_REG_MUL - !reload_completed >= 1"
+  "mulr	%2,%0"
+  [(set_attr "type" "dmpy")])
+
+(define_peephole2
+  [(set (reg:SI MACL_REG)
+	(mult:SI (match_operand:SI 1 "arith_reg_operand" "r,R03")
+		 (match_operand:SI 2 "arith_reg_operand" "R03,r")))
+   (set (match_operand:SI 0 "arith_reg_dest" "=r,r") (reg:SI MACL_REG))]
+  "TARGET_R0R3_TO_REG_MUL
+   && peep2_regno_dead_p (2, MACL_REG)
+   && ((!reg_overlap_mentioned_p (operands[0], operands[1])
+	&& true_regnum (operands[1]) <= R3_REG)
+       || (!reg_overlap_mentioned_p (operands[0], operands[2])
+	   && true_regnum (operands[2]) <= R3_REG))"
+  [(set (match_dup 0) (match_dup 3))
+   (set (match_dup 0) (mult:SI (match_dup 0) (match_dup 4)))]
+  "
+{
+  if (reg_overlap_mentioned_p (operands[0], operands[1])
+      || true_regnum (operands[1]) > R3_REG)
+    {
+      operands[4] = operands[2];
+      operands[3] = operands[1];
+    }
+  else
+    {
+      operands[4] = operands[1];
+      operands[3] = operands[2];
+    }
+}")
+
+(define_peephole2
+  [(match_scratch:SI 3 "R03")
+   (set (reg:SI MACL_REG)
+	(mult:SI (match_operand:SI 1 "arith_reg_operand" "r,0")
+		 (match_operand:SI 2 "arith_reg_operand" "0,r")))
+   (set (match_operand:SI 0 "arith_reg_dest" "=r,r") (reg:SI MACL_REG))]
+  "TARGET_R0R3_TO_REG_MUL
+   && peep2_regno_dead_p (3, MACL_REG)
+   && (true_regnum (operands[1]) == true_regnum (operands[0])
+       || true_regnum (operands[2]) == true_regnum (operands[0]))"
+  [(set (match_dup 3) (match_dup 4))
+   (set (match_dup 0) (mult:SI (match_dup 0) (match_dup 3)))]
+  "
+{
+  if (true_regnum (operands[1]) == true_regnum (operands[0]))
+    operands[4] = operands[2];
+  else
+    operands[4] = operands[1];
+}")
+
+;; ??? should we also use mulr if we'd need two reg-reg copies?
+
 (define_expand "mulsi3"
   [(set (reg:SI MACL_REG)
 	(mult:SI  (match_operand:SI 1 "arith_reg_operand" "")
@@ -2766,7 +2934,12 @@
 {
   rtx first, last;
 
-  if (!TARGET_SH2)
+  if (TARGET_R0R3_TO_REG_MUL == 2)
+    {
+      emit_insn (gen_mulr03 (operands[0], operands[1], operands[2]));
+      DONE;
+    }
+  else if (!TARGET_SH2)
     {
       /* The address must be set outside the libcall,
 	 since it goes into a pseudo.  */
@@ -3631,44 +3804,28 @@
 ;; code, so just let the machine independent code widen the mode.
 ;; That's why we don't have ashrhi3_k / lshrhi3_k / lshrhi3_m / lshrhi3 .
 
-
-;; ??? This should be a define expand.
-
-(define_insn "ashrsi2_16"
-  [(set (match_operand:SI 0 "arith_reg_dest" "=r")
-        (ashiftrt:SI (match_operand:SI 1 "arith_reg_operand" "r")
-                     (const_int 16)))]
-  "TARGET_SH1"
-  "#"
-  [(set_attr "length" "4")])
-
-(define_split
-  [(set (match_operand:SI 0 "arith_reg_dest" "")
+(define_expand "ashrsi2_16"
+  [(parallel [(set (match_operand:SI 0 "arith_reg_dest" "")
         (ashiftrt:SI (match_operand:SI 1 "arith_reg_operand" "")
-		     (const_int 16)))]
+				(const_int 16)))
+	      (clobber (reg:SI T_REG))])]
   "TARGET_SH1"
-  [(set (match_dup 0) (rotate:SI (match_dup 1) (const_int 16)))
-   (set (match_dup 0) (sign_extend:SI (match_dup 2)))]
-  "operands[2] = gen_lowpart (HImode, operands[0]);")
+  "
+{
+  rtx low0 = gen_lowpart (HImode, operands[0]);
 
-;; ??? This should be a define expand.
+  emit_insn (gen_rotlsi3_16 (operands[0], operands[1]));
+  emit_insn (gen_extendhisi2 (operands[0], low0));
+  DONE;
+}
+")
 
-(define_insn "ashrsi2_31"
-  [(set (match_operand:SI 0 "arith_reg_dest" "=r")
+(define_expand "ashrsi2_31"
+  [(parallel [(set (match_operand:SI 0 "arith_reg_dest" "=r")
 	(ashiftrt:SI (match_operand:SI 1 "arith_reg_operand" "0")
 		     (const_int 31)))
-   (clobber (reg:SI T_REG))]
-  "TARGET_SH1"
-  "#"
-  [(set_attr "length" "4")])
-
-(define_split
-  [(set (match_operand:SI 0 "arith_reg_dest" "")
-	(ashiftrt:SI (match_operand:SI 1 "arith_reg_operand" "")
-		     (const_int 31)))
-   (clobber (reg:SI T_REG))]
+	      (clobber (reg:SI T_REG))])]
   "TARGET_SH1"
-  [(const_int 0)]
   "
 {
   emit_insn (gen_ashlsi_c (operands[0], operands[1]));
@@ -4736,7 +4893,7 @@
   [(set (mem:SF (pre_dec:SI (reg:SI SP_REG))) (reg:SF FPUL_REG))]
   "TARGET_SH2E && ! TARGET_SH5"
   "sts.l	fpul,@-r15"
-  [(set_attr "type" "store")
+  [(set_attr "type" "fstore")
    (set_attr "late_fp_use" "yes")
    (set_attr "hit_stack" "yes")])
 
@@ -4759,12 +4916,22 @@
   "")
 
 (define_insn "pop_fpul"
-  [(set (reg:SF FPUL_REG) (mem:SF (post_inc:SI (reg:SI SP_REG))))]
-  "TARGET_SH2E && ! TARGET_SH5"
+  [(parallel [(set (reg:SF FPUL_REG) (mem:SF (post_inc:SI (reg:SI SP_REG))))
+	      (clobber (scratch:SI))])]
+  "TARGET_SH1 && ! TARGET_SH5"
   "lds.l	@r15+,fpul"
   [(set_attr "type" "load")
    (set_attr "hit_stack" "yes")])
 
+(define_insn "pop_fpul2"
+  [(parallel [(set (reg:SF FPUL_REG)
+	(mem:SF (post_inc:SI (match_operand:SI 0 "register_operand" "r"))))
+	      (clobber (scratch:SI))])]
+  ""
+  "lds.l      @%0+,fpul"
+  [(set_attr "type" "load")
+   (set_attr "hit_stack" "no")])
+
 (define_expand "pop_4"
   [(parallel [(set (match_operand:DF 0 "" "")
 		   (mem:DF (post_inc:SI (reg:SI SP_REG))))
@@ -4818,9 +4985,9 @@
 ;; (made from (set (subreg:SI (reg:QI ###) 0) ) into T.
 (define_insn "movsi_i"
   [(set (match_operand:SI 0 "general_movdst_operand"
-	    "=r,r,t,r,r,r,r,m,<,<,x,l,x,l,r")
+	    "=r,r,r,t,r,r,r,r,m,<,<,x,l,x,l,r")
 	(match_operand:SI 1 "general_movsrc_operand"
-	 "Q,rI08,r,mr,x,l,t,r,x,l,r,r,>,>,i"))]
+	 "Q,r,I08,r,mr,x,l,t,r,x,l,r,r,>,>,i"))]
   "TARGET_SH1
    && ! TARGET_SH2E
    && ! TARGET_SH2A
@@ -4829,6 +4996,7 @@
   "@
 	mov.l	%1,%0
 	mov	%1,%0
+	mov	%1,%0
 	cmp/pl	%1
 	mov.l	%1,%0
 	sts	%1,%0
@@ -4842,8 +5010,8 @@
 	lds.l	%1,%0
 	lds.l	%1,%0
 	fake	%1,%0"
-  [(set_attr "type" "pcload_si,move,mt_group,load_si,mac_gp,prget,move,store,store,pstore,move,prset,load,pload,pcload_si")
-   (set_attr "length" "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*")])
+  [(set_attr "type" "pcload_si,move,movi8,mt_group,load_si,mac_gp,prget,arith,store,mac_mem,pstore,gp_mac,prset,mem_mac,pload,pcload_si")
+   (set_attr "length" "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*")])
 
 ;; t/r must come after r/r, lest reload will try to reload stuff like
 ;; (subreg:SI (reg:SF FR14_REG) 0) into T (compiling stdlib/strtod.c -m3e -O2)
@@ -4853,15 +5021,16 @@
 ;; TARGET_FMOVD is in effect, and mode switching is done before reload.
 (define_insn "movsi_ie"
   [(set (match_operand:SI 0 "general_movdst_operand"
-	    "=r,r,r,t,r,r,r,r,m,<,<,x,l,x,l,y,<,r,y,r,*f,y,*f,y")
+	    "=r,r,r,r,t,r,r,r,r,m,<,<,x,l,x,l,y,<,r,y,r,*f,y,*f,y")
 	(match_operand:SI 1 "general_movsrc_operand"
-	 "Q,rI08,I20,r,mr,x,l,t,r,x,l,r,r,>,>,>,y,i,r,y,y,*f,*f,y"))]
+	 "Q,r,I08,I20,r,mr,x,l,t,r,x,l,r,r,>,>,>,y,i,r,y,y,*f,*f,y"))]
   "(TARGET_SH2E || TARGET_SH2A)
    && (register_operand (operands[0], SImode)
        || register_operand (operands[1], SImode))"
   "@
 	mov.l	%1,%0
 	mov	%1,%0
+	mov	%1,%0
 	movi20	%1,%0
 	cmp/pl	%1
 	mov.l	%1,%0
@@ -4884,26 +5053,27 @@
 	flds	%1,fpul
 	fmov	%1,%0
 	! move optimized away"
-  [(set_attr "type" "pcload_si,move,move,*,load_si,mac_gp,prget,move,store,store,pstore,move,prset,load,pload,load,store,pcload_si,gp_fpul,fpul_gp,fmove,fmove,fmove,nil")
-   (set_attr "late_fp_use" "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,yes,*,*,yes,*,*,*,*")
-   (set_attr "length" "*,*,4,*,4,*,*,*,4,*,*,*,*,*,*,*,*,*,*,*,*,*,*,0")])
+  [(set_attr "type" "pcload_si,move,movi8,move,*,load_si,mac_gp,prget,arith,store,mac_mem,pstore,gp_mac,prset,mem_mac,pload,load,fstore,pcload_si,gp_fpul,fpul_gp,fmove,fmove,fmove,nil")
+   (set_attr "late_fp_use" "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,yes,*,*,yes,*,*,*,*")
+   (set_attr "length" "*,*,*,4,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,0")])
 
 (define_insn "movsi_i_lowpart"
-  [(set (strict_low_part (match_operand:SI 0 "general_movdst_operand" "+r,r,r,r,r,r,m,r"))
-	(match_operand:SI 1 "general_movsrc_operand" "Q,rI08,mr,x,l,t,r,i"))]
+  [(set (strict_low_part (match_operand:SI 0 "general_movdst_operand" "+r,r,r,r,r,r,r,m,r"))
+	(match_operand:SI 1 "general_movsrc_operand" "Q,r,I08,mr,x,l,t,r,i"))]
    "TARGET_SH1
     && (register_operand (operands[0], SImode)
         || register_operand (operands[1], SImode))"
   "@
 	mov.l	%1,%0
 	mov	%1,%0
+	mov	%1,%0
 	mov.l	%1,%0
 	sts	%1,%0
 	sts	%1,%0
 	movt	%0
 	mov.l	%1,%0
 	fake	%1,%0"
-  [(set_attr "type" "pcload,move,load,move,prget,move,store,pcload")])
+  [(set_attr "type" "pcload,move,arith,load,mac_gp,prget,arith,store,pcload")])
 
 (define_insn_and_split "load_ra"
   [(set (match_operand:SI 0 "general_movdst_operand" "")
@@ -5070,7 +5240,7 @@
       emit_insn (gen_ic_invalidate_line_compact (operands[0], operands[1]));
       DONE;
     }
-  else if (TARGET_SH4A_ARCH)
+  else if (TARGET_SH4A_ARCH || TARGET_SH4_300)
     {
       emit_insn (gen_ic_invalidate_line_sh4a (operands[0]));
       DONE;
@@ -5098,7 +5268,7 @@
 (define_insn "ic_invalidate_line_sh4a"
   [(unspec_volatile [(match_operand:SI 0 "register_operand" "r")]
 		    UNSPEC_ICACHE)]
-  "TARGET_SH4A_ARCH"
+  "TARGET_SH4A_ARCH || TARGET_SH4_300"
   "ocbwb\\t@%0\;synco\;icbi\\t@%0"
   [(set_attr "length" "16")
    (set_attr "type" "cwb")])
@@ -5155,19 +5325,20 @@
    (set_attr "needs_delay_slot" "yes")])
 
 (define_insn "movqi_i"
-  [(set (match_operand:QI 0 "general_movdst_operand" "=r,r,m,r,r,l")
-	(match_operand:QI 1 "general_movsrc_operand"  "ri,m,r,t,l,r"))]
+  [(set (match_operand:QI 0 "general_movdst_operand" "=r,r,r,m,r,r,l")
+	(match_operand:QI 1 "general_movsrc_operand"  "r,i,m,r,t,l,r"))]
   "TARGET_SH1
    && (arith_reg_operand (operands[0], QImode)
        || arith_reg_operand (operands[1], QImode))"
   "@
 	mov	%1,%0
+	mov	%1,%0
 	mov.b	%1,%0
 	mov.b	%1,%0
 	movt	%0
 	sts	%1,%0
 	lds	%1,%0"
- [(set_attr "type" "move,load,store,move,move,move")])
+ [(set_attr "type" "move,movi8,load,store,arith,prget,prset")])
 
 (define_insn "*movqi_media"
   [(set (match_operand:QI 0 "general_movdst_operand" "=r,r,r,m")
@@ -5769,7 +5940,7 @@
       (if_then_else
        (ne (symbol_ref "TARGET_SHCOMPACT") (const_int 0))
        (const_int 10) (const_int 8))])
-   (set_attr "type" "fmove,move,pcfload,fload,store,pcload,load,store,load,fload")
+   (set_attr "type" "fmove,move,pcfload,fload,fstore,pcload,load,store,load,fload")
    (set_attr "late_fp_use" "*,*,*,*,yes,*,*,*,*,*")
    (set (attr "fp_mode") (if_then_else (eq_attr "fmovd" "yes")
 					   (const_string "double")
@@ -6486,9 +6657,9 @@
 	sts.l	%1,%0
 	lds.l	%1,%0
 	! move optimized away"
-  [(set_attr "type" "fmove,move,fmove,fmove,pcfload,fload,store,pcload,load,store,fmove,fmove,load,*,fpul_gp,gp_fpul,store,load,nil")
+  [(set_attr "type" "fmove,move,fmove,fmove,pcfload,fload,fstore,pcload,load,store,fmove,fmove,load,*,fpul_gp,gp_fpul,fstore,load,nil")
    (set_attr "late_fp_use" "*,*,*,*,*,*,yes,*,*,*,*,*,*,*,yes,*,yes,*,*")
-   (set_attr "length" "*,*,*,*,4,4,4,*,*,*,2,2,2,4,2,2,2,2,0")
+   (set_attr "length" "*,*,*,*,4,2,2,*,*,*,2,2,2,4,2,2,2,2,0")
    (set (attr "fp_mode") (if_then_else (eq_attr "fmovd" "yes")
 					   (const_string "single")
 					   (const_string "none")))])
@@ -6612,7 +6783,17 @@
 
 (define_insn "*movsi_y"
   [(set (match_operand:SI 0 "register_operand" "=y,y")
-	(match_operand:SI 1 "immediate_operand" "Qi,I08"))
+	(match_operand 1 "immediate_operand" "Qi,I08"))
+   (clobber (match_scratch:SI 2 "=&z,r"))]
+  "TARGET_SH2E
+   && (reload_in_progress || reload_completed)"
+  "#"
+  [(set_attr "length" "4")
+   (set_attr "type" "pcload,move")])
+
+(define_insn "*movsf_y"
+  [(set (match_operand:SF 0 "register_operand" "=y,y")
+	(match_operand 1 "immediate_operand" "Qi,I08"))
    (clobber (match_scratch:SI 2 "=&z,r"))]
   "TARGET_SH2E
    && (reload_in_progress || reload_completed)"
@@ -6695,6 +6876,59 @@
 
 ;; Conditional branch insns
 
+(define_expand "cbranchsf4"
+  [(set (pc) (if_then_else
+	      (match_operator 0 "comparison_operator"
+	       [(match_operand:SF 1 "register_operand" "")
+	        (match_operand:SF 2 "nonmemory_operand" "")])
+	      (label_ref (match_operand 3 "" ""))
+	      (pc)))]
+  "TARGET_SH1"
+  "
+{
+  sh_expand_float_cbranch (operands);
+  DONE;
+}")
+
+(define_expand "cbranchdf4"
+  [(set (pc) (if_then_else
+	      (match_operator 0 "comparison_operator"
+	       [(match_operand:DF 1 "register_operand" "")
+	        (match_operand:DF 2 "nonmemory_operand" "")])
+	      (label_ref (match_operand 3 "" ""))
+	      (pc)))]
+  "TARGET_SH1"
+  "
+{
+  sh_expand_float_cbranch (operands);
+  DONE;
+}")
+
+;; ??? cstore<mode> patterns cause compiler crashes.
+;;(define_expand "cstoresf4"
+;;  [(set (match_operand:SI 3 "arith_reg_operand" "")
+;;	      (match_operator:SI 0 "comparison_operator"
+;;	       [(match_operand:SF 1 "register_operand" "")
+;;	        (match_operand:SF 2 "nonmemory_operand" "")]))]
+;;  "TARGET_SH1"
+;;  "
+;;{
+;;  sh_expand_float_scc (operands);
+;;  DONE;
+;;}")
+;;
+;;(define_expand "cstoredf4"
+;;  [(set (match_operand:SI 3 "arith_reg_operand" "")
+;;	      (match_operator:SI 0 "comparison_operator"
+;;	       [(match_operand:DF 1 "register_operand" "")
+;;	        (match_operand:DF 2 "nonmemory_operand" "")]))]
+;;  "TARGET_SH1"
+;;  "
+;;{
+;;  sh_expand_float_scc (operands);
+;;  DONE;
+;;}")
+
 (define_expand "beq_media"
   [(set (pc)
 	(if_then_else (eq (match_operand:DI 1 "arith_reg_operand" "r,r")
@@ -6826,10 +7060,9 @@
 	(if_then_else (ne (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -6851,8 +7084,6 @@
 				     sh_compare_op0, sh_compare_op1));
       DONE;
     }
-
-  from_compare (operands, EQ);
 }")
 
 (define_expand "bne"
@@ -6860,10 +7091,9 @@
 	(if_then_else (eq (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -6885,8 +7115,6 @@
 				     sh_compare_op0, sh_compare_op1));
       DONE;
     }
-
-  from_compare (operands, EQ);
 }")
 
 (define_expand "bgt"
@@ -6894,10 +7122,9 @@
 	(if_then_else (ne (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -6918,8 +7145,6 @@
 				     sh_compare_op0, sh_compare_op1));
       DONE;
     }
-
-  from_compare (operands, GT);
 }")
 
 (define_expand "blt"
@@ -6927,10 +7152,9 @@
 	(if_then_else (eq (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -6951,16 +7175,6 @@
 				     sh_compare_op1, sh_compare_op0));
       DONE;
     }
-
-  if (GET_MODE_CLASS (GET_MODE (sh_compare_op0)) == MODE_FLOAT)
-    {
-      rtx tmp = sh_compare_op0;
-      sh_compare_op0 = sh_compare_op1;
-      sh_compare_op1 = tmp;
-      emit_insn (gen_bgt (operands[0]));
-      DONE;
-    }
-  from_compare (operands, GE);
 }")
 
 (define_expand "ble"
@@ -6968,10 +7182,9 @@
 	(if_then_else (eq (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -6992,18 +7205,6 @@
 				     sh_compare_op1, sh_compare_op0));
       DONE;
     }
-
-  if (TARGET_SH2E
-      && TARGET_IEEE
-      && GET_MODE_CLASS (GET_MODE (sh_compare_op0)) == MODE_FLOAT)
-    {
-      rtx tmp = sh_compare_op0;
-      sh_compare_op0 = sh_compare_op1;
-      sh_compare_op1 = tmp;
-      emit_insn (gen_bge (operands[0]));
-      DONE;
-    }
-  from_compare (operands, GT);
 }")
 
 (define_expand "bge"
@@ -7011,10 +7212,9 @@
 	(if_then_else (ne (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -7035,18 +7235,6 @@
 				     sh_compare_op0, sh_compare_op1));
       DONE;
     }
-
-  if (TARGET_SH2E
-      && ! TARGET_IEEE
-      && GET_MODE_CLASS (GET_MODE (sh_compare_op0)) == MODE_FLOAT)
-    {
-      rtx tmp = sh_compare_op0;
-      sh_compare_op0 = sh_compare_op1;
-      sh_compare_op1 = tmp;
-      emit_insn (gen_ble (operands[0]));
-      DONE;
-    }
-  from_compare (operands, GE);
 }")
 
 (define_expand "bgtu"
@@ -7054,10 +7242,9 @@
 	(if_then_else (ne (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -7069,8 +7256,6 @@
 				      sh_compare_op0, sh_compare_op1));
       DONE;
     }
-
-  from_compare (operands, GTU);
 }")
 
 (define_expand "bltu"
@@ -7078,10 +7263,9 @@
 	(if_then_else (eq (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -7093,8 +7277,6 @@
 				      sh_compare_op1, sh_compare_op0));
       DONE;
     }
-
-  from_compare (operands, GEU);
 }")
 
 (define_expand "bgeu"
@@ -7102,10 +7284,9 @@
 	(if_then_else (ne (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -7117,8 +7298,6 @@
 				      sh_compare_op0, sh_compare_op1));
       DONE;
     }
-
-  from_compare (operands, GEU);
 }")
 
 (define_expand "bleu"
@@ -7126,10 +7305,9 @@
 	(if_then_else (eq (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -7141,8 +7319,50 @@
 				      sh_compare_op1, sh_compare_op0));
       DONE;
     }
+}")
+
+(define_expand "cmpun_sdf"
+  [(unordered (match_operand 0 "" "") (match_operand 1 "" ""))]
+  ""
+  "
+{
+  HOST_WIDE_INT mask;
+  switch (GET_MODE (operands[0]))
+    {
+    case SFmode:
+      mask = SF_NAN_MASK;
+      break;
+    case DFmode:
+      mask = DF_NAN_MASK;
+      break;
+    default:
+      FAIL;
+    }
+  emit_insn (gen_cmpunsf_i1 (operands[0], operands[1],
+			     force_reg (SImode, GEN_INT (mask))));
+  DONE;
+}")
 
-  from_compare (operands, GTU);
+(define_expand "cmpuneq_sdf"
+  [(uneq (match_operand 0 "" "") (match_operand 1 "" ""))]
+  ""
+  "
+{
+  HOST_WIDE_INT mask;
+  switch (GET_MODE (operands[0]))
+    {
+    case SFmode:
+      mask = SF_NAN_MASK;
+      break;
+    case DFmode:
+      mask = DF_NAN_MASK;
+      break;
+    default:
+      FAIL;
+    }
+  emit_insn (gen_cmpuneqsf_i1 (operands[0], operands[1],
+			       force_reg (SImode, GEN_INT (mask))));
+  DONE;
 }")
 
 (define_expand "bunordered"
@@ -7154,18 +7374,52 @@
   "TARGET_SHMEDIA"
   "
 {
-  operands[0] = gen_rtx_LABEL_REF (Pmode, operands[0]);
-  operands[1] = gen_reg_rtx (SImode);
   operands[2] = force_reg (GET_MODE (sh_compare_op0), sh_compare_op0);
   operands[3] = force_reg (GET_MODE (sh_compare_op1), sh_compare_op1);
+  operands[0] = gen_rtx_LABEL_REF (Pmode, operands[0]);
+  operands[1] = gen_reg_rtx (SImode);
 }")
 
-;; combiner splitter for test-and-branch on single bit in register.  This
-;; is endian dependent because the non-paradoxical subreg looks different
-;; on big endian.
-(define_split
+;; ??? The entire compare/branch scheme is antiquated, we should define
+;; cbranchsf / cbranchdf instead.
+(define_expand "bunle"
   [(set (pc)
-	(if_then_else
+	(if_then_else (ne (reg:SI T_REG) (const_int 0))
+		      (match_operand 0 "" "")
+		      (pc)))]
+  "TARGET_SHMEDIA_FPU"
+  "
+{
+  rtx tmp = gen_reg_rtx (SImode);
+
+  operands[0] = gen_rtx_LABEL_REF (Pmode, operands[0]);
+  emit_insn (gen_sgt (tmp));
+  emit_jump_insn (gen_beq_media (operands[0], tmp, const0_rtx));
+  DONE;
+}")
+
+(define_expand "bunlt"
+  [(set (pc)
+	(if_then_else (eq (reg:SI T_REG) (const_int 0))
+		      (match_operand 0 "" "")
+		      (pc)))]
+  "TARGET_SHMEDIA_FPU"
+  "
+{
+  rtx tmp = gen_reg_rtx (SImode);
+
+  operands[0] = gen_rtx_LABEL_REF (Pmode, operands[0]);
+  emit_insn (gen_sge (tmp));
+  emit_jump_insn (gen_beq_media (operands[0], tmp, const0_rtx));
+  DONE;
+}")
+
+;; combiner splitter for test-and-branch on single bit in register.  This
+;; is endian dependent because the non-paradoxical subreg looks different
+;; on big endian.
+(define_split
+  [(set (pc)
+	(if_then_else
 	  (match_operator 3 "equality_comparison_operator"
 	    [(subreg:SI (zero_extract:DI (subreg:DI (match_operand:SI 1
 						      "extend_reg_operand" "")
@@ -7189,6 +7443,59 @@
 		 : gen_rtx_GT (VOIDmode, const0_rtx, operands[4]));
 }")
 
+; operand 0 is the loop count pseudo register
+; operand 1 is the number of loop iterations or 0 if it is unknown
+; operand 2 is the maximum number of loop iterations
+; operand 3 is the number of levels of enclosed loops
+; operand 4 is the label to jump to at the top of the loop
+
+(define_expand "doloop_end"
+  [(parallel [(set (pc) (if_then_else
+			  (ne:SI (match_operand:SI 0 "" "")
+			      (const_int 1))
+			  (label_ref (match_operand 4 "" ""))
+			  (pc)))
+	      (set (match_dup 0)
+		   (plus:SI (match_dup 0) (const_int -1)))
+	      (clobber (reg:SI T_REG))])]
+  "TARGET_SH2 && !optimize_size"
+  "
+{
+  if (GET_MODE (operands[0]) != SImode)
+    FAIL;
+}
+")
+
+(define_insn_and_split "doloop_end_split"
+  [(set (pc)
+	(if_then_else (ne:SI (match_operand:SI 0 "arith_reg_dest" "+r")
+			  (const_int 1))
+		      (label_ref (match_operand 1 "" ""))
+		      (pc)))
+   (set (match_dup 0)
+	(plus (match_dup 0) (const_int -1)))
+   (clobber (reg:SI T_REG))]
+  "TARGET_SH2"
+  "#"
+  ""
+  [(parallel [(set (reg:SI T_REG)
+		   (eq:SI (match_operand:SI 0 "arith_reg_dest" "+r")
+			  (const_int 1)))
+	      (set (match_dup 0) (plus:SI (match_dup 0) (const_int -1)))])
+   (set (pc) (if_then_else (eq (reg:SI T_REG) (const_int 0))
+			   (label_ref (match_operand 1 "" ""))
+			   (pc)))]
+""
+   [(set_attr "type" "cbranch")])
+
+(define_insn "dup_db_insn"
+  [(unspec_volatile [(const_int 0)] UNSPEC_DB_INSN)]
+  "TARGET_DEAD_DELAY"
+  ""
+  [(set_attr "length" "0")
+   (set_attr "in_delay_slot" "yes")])
+
+
 ;; ------------------------------------------------------------------------
 ;; Jump and linkage insns
 ;; ------------------------------------------------------------------------
@@ -7196,7 +7503,7 @@
 (define_insn "jump_compact"
   [(set (pc)
 	(label_ref (match_operand 0 "" "")))]
-  "TARGET_SH1"
+  "TARGET_SH1 && !find_reg_note (insn, REG_CROSSING_JUMP, NULL_RTX)"
   "*
 {
   /* The length is 16 if the delay slot is unfilled.  */
@@ -8593,7 +8900,7 @@
 }")
 
 (define_insn "load_gbr"
-  [(set (match_operand:SI 0 "register_operand" "") (reg:SI GBR_REG))
+  [(set (match_operand:SI 0 "register_operand" "=r") (reg:SI GBR_REG))
    (use (reg:SI GBR_REG))]
   ""
   "stc	gbr,%0"
@@ -8690,8 +8997,8 @@
 
 (define_insn "casesi_worker_0"
   [(set (match_operand:SI 0 "register_operand" "=r,r")
-	(unspec:SI [(match_operand:SI 1 "register_operand" "0,r")
-		 (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI))
+	(mem:SI (unspec:SI [(match_operand:SI 1 "register_operand" "0,r")
+		 (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI)))
    (clobber (match_scratch:SI 3 "=X,1"))
    (clobber (match_scratch:SI 4 "=&z,z"))]
   "TARGET_SH1"
@@ -8699,38 +9006,38 @@
 
 (define_split
   [(set (match_operand:SI 0 "register_operand" "")
-	(unspec:SI [(match_operand:SI 1 "register_operand" "")
-		    (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI))
+	(mem:SI (unspec:SI [(match_operand:SI 1 "register_operand" "")
+		    (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI)))
    (clobber (match_scratch:SI 3 ""))
    (clobber (match_scratch:SI 4 ""))]
   "TARGET_SH1 && ! TARGET_SH2 && reload_completed"
   [(set (reg:SI R0_REG) (unspec:SI [(label_ref (match_dup 2))] UNSPEC_MOVA))
    (parallel [(set (match_dup 0)
-	      (unspec:SI [(reg:SI R0_REG) (match_dup 1)
-			  (label_ref (match_dup 2))] UNSPEC_CASESI))
+	      (mem:SI (unspec:SI [(reg:SI R0_REG) (match_dup 1)
+			  (label_ref (match_dup 2))] UNSPEC_CASESI)))
 	      (clobber (match_dup 3))])
    (set (match_dup 0) (plus:SI (match_dup 0) (reg:SI R0_REG)))]
   "if (GET_CODE (operands[2]) == CODE_LABEL) LABEL_NUSES (operands[2])++;")
 
 (define_split
   [(set (match_operand:SI 0 "register_operand" "")
-	(unspec:SI [(match_operand:SI 1 "register_operand" "")
-		    (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI))
+	(mem:SI (unspec:SI [(match_operand:SI 1 "register_operand" "")
+		    (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI)))
    (clobber (match_scratch:SI 3 ""))
    (clobber (match_scratch:SI 4 ""))]
   "TARGET_SH2 && reload_completed"
   [(set (reg:SI R0_REG) (unspec:SI [(label_ref (match_dup 2))] UNSPEC_MOVA))
    (parallel [(set (match_dup 0)
-	      (unspec:SI [(reg:SI R0_REG) (match_dup 1)
-			  (label_ref (match_dup 2))] UNSPEC_CASESI))
+	      (mem:SI (unspec:SI [(reg:SI R0_REG) (match_dup 1)
+			  (label_ref (match_dup 2))] UNSPEC_CASESI)))
 	      (clobber (match_dup 3))])]
   "if (GET_CODE (operands[2]) == CODE_LABEL) LABEL_NUSES (operands[2])++;")
 
 (define_insn "casesi_worker_1"
   [(set (match_operand:SI 0 "register_operand" "=r,r")
-	(unspec:SI [(reg:SI R0_REG)
+	(mem:SI (unspec:SI [(reg:SI R0_REG)
 		    (match_operand:SI 1 "register_operand" "0,r")
-		    (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI))
+		    (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI)))
    (clobber (match_scratch:SI 3 "=X,1"))]
   "TARGET_SH1"
   "*
@@ -8757,10 +9064,10 @@
 
 (define_insn "casesi_worker_2"
   [(set (match_operand:SI 0 "register_operand" "=r,r")
-	(unspec:SI [(reg:SI R0_REG)
+	(mem:SI (unspec:SI [(reg:SI R0_REG)
 		    (match_operand:SI 1 "register_operand" "0,r")
 		    (label_ref (match_operand 2 "" ""))
-		    (label_ref (match_operand 3 "" ""))] UNSPEC_CASESI))
+		    (label_ref (match_operand 3 "" ""))] UNSPEC_CASESI)))
    (clobber (match_operand:SI 4 "" "=X,1"))]
   "TARGET_SH2 && reload_completed && flag_pic"
   "*
@@ -9161,6 +9468,12 @@
     DONE;
   if (! currently_expanding_to_rtl)
     FAIL;
+  if (GET_MODE_CLASS (GET_MODE (sh_compare_op0)) == MODE_FLOAT)
+   {
+      from_compare (operands, EQ);
+      emit_insn (gen_movt (operands[0]));
+      DONE;
+    }
   operands[1] = prepare_scc_operands (EQ);
 }")
 
@@ -9433,7 +9746,12 @@
     FAIL;
   if (GET_MODE_CLASS (GET_MODE (sh_compare_op0)) == MODE_FLOAT)
     {
-      if (TARGET_IEEE)
+      if (TARGET_SH1_SOFTFP_MODE (GET_MODE (sh_compare_op0)))
+	{
+	  from_compare (operands, GE);
+	  emit_insn (gen_movt (operands[0]));
+	}
+      else if (TARGET_IEEE)
 	{
 	  rtx lab = gen_label_rtx ();
 	  prepare_scc_operands (EQ);
@@ -9443,7 +9761,10 @@
 	  emit_insn (gen_movt (operands[0]));
 	}
       else
-	emit_insn (gen_movnegt (operands[0], prepare_scc_operands (LT)));
+	{
+	  prepare_scc_operands (LT);
+	  emit_insn (gen_movnegt (operands[0]));
+	}
       DONE;
     }
   operands[1] = prepare_scc_operands (GE);
@@ -9586,6 +9907,19 @@
   operands[1] = prepare_scc_operands (GEU);
 }")
 
+(define_expand "sunle"
+  [(set (match_operand:SI 0 "arith_reg_operand" "")
+	(match_dup 1))]
+  "TARGET_SH1_SOFTFP"
+  "
+{
+  if (! currently_expanding_to_rtl)
+    FAIL;
+  from_compare (operands, UNLE);
+  emit_insn (gen_movt (operands[0]));
+  DONE;
+}")
+
 ;; sne moves the complement of the T reg to DEST like this:
 ;;      cmp/eq ...
 ;;      mov    #-1,temp
@@ -9639,6 +9973,13 @@
     DONE;
   if (! currently_expanding_to_rtl)
     FAIL;
+  if (GET_MODE_CLASS (GET_MODE (sh_compare_op0)) == MODE_FLOAT
+      && TARGET_SH1_SOFTFP_MODE (GET_MODE (sh_compare_op0) == DFmode))
+    {
+      from_compare (operands, EQ);
+      operands[1] = gen_rtx_REG (SImode, T_REG);
+    }
+  else
   operands[1] = prepare_scc_operands (EQ);
   operands[2] = gen_reg_rtx (SImode);
 }")
@@ -9664,10 +10005,14 @@
 		   (neg:SI (plus:SI (match_dup 1)
 				    (match_dup 2))))
 	      (set (reg:SI T_REG)
-		   (ne:SI (ior:SI (match_operand 1 "" "") (match_dup 2))
+		   (ne:SI (ior:SI (match_dup 1) (match_dup 2))
 			  (const_int 0)))])]
   "TARGET_SH1"
-  "operands[2] = gen_reg_rtx (SImode);")
+  "
+{
+  operands[1] = gen_rtx_REG (SImode, T_REG);
+  operands[2] = gen_reg_rtx (SImode);
+}")
 
 ;; Recognize mov #-1/negc/neg sequence, and change it to movt/add #-1.
 ;; This prevents a regression that occurred when we switched from xor to
@@ -9812,7 +10157,8 @@
   [(unspec_volatile [(const_int 0)] UNSPECV_CONST_END)]
   ""
   "* return output_jump_label_table ();"
-  [(set_attr "in_delay_slot" "no")])
+  [(set_attr "length" "0")
+   (set_attr "in_delay_slot" "no")])
 
 ; emitted at the end of the window in the literal table.
 
@@ -9943,7 +10289,7 @@
 	sts	fpscr,%0
 	sts.l	fpscr,%0"
   [(set_attr "length" "0,2,2,4,2,2,2,2,2")
-   (set_attr "type" "nil,mem_fpscr,load,mem_fpscr,gp_fpscr,move,store,mac_gp,store")])
+   (set_attr "type" "nil,mem_fpscr,load,mem_fpscr,gp_fpscr,move,store,mac_gp,fstore")])
 
 (define_peephole2
   [(set (reg:PSI FPSCR_REG)
@@ -9994,25 +10340,20 @@
 	(xor:PSI (reg:PSI FPSCR_REG) (const_int 1048576)))]
   "(TARGET_SH4 || TARGET_SH2A_DOUBLE)"
   "fschg"
-  [(set_attr "type" "fp") (set_attr "fp_set" "unknown")])
+  [(set_attr "type" "fpscr_toggle") (set_attr "fp_set" "unknown")])
 
-;; There's no way we can use it today, since optimize mode switching
-;; doesn't enable us to know from which mode we're switching to the
-;; mode it requests, to tell whether we can use a relative mode switch
-;; (like toggle_pr) or an absolute switch (like loading fpscr from
-;; memory).
 (define_insn "toggle_pr"
   [(set (reg:PSI FPSCR_REG)
 	(xor:PSI (reg:PSI FPSCR_REG) (const_int 524288)))]
-  "TARGET_SH4A_FP && ! TARGET_FPU_SINGLE"
+  "(TARGET_SH4A_FP || TARGET_SH4_300)"
   "fpchg"
-  [(set_attr "type" "fp")])
+  [(set_attr "type" "fpscr_toggle")])
 
 (define_expand "addsf3"
   [(set (match_operand:SF 0 "arith_reg_operand" "")
 	(plus:SF (match_operand:SF 1 "arith_reg_operand" "")
 		 (match_operand:SF 2 "arith_reg_operand" "")))]
-  "TARGET_SH2E || TARGET_SHMEDIA_FPU"
+  "TARGET_SH2E || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
   "
 {
   if (TARGET_SH2E)
@@ -10020,6 +10361,12 @@
       expand_sf_binop (&gen_addsf3_i, operands);
       DONE;
     }
+  else if (TARGET_OSFP)
+    {
+      expand_sfunc_binop (SFmode, &gen_addsf3_i3, \"__addsf3\", PLUS,
+			  operands);
+      DONE;
+    }
 }")
 
 (define_insn "*addsf3_media"
@@ -10096,6 +10443,22 @@
 }"
   [(set_attr "type" "fparith_media")])
 
+(define_insn "addsf3_i3"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(plus:SF (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R6_REG))
+   (clobber (reg:SI R7_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_insn "addsf3_i"
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
 	(plus:SF (match_operand:SF 1 "fp_arith_reg_operand" "%0")
@@ -10110,7 +10473,7 @@
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "")
 	(minus:SF (match_operand:SF 1 "fp_arith_reg_operand" "")
 		  (match_operand:SF 2 "fp_arith_reg_operand" "")))]
-  "TARGET_SH2E || TARGET_SHMEDIA_FPU"
+  "TARGET_SH2E || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
   "
 {
   if (TARGET_SH2E)
@@ -10118,6 +10481,12 @@
       expand_sf_binop (&gen_subsf3_i, operands);
       DONE;
     }
+  else if (TARGET_OSFP)
+    {
+      expand_sfunc_binop (SFmode, &gen_subsf3_i3, \"__subsf3\", MINUS,
+			  operands);
+      DONE;
+    }
 }")
 
 (define_insn "*subsf3_media"
@@ -10128,6 +10497,23 @@
   "fsub.s	%1, %2, %0"
   [(set_attr "type" "fparith_media")])
 
+(define_insn "subsf3_i3"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(minus:SF (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R5_REG))
+   (clobber (reg:SI R6_REG))
+   (clobber (reg:SI R7_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_insn "subsf3_i"
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
 	(minus:SF (match_operand:SF 1 "fp_arith_reg_operand" "0")
@@ -10138,24 +10524,19 @@
   [(set_attr "type" "fp")
    (set_attr "fp_mode" "single")])
 
-;; Unfortunately, the combiner is unable to cope with the USE of the FPSCR
-;; register in feeding fp instructions.  Thus, we cannot generate fmac for
-;; mixed-precision SH4 targets.  To allow it to be still generated for the
-;; SH3E, we use a separate insn for SH3E mulsf3.
-
 (define_expand "mulsf3"
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "")
 	(mult:SF (match_operand:SF 1 "fp_arith_reg_operand" "")
 		 (match_operand:SF 2 "fp_arith_reg_operand" "")))]
-  "TARGET_SH2E || TARGET_SHMEDIA_FPU"
+  "TARGET_SH2E || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
   "
 {
-  if (TARGET_SH4 || TARGET_SH2A_SINGLE)
-    expand_sf_binop (&gen_mulsf3_i4, operands);
-  else if (TARGET_SH2E)
-    emit_insn (gen_mulsf3_ie (operands[0], operands[1], operands[2]));
-  if (! TARGET_SHMEDIA)
+  if (!TARGET_SH2E && TARGET_OSFP)
+    {
+      expand_sfunc_binop (SFmode, &gen_mulsf3_i3, \"__mulsf3\", MULT,
+                         operands);
     DONE;
+    }
 }")
 
 (define_insn "*mulsf3_media"
@@ -10166,6 +10547,27 @@
   "fmul.s	%1, %2, %0"
   [(set_attr "type" "fparith_media")])
 
+;; Unfortunately, the combiner is unable to cope with the USE of the FPSCR
+;; register in feeding fp instructions.  Thus, in order to generate fmac,
+;; we start out with a mulsf pattern that does not depend on fpscr.
+;; This is split after combine to introduce the dependency, in order to
+;; get mode switching and scheduling right.
+(define_insn_and_split "mulsf3_ie"
+  [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
+	(mult:SF (match_operand:SF 1 "fp_arith_reg_operand" "%0")
+		 (match_operand:SF 2 "fp_arith_reg_operand" "f")))]
+  "TARGET_SH2E"
+  "fmul	%2,%0"
+  "TARGET_SH4 || TARGET_SH2A_SINGLE"
+  [(const_int 0)]
+  "
+{
+  emit_insn (gen_mulsf3_i4 (operands[0], operands[1], operands[2],
+	     get_fpscr_rtx ()));
+  DONE;
+}"
+  [(set_attr "type" "fp")])
+
 (define_insn "mulsf3_i4"
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
 	(mult:SF (match_operand:SF 1 "fp_arith_reg_operand" "%0")
@@ -10176,20 +10578,28 @@
   [(set_attr "type" "fp")
    (set_attr "fp_mode" "single")])
 
-(define_insn "mulsf3_ie"
-  [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
-	(mult:SF (match_operand:SF 1 "fp_arith_reg_operand" "%0")
-		 (match_operand:SF 2 "fp_arith_reg_operand" "f")))]
-  "TARGET_SH2E && ! (TARGET_SH4 || TARGET_SH2A_SINGLE)"
-  "fmul	%2,%0"
-  [(set_attr "type" "fp")])
+(define_insn "mulsf3_i3"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(mult:SF (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI MACH_REG))
+   (clobber (reg:SI MACL_REG))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
 
 (define_insn "mac_media"
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
 	(plus:SF (mult:SF (match_operand:SF 1 "fp_arith_reg_operand" "%f")
 			  (match_operand:SF 2 "fp_arith_reg_operand" "f"))
 		 (match_operand:SF 3 "fp_arith_reg_operand" "0")))]
-  "TARGET_SHMEDIA_FPU"
+  "TARGET_SHMEDIA_FPU && TARGET_FMAC"
   "fmac.s %1, %2, %0"
   [(set_attr "type" "fparith_media")])
 
@@ -10199,7 +10609,7 @@
 			  (match_operand:SF 2 "fp_arith_reg_operand" "f"))
 		 (match_operand:SF 3 "arith_reg_operand" "0")))
    (use (match_operand:PSI 4 "fpscr_operand" "c"))]
-  "TARGET_SH2E && ! TARGET_SH4"
+  "TARGET_SH2E && TARGET_FMAC"
   "fmac	fr0,%2,%0"
   [(set_attr "type" "fp")
    (set_attr "fp_mode" "single")])
@@ -10344,13 +10754,156 @@
   "ftrc	%1,%0"
   [(set_attr "type" "fp")])
 
+(define_insn "cmpnesf_i1"
+  [(set (match_operand:CC_FP_NE 0 "register_operand" "=z")
+	(compare:CC_FP_NE (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1 && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpgtsf_i1"
+  [(set (match_operand:CC_FP_GT 0 "register_operand" "=z")
+	(compare:CC_FP_GT (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1 && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpunltsf_i1"
+  [(set (match_operand:CC_FP_UNLT 0 "register_operand" "=z")
+	(compare:CC_FP_UNLT (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1 && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpeqsf_i1_finite"
+  [(set (reg:SI T_REG)
+	(eq:SI (match_operand:SF 0 "arith_reg_operand" "r,r,r")
+	       (match_operand:SF 1 "arith_reg_operand" "r,r,r")))
+   (clobber (match_scratch:SI 2 "=0,1,?r"))]
+  "TARGET_SH1 && ! TARGET_SH2E && flag_finite_math_only"
+  "*
+{
+  if (which_alternative == 0)
+     output_asm_insn (\"cmp/eq\t%0,%1\;or\t%1,%2\;bt\t0f\", operands);
+  else if (which_alternative == 1)
+     output_asm_insn (\"cmp/eq\t%0,%1\;or\t%0,%2\;bt\t0f\", operands);
+  else
+    output_asm_insn (\"cmp/eq\t%0,%1\;mov\t%0,%2\;bt\t0f\;or\t%1,%2\",
+		     operands);
+  return \"add\t%2,%2\;tst\t%2,%2\\n0:\";
+}"
+  [(set_attr "length" "10,10,12")])
+
+(define_insn "cmplesf_i1_finite"
+  [(set (reg:SI T_REG)
+	(le:SI (match_operand:SF 0 "arith_reg_operand" "r,r,r")
+	       (match_operand:SF 1 "arith_reg_operand" "r,r,r")))
+   (clobber (match_scratch:SI 2 "=0,1,r"))]
+  "TARGET_SH1 && ! TARGET_SH2E && flag_finite_math_only"
+  "*
+{
+  output_asm_insn (\"cmp/pz\t%0\", operands);
+  if (which_alternative == 2)
+    output_asm_insn (\"mov\t%0,%2\", operands);
+  if (TARGET_SH2)
+    output_asm_insn (\"bf/s\t0f\;cmp/hs\t%1,%0\;cmp/ge\t%0,%1\", operands);
+  else
+    output_asm_insn (\"bt\t1f\;bra\t0f\;cmp/hs\t%1,%0\\n1:\tcmp/ge\t%0,%1\",
+		     operands);
+  if (which_alternative == 1)
+    output_asm_insn (\"or\t%0,%2\", operands);
+  else
+    output_asm_insn (\"or\t%1,%2\", operands);
+  return \"bt\t0f\;add\t%2,%2\;tst\t%2,%2\\n0:\";
+}"
+  [(set_attr "length" "18,18,20")])
+
+(define_insn "cmpunsf_i1"
+  [(set (reg:SI T_REG)
+	(unordered:SI (match_operand:SF 0 "arith_reg_operand" "r,r")
+		      (match_operand:SF 1 "arith_reg_operand" "r,r")))
+   (use (match_operand:SI 2 "arith_reg_operand" "r,r"))
+   (clobber (match_scratch:SI 3 "=0,&r"))]
+  "TARGET_SH1 && ! TARGET_SH2E"
+  "not\t%0,%3\;tst\t%2,%3\;not\t%1,%3\;bt\t0f\;tst\t%2,%3\;0:"
+  [(set_attr "length" "10")])
+
+;; ??? This is a lot of code with a lot of branches; a library function
+;; might be better.
+(define_insn "cmpuneqsf_i1"
+  [(set (reg:SI T_REG)
+	(uneq:SI (match_operand:SF 0 "arith_reg_operand" "r")
+		 (match_operand:SF 1 "arith_reg_operand" "r")))
+   (use (match_operand:SI 2 "arith_reg_operand" "r"))
+   (clobber (match_scratch:SI 3 "=&r"))]
+  "TARGET_SH1 && ! TARGET_SH2E"
+  "*
+{
+  output_asm_insn (\"not\t%0,%3\;tst\t%2,%3\;not\t%1,%3\", operands);
+  output_asm_insn (\"bt\t0f\;tst\t%2,%3\;bt\t0f\;cmp/eq\t%0,%1\", operands);
+  output_asm_insn (\"mov\t%0,%3\;bt\t0f\;or\t%1,%3\", operands);
+  return \"add\t%3,%3\;tst\t%3,%3\\n0:\";
+}"
+  [(set_attr "length" "24")])
+
+(define_insn "movcc_fp_ne"
+  [(set (match_operand:CC_FP_NE 0 "general_movdst_operand"
+	    "=r,r,m")
+	(match_operand:CC_FP_NE 1 "general_movsrc_operand"
+	 "rI08,mr,r"))]
+  "TARGET_SH1"
+  "@
+	mov	%1,%0
+	mov.l	%1,%0
+	mov.l	%1,%0"
+  [(set_attr "type" "move,load,store")])
+
+(define_insn "movcc_fp_gt"
+  [(set (match_operand:CC_FP_GT 0 "general_movdst_operand"
+	    "=r,r,m")
+	(match_operand:CC_FP_GT 1 "general_movsrc_operand"
+	 "rI08,mr,r"))]
+  "TARGET_SH1"
+  "@
+	mov	%1,%0
+	mov.l	%1,%0
+	mov.l	%1,%0"
+  [(set_attr "type" "move,load,store")])
+
+(define_insn "movcc_fp_unlt"
+  [(set (match_operand:CC_FP_UNLT 0 "general_movdst_operand"
+	    "=r,r,m")
+	(match_operand:CC_FP_UNLT 1 "general_movsrc_operand"
+	 "rI08,mr,r"))]
+  "TARGET_SH1"
+  "@
+	mov	%1,%0
+	mov.l	%1,%0
+	mov.l	%1,%0"
+  [(set_attr "type" "move,load,store")])
+
 (define_insn "cmpgtsf_t"
   [(set (reg:SI T_REG)
 	(gt:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
 	       (match_operand:SF 1 "fp_arith_reg_operand" "f")))]
   "TARGET_SH2E && ! (TARGET_SH4 || TARGET_SH2A_SINGLE)"
   "fcmp/gt	%1,%0"
-  [(set_attr "type" "fp")
+  [(set_attr "type" "fp_cmp")
    (set_attr "fp_mode" "single")])
 
 (define_insn "cmpeqsf_t"
@@ -10359,7 +10912,7 @@
 	       (match_operand:SF 1 "fp_arith_reg_operand" "f")))]
   "TARGET_SH2E && ! (TARGET_SH4 || TARGET_SH2A_SINGLE)"
   "fcmp/eq	%1,%0"
-  [(set_attr "type" "fp")
+  [(set_attr "type" "fp_cmp")
    (set_attr "fp_mode" "single")])
 
 (define_insn "ieee_ccmpeqsf_t"
@@ -10371,6 +10924,22 @@
   "* return output_ieee_ccmpeq (insn, operands);"
   [(set_attr "length" "4")])
 
+(define_insn "*cmpltgtsf_t"
+  [(set (reg:SI T_REG)
+	(ltgt:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
+		 (match_operand:SF 1 "fp_arith_reg_operand" "f")))]
+  "TARGET_SH2E && ! (TARGET_SH4 || TARGET_SH2A_SINGLE)"
+  "fcmp/gt\t%1,%0\;bt\t0f\;fcmp/gt\t%0,%1\\n0:"
+  [(set_attr "length" "6")])
+
+(define_insn "*cmporderedsf_t"
+  [(set (reg:SI T_REG)
+	(ordered:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
+		    (match_operand:SF 1 "fp_arith_reg_operand" "f")))]
+  "TARGET_SH2E && ! (TARGET_SH4 || TARGET_SH2A_SINGLE)"
+  "fcmp/eq\t%0,%0\;bf\t0f\;fcmp/eq\t%1,%1\\n0:"
+  [(set_attr "length" "6")])
+
 
 (define_insn "cmpgtsf_t_i4"
   [(set (reg:SI T_REG)
@@ -10379,7 +10948,7 @@
    (use (match_operand:PSI 2 "fpscr_operand" "c"))]
   "(TARGET_SH4 || TARGET_SH2A_SINGLE)"
   "fcmp/gt	%1,%0"
-  [(set_attr "type" "fp")
+  [(set_attr "type" "fp_cmp")
    (set_attr "fp_mode" "single")])
 
 (define_insn "cmpeqsf_t_i4"
@@ -10389,7 +10958,7 @@
    (use (match_operand:PSI 2 "fpscr_operand" "c"))]
   "(TARGET_SH4 || TARGET_SH2A_SINGLE)"
   "fcmp/eq	%1,%0"
-  [(set_attr "type" "fp")
+  [(set_attr "type" "fp_cmp")
    (set_attr "fp_mode" "single")])
 
 (define_insn "*ieee_ccmpeqsf_t_4"
@@ -10403,6 +10972,26 @@
   [(set_attr "length" "4")
    (set_attr "fp_mode" "single")])
 
+(define_insn "*cmpltgtsf_t_4"
+  [(set (reg:SI T_REG)
+	(ltgt:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
+		 (match_operand:SF 1 "fp_arith_reg_operand" "f")))
+   (use (match_operand:PSI 2 "fpscr_operand" "c"))]
+  "TARGET_SH4 || TARGET_SH2A_SINGLE"
+  "fcmp/gt\t%1,%0\;bt\t0f\;fcmp/gt\t%0,%1\\n0:"
+  [(set_attr "length" "6")
+   (set_attr "fp_mode" "single")])
+
+(define_insn "*cmporderedsf_t_4"
+  [(set (reg:SI T_REG)
+	(ordered:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
+		    (match_operand:SF 1 "fp_arith_reg_operand" "f")))
+   (use (match_operand:PSI 2 "fpscr_operand" "c"))]
+  "TARGET_SH4 || TARGET_SH2A_SINGLE"
+  "fcmp/eq\t%0,%0\;bf\t0f\;fcmp/eq\t%1,%1\\n0:"
+  [(set_attr "length" "6")
+   (set_attr "fp_mode" "single")])
+
 (define_insn "cmpeqsf_media"
   [(set (match_operand:SI 0 "register_operand" "=r")
 	(eq:SI (match_operand:SF 1 "fp_arith_reg_operand" "f")
@@ -10645,11 +11234,39 @@
   [(set_attr "type" "fmove")
    (set_attr "fp_mode" "single")])
 
+(define_expand "abssc2"
+  [(set (match_operand:SF 0 "fp_arith_reg_operand" "")
+	(abs:SF (match_operand:SC 1 "fp_arith_reg_operand" "")))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+  "
+{
+  expand_sfunc_unop (SCmode, &gen_abssc2_i3, \"__hypotf\", ABS, operands);
+  DONE;
+}")
+
+(define_insn "abssc2_i3"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(abs:SF (reg:SC R4_REG)))
+   (clobber (reg:SI MACH_REG))
+   (clobber (reg:SI MACL_REG))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R4_REG))
+   (clobber (reg:SI R5_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_expand "adddf3"
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(plus:DF (match_operand:DF 1 "fp_arith_reg_operand" "")
 		 (match_operand:DF 2 "fp_arith_reg_operand" "")))]
-  "(TARGET_SH4 || TARGET_SH2A_DOUBLE) || TARGET_SHMEDIA_FPU"
+  "TARGET_FPU_DOUBLE || (TARGET_SH3 && TARGET_OSFP)"
   "
 {
   if (TARGET_SH4 || TARGET_SH2A_DOUBLE)
@@ -10657,6 +11274,12 @@
       expand_df_binop (&gen_adddf3_i, operands);
       DONE;
     }
+  else if (TARGET_SH3 && TARGET_OSFP)
+    {
+      expand_sfunc_binop (DFmode, &gen_adddf3_i3_wrap, \"__adddf3\", PLUS,
+			  operands);
+      DONE;
+    }
 }")
 
 (define_insn "*adddf3_media"
@@ -10677,6 +11300,30 @@
   [(set_attr "type" "dfp_arith")
    (set_attr "fp_mode" "double")])
 
+(define_expand "adddf3_i3_wrap"
+  [(match_operand:DF 0 "" "") (match_operand:SI 1 "" "")]
+  "TARGET_SH3"
+  "
+{
+  emit_insn (gen_adddf3_i3 (operands[1]));
+  emit_move_insn (operands[0], gen_rtx_REG (DFmode, R0_REG));
+  DONE;
+}")
+
+(define_insn "adddf3_i3"
+  [(set (reg:DF R0_REG)
+	(plus:DF (reg:DF R4_REG) (reg:DF R6_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:DI R2_REG))
+   (clobber (reg:DF R4_REG))
+   (clobber (reg:DF R6_REG))
+   (use (match_operand:SI 0 "arith_reg_operand" "r"))]
+  "TARGET_SH3"
+  "jsr	@%0%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_expand "subdf3"
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(minus:DF (match_operand:DF 1 "fp_arith_reg_operand" "")
@@ -10713,7 +11360,7 @@
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(mult:DF (match_operand:DF 1 "fp_arith_reg_operand" "")
 		 (match_operand:DF 2 "fp_arith_reg_operand" "")))]
-  "(TARGET_SH4 || TARGET_SH2A_DOUBLE) || TARGET_SHMEDIA_FPU"
+  "TARGET_FPU_DOUBLE || (TARGET_SH3 && TARGET_OSFP)"
   "
 {
   if (TARGET_SH4 || TARGET_SH2A_DOUBLE)
@@ -10721,6 +11368,12 @@
       expand_df_binop (&gen_muldf3_i, operands);
       DONE;
     }
+  else if (TARGET_SH3 && TARGET_OSFP)
+    {
+      expand_sfunc_binop (DFmode, &gen_muldf3_i3_wrap, \"__muldf3\", MULT,
+			  operands);
+      DONE;
+    }
 }")
 
 (define_insn "*muldf3_media"
@@ -10738,9 +11391,35 @@
    (use (match_operand:PSI 3 "fpscr_operand" "c"))]
   "(TARGET_SH4 || TARGET_SH2A_DOUBLE)"
   "fmul	%2,%0"
-  [(set_attr "type" "dfp_arith")
+  [(set_attr "type" "dfp_mul")
    (set_attr "fp_mode" "double")])
 
+(define_expand "muldf3_i3_wrap"
+  [(match_operand:DF 0 "" "") (match_operand:SI 1 "" "")]
+  "TARGET_SH3"
+  "
+{
+  emit_insn (gen_muldf3_i3 (operands[1]));
+  emit_move_insn (operands[0], gen_rtx_REG (DFmode, R0_REG));
+  DONE;
+}")
+
+(define_insn "muldf3_i3"
+  [(set (reg:DF R0_REG)
+	(mult:DF (reg:DF R4_REG) (reg:DF R6_REG)))
+   (clobber (reg:SI MACH_REG))
+   (clobber (reg:SI MACL_REG))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:DI R2_REG))
+   (clobber (reg:DF R4_REG))
+   (clobber (reg:DF R6_REG))
+   (use (match_operand:SI 0 "arith_reg_operand" "r"))]
+  "TARGET_SH3"
+  "jsr	@%0%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_expand "divdf3"
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(div:DF (match_operand:DF 1 "fp_arith_reg_operand" "")
@@ -10870,6 +11549,73 @@
 ;; 	      (use (match_dup 2))])
 ;;    (set (match_dup 0) (reg:SI FPUL_REG))])
 
+(define_insn "cmpnedf_i1"
+  [(set (match_operand:CC_FP_NE 0 "register_operand" "=z")
+	(compare:CC_FP_NE (reg:DF R4_REG) (reg:DF R6_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpgtdf_i1"
+  [(set (match_operand:CC_FP_GT 0 "register_operand" "=z")
+	(compare:CC_FP_GT (reg:DF R4_REG) (reg:DF R6_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpunltdf_i1"
+  [(set (match_operand:CC_FP_UNLT 0 "register_operand" "=z")
+	(compare:CC_FP_UNLT (reg:DF R4_REG) (reg:DF R6_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpeqdf_i1_finite"
+  [(set (reg:SI T_REG)
+	(eq:SI (match_operand:DF 0 "arith_reg_operand" "r")
+	       (match_operand:DF 1 "arith_reg_operand" "r")))
+   (clobber (match_scratch:SI 2 "=&r"))]
+  "TARGET_SH1_SOFTFP && flag_finite_math_only"
+  "cmp/eq\t%R0,%R1\;mov\t%S0,%2\;bf\t0f\;cmp/eq\t%S0,%S1\;bt\t0f\;or\t%S1,%2\;add\t%2,%2\;or\t%R0,%2\;tst\t%2,%2\\n0:"
+  [(set_attr "length" "18")])
+
+(define_insn "cmpundf_i1"
+  [(set (reg:SI T_REG)
+	(unordered:SI (match_operand:DF 0 "arith_reg_operand" "r,r")
+		      (match_operand:DF 1 "arith_reg_operand" "r,r")))
+   (use (match_operand:SI 2 "arith_reg_operand" "r,r"))
+   (clobber (match_scratch:SI 3 "=0,&r"))]
+  "TARGET_SH1 && ! TARGET_SH2E"
+  "not\t%S0,%3\;tst\t%2,%3\;not\t%S1,%3\;bt\t0f\;tst\t%2,%3\;0:"
+  [(set_attr "length" "10")])
+
+;; ??? This is a lot of code with a lot of branches; a library function
+;; might be better.
+(define_insn "cmpuneqdf_i1"
+  [(set (reg:SI T_REG)
+	(uneq:SI (match_operand:DF 0 "arith_reg_operand" "r")
+		 (match_operand:DF 1 "arith_reg_operand" "r")))
+   (use (match_operand:SI 2 "arith_reg_operand" "r"))
+   (clobber (match_scratch:SI 3 "=&r"))]
+  "TARGET_SH1_SOFTFP"
+  "not\t%S0,%3\;tst\t%2,%3\;not\t%S1,%3\;bt\t0f\;tst\t%2,%3\;bt\t0f\;cmp/eq\t%R0,%R1\; bf\t0f\;cmp/eq\t%S0,%S1\;bt\t0f\;mov\t%S0,%3\;or\t%S1,%3\;add\t%3,%3\;or\t%R0,%3\;tst\t%3,%3\\n0:"
+  [(set_attr "length" "30")])
+
 (define_insn "cmpgtdf_t"
   [(set (reg:SI T_REG)
 	(gt:SI (match_operand:DF 0 "arith_reg_operand" "f")
@@ -10901,6 +11647,26 @@
   [(set_attr "length" "4")
    (set_attr "fp_mode" "double")])
 
+(define_insn "*cmpltgtdf_t"
+  [(set (reg:SI T_REG)
+	(ltgt:SI (match_operand:DF 0 "fp_arith_reg_operand" "f")
+		 (match_operand:DF 1 "fp_arith_reg_operand" "f")))
+   (use (match_operand:PSI 2 "fpscr_operand" "c"))]
+  "TARGET_SH4 || TARGET_SH2A_DOUBLE"
+  "fcmp/gt\t%1,%0\;bt\t0f\;fcmp/gt\t%0,%1\\n0:"
+  [(set_attr "length" "6")
+   (set_attr "fp_mode" "double")])
+
+(define_insn "*cmpordereddf_t_4"
+  [(set (reg:SI T_REG)
+	(ordered:SI (match_operand:DF 0 "fp_arith_reg_operand" "f")
+		    (match_operand:DF 1 "fp_arith_reg_operand" "f")))
+   (use (match_operand:PSI 2 "fpscr_operand" "c"))]
+  "TARGET_SH4 || TARGET_SH2A_SINGLE"
+  "fcmp/eq\t%0,%0\;bf\t0f\;fcmp/eq\t%1,%1\\n0:"
+  [(set_attr "length" "6")
+   (set_attr "fp_mode" "double")])
+
 (define_insn "cmpeqdf_media"
   [(set (match_operand:SI 0 "register_operand" "=r")
 	(eq:SI (match_operand:DF 1 "fp_arith_reg_operand" "f")
@@ -11035,7 +11801,7 @@
 (define_expand "extendsfdf2"
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(float_extend:DF (match_operand:SF 1 "fpul_operand" "")))]
-  "(TARGET_SH4 || TARGET_SH2A_DOUBLE) || TARGET_SHMEDIA_FPU"
+  "TARGET_FPU_DOUBLE || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
   "
 {
   if (TARGET_SH4 || TARGET_SH2A_DOUBLE)
@@ -11044,6 +11810,18 @@
 					get_fpscr_rtx ()));
       DONE;
     }
+  if (TARGET_SH2E && TARGET_OSFP)
+    {
+      expand_sfunc_unop (SFmode, &gen_extendsfdf2_i2e, \"__extendsfdf2\",
+ 		 FLOAT_EXTEND, operands);
+      DONE;
+    }
+  else if (TARGET_SH1 && TARGET_OSFP)
+    {
+      expand_sfunc_unop (SFmode, &gen_extendsfdf2_i1, \"__extendsfdf2\",
+			 FLOAT_EXTEND, operands);
+      DONE;
+    }
 }")
 
 (define_insn "*extendsfdf2_media"
@@ -11062,10 +11840,76 @@
   [(set_attr "type" "fp")
    (set_attr "fp_mode" "double")])
 
+;; ??? In order to use this efficiently, we'd have to have an extra
+;; register class for r0 and r1 - and that would cause repercussions in
+;; register allocation elsewhere.  So just say we clobber r0 / r1, and
+;; that we can use an arbitrary target.  */
+(define_insn_and_split "extendsfdf2_i1"
+  [(set (match_operand:DF 0 "arith_reg_dest" "=r")
+	(float_extend:DF (reg:SF R4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R0_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && !TARGET_SH2E"
+  "#"
+  "&& reload_completed"
+  [(set (match_dup 0) (reg:DF R0_REG))]
+  "emit_insn (gen_extendsfdf2_i1_r0 (operands[1]));"
+  [(set_attr "type" "sfunc")])
+
+(define_insn "extendsfdf2_i1_r0"
+  [(set (reg:DF R0_REG) (float_extend:DF (reg:SF R4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 0 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && !TARGET_SH2E"
+  "jsr	@%0%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn_and_split "extendsfdf2_i2e"
+  [(set (match_operand:DF 0 "arith_reg_dest" "=r")
+	(float_extend:DF (reg:SF FR4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R0_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R4_REG))
+   (clobber (reg:SI FPUL_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && TARGET_SH2E"
+  "#"
+  "&& reload_completed"
+  [(set (match_dup 0) (reg:DF R0_REG))]
+  "emit_insn (gen_extendsfdf2_i2e_r0 (operands[1]));"
+  [(set_attr "type" "sfunc")])
+
+(define_insn "extendsfdf2_i2e_r0"
+  [(set (reg:DF R0_REG) (float_extend:DF (reg:SF FR4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R4_REG))
+   (clobber (reg:SI FPUL_REG))
+   (use (match_operand:SI 0 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && TARGET_SH2E"
+  "jsr	@%0%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_expand "truncdfsf2"
   [(set (match_operand:SF 0 "fpul_operand" "")
 	(float_truncate:SF (match_operand:DF 1 "fp_arith_reg_operand" "")))]
-  "(TARGET_SH4 || TARGET_SH2A_DOUBLE) || TARGET_SHMEDIA_FPU"
+  "TARGET_FPU_DOUBLE || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
   "
 {
   if (TARGET_SH4 || TARGET_SH2A_DOUBLE)
@@ -11074,6 +11918,18 @@
 				       get_fpscr_rtx ()));
       DONE;
     }
+  else if (TARGET_SH2E && TARGET_OSFP)
+    {
+      expand_sfunc_unop (DFmode, &gen_truncdfsf2_i2e, \"__truncdfsf2\",
+			 FLOAT_TRUNCATE, operands);
+      DONE;
+    }
+  else if (TARGET_SH1 && TARGET_OSFP)
+    {
+      expand_sfunc_unop (DFmode, &gen_truncdfsf2_i1, \"__truncdfsf2\",
+			 FLOAT_TRUNCATE, operands);
+      DONE;
+    }
 }")
 
 (define_insn "*truncdfsf2_media"
@@ -11092,6 +11948,37 @@
   [(set_attr "type" "fp")
    (set_attr "fp_mode" "double")])
 
+(define_insn "truncdfsf2_i1"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(float_truncate:SF (reg:DF R4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && !TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "truncdfsf2_i2e"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=w")
+	(float_truncate:SF (reg:DF R4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI FPUL_REG))
+   (clobber (reg:SI R0_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+
 ;; Bit field extract patterns.  These give better code for packed bitfields,
 ;; because they allow auto-increment addresses to be generated.
 
Index: gcc/config/sh/netbsd-elf.h
===================================================================
Index: gcc/config/sh/vxworks.h
===================================================================
Index: gcc/cfgrtl.c
===================================================================
--- gcc/cfgrtl.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/cfgrtl.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,6 +1,7 @@
 /* Control flow graph manipulation code for GNU compiler.
    Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005, 2007 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -802,9 +803,10 @@
 	fprintf (dump_file, "Replacing insn %i by jump %i\n",
 		 INSN_UID (insn), INSN_UID (BB_END (src)));
 
-
       delete_insn_chain (kill_from, insn);
 
+      src->flags |= BB_DIRTY;
+
       /* Recognize a tablejump that we are converting to a
 	 simple jump and remove its associated CODE_LABEL
 	 and ADDR_VEC or ADDR_DIFF_VEC.  */
Index: gcc/stmt.c
===================================================================
--- gcc/stmt.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/stmt.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1117,7 +1117,7 @@
     {
       if (o[i] != TREE_VALUE (tail))
 	{
-	  expand_assignment (o[i], TREE_VALUE (tail));
+	  expand_assignment (o[i], TREE_VALUE (tail), false);
 	  free_temp_slots ();
 
 	  /* Restore the original value so that it's correct the next
Index: gcc/prefix.c
===================================================================
--- gcc/prefix.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/prefix.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1,6 +1,7 @@
 /* Utility to update paths from internal to external forms.
    Copyright (C) 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005,
    2007  Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -251,6 +252,8 @@
   char *result, *p;
   const int len = strlen (std_prefix);
 
+  CYGPATH (path);
+  
   if (! strncmp (path, std_prefix, len)
       && (IS_DIR_SEPARATOR(path[len])
           || path[len] == '\0')
@@ -345,6 +348,8 @@
     tr (result, '/', DIR_SEPARATOR);
 #endif
 
+  CYGPATH_FREE (path);
+  
   return result;
 }
 
Index: gcc/collect2.c
===================================================================
--- gcc/collect2.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ gcc/collect2.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -5,6 +5,7 @@
    Contributed by Chris Smith (csmith@convex.com).
    Heavily modified by Michael Meissner (meissner@cygnus.com),
    Per Bothner (bothner@cygnus.com), and John Gilmore (gnu@cygnus.com).
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -573,7 +574,10 @@
 {
   char *temp;
   struct prefix_list *pl;
-  int len = pprefix->max_len + strlen (name) + 1;
+  int len;
+
+  CYGPATH (name);
+  len = pprefix->max_len + strlen (name) + 1;
 
   if (debug)
     fprintf (stderr, "Looking for '%s'\n", name);
@@ -599,6 +603,7 @@
 	  if (debug)
 	    fprintf (stderr, "  - found: absolute path\n");
 
+	  CYGPATH_FREE (name);
 	  return temp;
 	}
 
@@ -609,7 +614,10 @@
 	strcat (temp, HOST_EXECUTABLE_SUFFIX);
 
 	if (access (temp, X_OK) == 0)
+	  {
+	    CYGPATH_FREE (name);
 	  return temp;
+	  }
 #endif
 
       if (debug)
@@ -626,7 +634,10 @@
 	if (stat (temp, &st) >= 0
 	    && ! S_ISDIR (st.st_mode)
 	    && access (temp, X_OK) == 0)
+	  {
+	    CYGPATH_FREE (name);
 	  return temp;
+	  }
 
 #ifdef HOST_EXECUTABLE_SUFFIX
 	/* Some systems have a suffix for executable files.
@@ -636,7 +647,10 @@
 	if (stat (temp, &st) >= 0
 	    && ! S_ISDIR (st.st_mode)
 	    && access (temp, X_OK) == 0)
+	  {
+	    CYGPATH_FREE (name);
 	  return temp;
+	  }
 #endif
       }
 
@@ -644,6 +658,7 @@
     fprintf (stderr, "  - failed: no entries in prefix list\n");
 
   free (temp);
+  CYGPATH (name);
   return 0;
 }
 
@@ -666,6 +681,7 @@
 
   /* Keep track of the longest prefix.  */
 
+  CYGPATH (prefix);
   len = strlen (prefix);
   if (len > pprefix->max_len)
     pprefix->max_len = len;
Index: Makefile.tpl
===================================================================
--- Makefile.tpl	(.../vendor/tags/4.2.4)	(revision 920)
+++ Makefile.tpl	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -362,8 +362,9 @@
 
 #### host and target specific makefile fragments come in here.
 @target_makefile_frag@
-@alphaieee_frag@
 @ospace_frag@
+@ieee_frag@
+@relax_frag@
 @host_makefile_frag@
 ###
 
Index: libcpp/files.c
===================================================================
--- libcpp/files.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ libcpp/files.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -6,6 +6,7 @@
    Adapted to ANSI C, Richard Stallman, Jan 1987
    Split out of cpplib.c, Zack Weinberg, Oct 1998
    Reimplemented, Neil Booth, Jul 2003
+   Copyright (c) 2006  STMicroelectronics.
 
 This program is free software; you can redistribute it and/or modify it
 under the terms of the GNU General Public License as published by the
@@ -206,7 +207,12 @@
       set_stdin_to_binary_mode ();
     }
   else
-    file->fd = open (file->path, O_RDONLY | O_NOCTTY | O_BINARY, 0666);
+    {
+      char *filename = file->path;
+      CYGPATH (filename);
+      file->fd = open (filename, O_RDONLY | O_NOCTTY | O_BINARY, 0666);
+      CYGPATH_FREE (filename);
+    }
 
   if (file->fd != -1)
     {
@@ -259,6 +265,16 @@
   memcpy (pchname, path, flen);
   memcpy (pchname + flen, extension, sizeof (extension));
 
+#ifdef __MINGW32__
+    {
+      char *temp = cygpath (pchname);
+      len = strlen (temp) + 1;
+      pchname = XRESIZEVEC (char, pchname, len);
+      memcpy (pchname, temp, len);
+      free (temp);
+    }
+#endif
+  
   if (stat (pchname, &st) == 0)
     {
       DIR *pchdir;
Index: libcpp/ChangeLog.STM
===================================================================
--- libcpp/ChangeLog.STM	(.../vendor/tags/4.2.4)	(revision 0)
+++ libcpp/ChangeLog.STM	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,26 @@
+2008-04-08  Antony King  <antony.king@st.com>
+
+	Fix INSbl28582:
+	* files.c (pch_open_file): Fix incorrect use of XRESIZEVEC.
+
+2007-08-14  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* mkdeps.c (deps_write): Convert paths to Cygwin format on MinGW,
+	if GCC_CYGWIN_DEPS environment variable is set.
+	(deps_phony_targets): Likewise.
+
+2006-12-28  Tom Tromey  <tromey@redhat.com>
+
+	PR preprocessor/30001:
+	* charset.c (_cpp_convert_input): Check that to.len is greater
+	than zero.
+
+2006-03-27  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* files.c (open_file, pch_open_file): Remove CYGDRIVE markers.
+	Add Cygwin pathname support for MinGW.
+
+2006-03-10  J"orn Rennecke <joern.rennecke@st.com>
+
+	* files.c (open_file, pch_open_file): Add CYGDRIVE markers.
+
Index: libcpp/mkdeps.c
===================================================================
--- libcpp/mkdeps.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ libcpp/mkdeps.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -308,6 +308,17 @@
 	  putc (' ', fp);
 	  column++;
 	}
+#ifdef __MINGW32__
+      if (getenv ("GCC_CYGWIN_DEPS") != NULL
+	  && isalpha (d->targetv[i][0])
+	  && d->targetv[i][1] == ':')
+	{
+	  fputs ("/cygdrive/", fp);
+	  fputc (d->targetv[i][0], fp);
+	  fputs (d->targetv[i]+2, fp);
+	}
+      else
+#endif
       fputs (d->targetv[i], fp);
     }
 
@@ -329,6 +340,17 @@
 	  putc (' ', fp);
 	  column++;
 	}
+#ifdef __MINGW32__
+      if (getenv ("GCC_CYGWIN_DEPS") != NULL
+	  && isalpha (d->depv[i][0])
+	  && d->depv[i][1] == ':')
+	{
+	  fputs ("/cygdrive/", fp);
+	  fputc (d->depv[i][0], fp);
+	  fputs (d->depv[i]+2, fp);
+	}
+      else
+#endif
       fputs (d->depv[i], fp);
     }
   putc ('\n', fp);
@@ -342,6 +364,17 @@
   for (i = 1; i < d->ndeps; i++)
     {
       putc ('\n', fp);
+#ifdef __MINGW32__
+      if (getenv ("GCC_CYGWIN_DEPS") != NULL
+	  && isalpha (d->depv[i][0])
+	  && d->depv[i][1] == ':')
+	{
+	  fputs ("/cygdrive/", fp);
+	  fputc (d->depv[i][0], fp);
+	  fputs (d->depv[i]+2, fp);
+	}
+      else
+#endif
       fputs (d->depv[i], fp);
       putc (':', fp);
       putc ('\n', fp);
Index: libcpp/charset.c
===================================================================
--- libcpp/charset.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ libcpp/charset.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -1628,7 +1628,7 @@
      terminate with another \r, not an \n, so that we do not mistake
      the \r\n sequence for a single DOS line ending and erroneously
      issue the "No newline at end of file" diagnostic.  */
-  if (to.text[to.len - 1] == '\r')
+  if (to.len && to.text[to.len - 1] == '\r')
     to.text[to.len] = '\r';
   else
     to.text[to.len] = '\n';
Index: fixincludes/ChangeLog.STM
===================================================================
--- fixincludes/ChangeLog.STM	(.../vendor/tags/4.2.4)	(revision 0)
+++ fixincludes/ChangeLog.STM	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -0,0 +1,20 @@
+2008-28-08  Antony King  <antony.king@st.com>
+
+	* check.tpl: Avoid premature termination of script on fgrep -v
+	failure.
+
+2008-27-08  Christian Bruel  <christian.bruel@st.com>
+
+	* fixincl.c (test_test). Dont quote test.
+
+2007-10-02  Antony King  <antony.king@st.com>
+
+	* fixincl.c (cygpath_open): New function.
+	(load_file): Replace open() with cygpath_open().
+	(create_file): Likewise.
+	(process): Likewise.
+	(initialize): Add calls to CYGPATH() and CYGPATH_FREE().
+	(test_test): Add missing quotes.
+	(fix_with_system): Likewise.
+	(fix_with_system): Force use of Unix shell.
+	(main): Redirect stdin to nul: on Windows.
Index: fixincludes/check.tpl
===================================================================
--- fixincludes/check.tpl	(.../vendor/tags/4.2.4)	(revision 920)
+++ fixincludes/check.tpl	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -143,9 +143,9 @@
 
 cd $TESTBASE
 
-find * -type f -print | \
+( find * -type f -print | \
 fgrep -v 'CVS/' | \
-fgrep -v '.svn/' > ${TESTDIR}/LIST
+fgrep -v '.svn/' > ${TESTDIR}/LIST ; true )
 
 exitok=`
 exec < ${TESTDIR}/LIST
Index: fixincludes/fixincl.c
===================================================================
--- fixincludes/fixincl.c	(.../vendor/tags/4.2.4)	(revision 920)
+++ fixincludes/fixincl.c	(.../tags/gcc-st40-4.2.4.090602)	(revision 920)
@@ -100,6 +100,22 @@
 
 #include "fixincl.x"
 
+static int
+cygpath_open (const char *file, int oflag, mode_t mode)
+{
+  int fd;
+
+#ifdef _O_BINARY
+  oflag |= _O_BINARY;
+#endif
+
+  CYGPATH (file);
+  fd = open (file, oflag, mode);
+  CYGPATH_FREE (file);
+
+  return fd;
+}
+
 /* * * * * * * * * * * * * * * * * * *
  *
  *  MAIN ROUTINE
@@ -121,7 +137,11 @@
       and err open so that the proper input file does not get closed
       by accident  */
 
+#if defined(__MSDOS__) || defined(_WIN32)
+  freopen ("nul:", "r", stdin);
+#else
   freopen ("/dev/null", "r", stdin);
+#endif
 
   if (file_name_buf == (char *) NULL)
     {
@@ -210,6 +230,8 @@
 void
 initialize ( int argc, char** argv )
 {
+  char *arg;
+
   xmalloc_set_program_name (argv[0]);
 
   switch (argc)
@@ -220,12 +242,15 @@
     case 2:
       if (strcmp (argv[1], "-v") == 0)
         do_version ();
-      if (freopen (argv[1], "r", stdin) == (FILE*)NULL)
+      arg = argv[1];
+      CYGPATH (arg);
+      if (freopen (arg, "r", stdin) == (FILE*)NULL)
         {
           fprintf (stderr, "Error %d (%s) reopening %s as stdin\n",
                    errno, xstrerror (errno), argv[1] );
           exit (EXIT_FAILURE);
         }
+      CYGPATH_FREE (arg);
       break;
 
     default:
@@ -324,7 +349,7 @@
       the file size is not a multiple of the page size.  If it is a multiple,
       then this adjustment sometimes fails anyway.  */
   data_map_size = stbf.st_size+1;
-  data_map_fd   = open (fname, O_RDONLY);
+  data_map_fd   = cygpath_open (fname, O_RDONLY, 0);
   ttl_data_size += data_map_size-1;
 
   if (data_map_fd < 0)
@@ -535,7 +560,7 @@
 
   sprintf (fname, "%s/%s", pz_dest_dir, pz_curr_file + find_base_len);
 
-  fd = open (fname, O_WRONLY | O_CREAT | O_TRUNC, S_IRALL);
+  fd = cygpath_open (fname, O_WRONLY | O_CREAT | O_TRUNC, S_IRALL);
 
   /*  We may need to create the directories needed... */
   if ((fd < 0) && (errno == ENOENT))
@@ -560,7 +585,7 @@
         }
 
       /*  Now, lets try the open again... */
-      fd = open (fname, O_WRONLY | O_CREAT | O_TRUNC, S_IRALL);
+      fd = cygpath_open (fname, O_WRONLY | O_CREAT | O_TRUNC, S_IRALL);
     }
   if (fd < 0)
     {
@@ -598,7 +623,7 @@
 test_test (tTestDesc* p_test, char* pz_test_file)
 {
   tSCC cmd_fmt[] =
-"file=%s\n\
+"file='%s'\n\
 if ( test %s ) > /dev/null 2>&1\n\
 then echo TRUE\n\
 else echo FALSE\n\
@@ -911,7 +936,7 @@
 #else
       /* Don't use positional formatting arguments because some lame-o
          implementations cannot cope  :-(.  */
-      tSCC   z_cmd_fmt[] = " %s > %sX ; rm -f %s; mv -f %sX %s";
+      tSCC   z_cmd_fmt[] = " '%s' > '%sX'; rm -f '%s'; mv -f '%sX' '%s'";
 #endif
       tCC**  ppArgs = p_fixd->patch_args;
 
@@ -994,7 +1019,34 @@
                pz_temp_file, pz_temp_file, pz_temp_file);
 #endif
     }
+#if 1
+  {
+    char *cmd;
+    char *fname = make_temp_file( 0 );
+    FILE *pf = fopen( fname, "w" );
+    if (pf == NULL)
+      {
+	fprintf (stderr, "Error %d (%s) creating %s\n",
+		 errno, xstrerror (errno), fname);
+	exit (EXIT_FAILURE);
+      }
+    fwrite( pz_cmd, 1, strlen( pz_cmd ), pf );
+    fclose( pf );
+    asprintf( &cmd, "sh %s", fname );
+    if (cmd == NULL)
+      {
+	fprintf (stderr, "Error %d (%s)\n",
+		 errno, xstrerror (errno));
+	exit (EXIT_FAILURE);
+      }
+    system( cmd );
+    free( (void*)cmd );
+    unlink( fname );
+    free( (void*)fname );
+  }
+#else
   system( pz_cmd );
+#endif
   free( (void*)pz_cmd );
 }
 
@@ -1351,7 +1403,7 @@
 
       if (read_fd == -1)
         {
-          read_fd = open (pz_curr_file, O_RDONLY);
+          read_fd = cygpath_open (pz_curr_file, O_RDONLY, 0);
           if (read_fd < 0)
             {
               fprintf (stderr, "Error %d (%s) opening %s\n", errno,
@@ -1405,7 +1457,7 @@
       pz_file_source = pz_temp_file;
     }
 
-  read_fd = open (pz_temp_file, O_RDONLY);
+  read_fd = cygpath_open (pz_temp_file, O_RDONLY, 0);
   if (read_fd < 0)
     {
       if (errno != ENOENT)
